{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: One Hunter vs Static Targets\n## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os, sys, csv, cv2, imageio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.bool = bool\n",
    "np.long = int\n",
    "np.int = int\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML  # (for display_side_by_side)",
    "from collections import deque\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "import math\n",
    "import copy\n",
    "from math import inf\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import random\n",
    "import pickle\n",
    "sys.path.append(\"C:/Users/aveexela/Desktop/rl_project\")\n",
    "\n",
    "from world.realm import Realm\n",
    "from world.map_loaders.base import MixedMapLoader\n",
    "from world.map_loaders.single_team import (\n",
    "    SingleTeamLabyrinthMapLoader,\n",
    "    SingleTeamRocksMapLoader,\n",
    ")\n",
    "from world.map_loaders.pregenerated_loader import PregeneratedMapLoader\n",
    "from world.envs import OnePlayerEnv\n",
    "from world.utils import RenderedEnvWrapper\n",
    "from world.scripted_agents import ClosestTargetAgent\n",
    "\n",
    "from world.envs import VersusBotEnv\n",
    "from world.realm import Realm\n",
    "from world.map_loaders.base import MixedMapLoader\n",
    "from world.map_loaders.pregenerated_loader import PregeneratedMapLoader\n",
    "from world.map_loaders.two_teams import (\n",
    "    TwoTeamRocksMapLoader, TwoTeamLabyrinthMapLoader\n",
    ")\n",
    "from world.map_loaders.single_team import (\n",
    "    SingleTeamRocksMapLoader, SingleTeamLabyrinthMapLoader\n",
    ")\n",
    "from world.scripted_agents import ClosestTargetAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "TEAM_SIZE = 5  # пять hunterов",
    "\n",
    "MAP_SIZE = 40       # << был 20",
    "PREYS = 100\n",
    "STEPS = 300        # лимит шагов эпизода",
    "LR = 1e-4       # единый LR, без дубля",
    "\n",
    "LOG_DIR   = os.environ.get(\"LOG_DIR\", \"logs\")\n",
    "FRAME_DIR = os.path.join(LOG_DIR, \"frames\")\n",
    "MAP_DIR   = os.path.join(LOG_DIR, \"maps\")\n",
    "LOG_STEP_PATH = os.path.join(LOG_DIR, \"step_log.csv\")\n",
    "\n",
    "for d in (LOG_DIR, FRAME_DIR, MAP_DIR, os.path.join(LOG_DIR, \"checkpoints\"),\n",
    "          os.path.join(LOG_DIR, \"checkpoints_pvp\"),\n",
    "          os.path.join(LOG_DIR, \"checkpoints_pvp_eval\")):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# — удаляем папки с картинками, if были, and создаём заново —",
    "for d in (FRAME_DIR, MAP_DIR):\n",
    "    if os.path.isdir(d):\n",
    "        shutil.rmtree(d)\n",
    "os.makedirs(FRAME_DIR, exist_ok=True)\n",
    "os.makedirs(MAP_DIR,   exist_ok=True)\n",
    "\n",
    "# overwrite CSV-хедер (as and раньше)",
    "if os.path.exists(LOG_STEP_PATH):\n",
    "    os.remove(LOG_STEP_PATH)\n",
    "    print(f\"Старый лог удалён: {LOG_STEP_PATH}\")\n",
    "    \n",
    "with open(LOG_STEP_PATH, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\n",
    "        \"phase\", \"episode\", \"step\",\n",
    "        \"pred_id\", \"pred_y\", \"pred_x\",\n",
    "        \"exec_action\", \"teacher_action\",\n",
    "        \"alive_preys\", \"caught_total\", \"new_caught\",\n",
    "        \"team_score\", \"score_delta\",\n",
    "        \"reward\", \"r_base\", \"r_capture\", \"r_explore\", \"r_standstill\", \"r_revisit\", \"r_bfs\", \"r_repulse\",\n",
    "        \"r_flipflop\", \"r_same_dir_close\",           # <— new",
    "        \"idle_preds\", \"pair_d0\", \"pair_d1\", \"team_disp\",\n",
    "        \"nearest_prey_y\", \"nearest_prey_x\", \"dy_to_nearest\", \"dx_to_nearest\",\n",
    "        \"caught_coords\"\n",
    "    ])\n",
    "\n",
    "\n",
    "# for eval используем фикс-visited-map matrix под текущий MAP_SIZE",
    "visited_map = np.zeros((MAP_SIZE, MAP_SIZE), dtype=np.int32)\n",
    "STEP_LOG_BUFFER = []\n",
    "# кэш векtorusов движения за прошлый step: {pred_id: (dy, dx)}",
    "LAST_DIRS = {}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# фичи for NetAgent1",
    "PATCH_SIZE = 7\n",
    "K_NEAREST  = 4\n",
    "\n",
    "# === REWARD COEFFS (обновить values) ===",
    "exploration_coef     = 0.002   # было 0.01 — сильно раздувало «бродилки»",
    "stand_still_penalty  = 0.004   # было 0.005",
    "revisit_penalty      = 0.0007  # было 0.001",
    "\n",
    "# потенциальный shaping будет основным:",
    "shaping_coef         = 0.02    # оставим таким — но он теперь «безопасный» (разность потенциала)",
    "\n",
    "# ранний буст исследования — помягче:",
    "early_steps_boost    = 20\n",
    "early_explore_scale  = 1.4\n",
    "\n",
    "# repulsion сильно помягчить + cap:",
    "repulse_same_cell    = 0.02    # было 0.05",
    "repulse_adjacent     = 0.01    # было 0.02",
    "repulse_radius2      = 0.005   # было 0.01",
    "repulse_boost        = 1.5     # было 2.0",
    "REPULSE_CAP_PER_AGENT = 0.05   # new: кэп суммарного repulsionа на агента за step",
    "REPULSE_KICKS_IN_AFTER = 40    # new: enable repulsion только после N шагов эпизода",
    "\n",
    "# anti-«пиление» and «lockstep» — только if прогресса к жертве нет:",
    "flipflop_penalty        = 0.01   # было 0.03",
    "same_dir_close_penalty  = 0.007  # было 0.015",
    "\n",
    "# bonus плитка — очень маленький, and не вредить погоне:",
    "bonus_reward_base = 0.3          # было 2.0 — слишком жирно",
    "BONUS_TAKEN = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBuilder:\n",
    "    \"\"\"\n",
    "    Готовит фиксированный по длине векtorus признаков for ОДНОГО hunterа when ЛЮБОМ размере maps.\n",
    "    Состав: [голова (5)] + [патч P×P] + [K_preys*3] + [K_mates*3].\n",
    "      - K_preys: K closest жертв: [dy, dx, dn]\n",
    "      - K_mates: K closest teammates (кроме себя): [dy, dx, dn]\n",
    "    Все dy/dx нормированы на H/W, dn — на (H+W).\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=PATCH_SIZE, k_nearest=K_NEAREST, k_mates: int = 2):\n",
    "        self.patch_size = patch_size\n",
    "        self.k_nearest  = k_nearest\n",
    "        self.k_mates    = k_mates\n",
    "\n",
    "    def __call__(self, info: dict, world_map, pred_idx: int = 0) -> np.ndarray:\n",
    "        wm = np.array(world_map)\n",
    "        H, W = wm.shape[:2]\n",
    "        # mask passability",
    "        if wm.ndim == 3:\n",
    "            walls = (wm[:, :, 0] == -1) & (wm[:, :, 1] == -1)\n",
    "        else:\n",
    "            walls = (wm == -1)\n",
    "        passable = (~walls).astype(np.float32)  # [H, W]",
    "\n",
    "        # --- этот hunter ---",
    "        p = info[\"predators\"][pred_idx]\n",
    "        y, x = int(p[\"y\"]), int(p[\"x\"])\n",
    "\n",
    "        # --- alive preys ---",
    "        preys = [q for q in info[\"preys\"] if q[\"alive\"]]\n",
    "        alive_cnt = len(preys)\n",
    "\n",
    "        # --- локальный патч ---",
    "        patch = extract_toroidal_patch(passable, y, x, self.patch_size).reshape(1, -1)\n",
    "\n",
    "        # --- K closest жертв ---",
    "        # --- K closest жертв (torus) ---",
    "        prey_feats = []\n",
    "        if preys:\n",
    "            dgrid = shortest_path_grid_from_torus(wm, (y, x))\n",
    "            for i in range(min(self.k_nearest, len(preys))):\n",
    "                pass  # заполним ниже из отсортированного списка",
    "\n",
    "            dlist = [(float(dgrid[int(q[\"y\"]) % H, int(q[\"x\"]) % W]), q) for q in preys]\n",
    "            dlist.sort(key=lambda t: t[0])\n",
    "            for i in range(min(self.k_nearest, len(dlist))):\n",
    "                d, q = dlist[i]\n",
    "                if not np.isfinite(d):\n",
    "                    dy = dx = 0.0; dn = 1.0\n",
    "                else:\n",
    "                    qy, qx = int(q[\"y\"]) % H, int(q[\"x\"]) % W\n",
    "                    dy = ( (qy - y) % H ); dy = min(dy, H - dy) / max(1, H)  # torus-нормировка",
    "                    dx = ( (qx - x) % W ); dx = min(dx, W - dx) / max(1, W)\n",
    "                    dn = float(min(d, H + W) / (H + W))\n",
    "                prey_feats.extend([dy, dx, dn])\n",
    "                \n",
    "        # паддинг",
    "        need = self.k_nearest * 3\n",
    "        while len(prey_feats) < need:\n",
    "            prey_feats.extend([0.0, 0.0, 1.0])\n",
    "\n",
    "        # --- K closest тиммейтов (кроме себя) ---",
    "        mates = [q for i, q in enumerate(info[\"predators\"]) if i != pred_idx and q.get(\"alive\", True)]\n",
    "        mate_feats = []\n",
    "        if mates:\n",
    "            # Manhattan",
    "            dlist = []\n",
    "            for q in mates:\n",
    "                qy, qx = int(q[\"y\"]), int(q[\"x\"])\n",
    "                d = abs(qy - y) + abs(qx - x)\n",
    "                dlist.append((d, qy, qx))\n",
    "            dlist.sort(key=lambda t: t[0])\n",
    "            for i in range(min(self.k_mates, len(dlist))):\n",
    "                d, qy, qx = dlist[i]\n",
    "                dy = (qy - y) / max(1, H)\n",
    "                dx = (qx - x) / max(1, W)\n",
    "                dn = float(min(d, H + W) / (H + W))\n",
    "                mate_feats.extend([dy, dx, dn])\n",
    "        # паддинг",
    "        need_m = self.k_mates * 3\n",
    "        while len(mate_feats) < need_m:\n",
    "            mate_feats.extend([0.0, 0.0, 1.0])\n",
    "\n",
    "        # --- голова ---",
    "        alive_pred = float(p[\"alive\"])\n",
    "        nearest_dn = prey_feats[2] if len(prey_feats) >= 3 else 1.0\n",
    "        head = np.array([[y / max(1, H),\n",
    "                          x / max(1, W),\n",
    "                          alive_pred,\n",
    "                          alive_cnt / max(1, H * W),\n",
    "                          nearest_dn]], dtype=np.float32)\n",
    "\n",
    "        feats = np.concatenate(\n",
    "            [head, patch, np.array(prey_feats, dtype=np.float32).reshape(1, -1),\n",
    "             np.array(mate_feats, dtype=np.float32).reshape(1, -1)],\n",
    "            axis=1\n",
    "        )\n",
    "        return feats\n",
    "\n",
    "class FeatureBuilderPvP(FeatureBuilder):\n",
    "    \"\"\"\n",
    "    Расширение: добавляем K closest enemies [dy,dx,dn] с torus-нормировкой.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=PATCH_SIZE, k_nearest=K_NEAREST, k_mates=2, k_enemies=2):\n",
    "        super().__init__(patch_size, k_nearest, k_mates)\n",
    "        self.k_enemies = k_enemies\n",
    "\n",
    "    def __call__(self, info: dict, world_map, pred_idx: int = 0) -> np.ndarray:\n",
    "        base = super().__call__(info, world_map, pred_idx)  # уже содержит head+patch+preys+mates",
    "        wm = np.array(world_map)\n",
    "        H, W = wm.shape[:2]\n",
    "        # враги ожидаются в info[\"enemies\"] таким же форматом, as \"predators\"",
    "        enemies = [q for q in info.get(\"enemies\", []) if q.get(\"alive\", True)]\n",
    "        p = info[\"predators\"][pred_idx]\n",
    "        y, x = int(p[\"y\"]), int(p[\"x\"])\n",
    "\n",
    "        enemy_feats = []\n",
    "        if enemies:\n",
    "            # torus-Manhattan",
    "            dlist = []\n",
    "            for q in enemies:\n",
    "                qy, qx = int(q[\"y\"]), int(q[\"x\"])\n",
    "                d = manhattan_torus(y, x, qy, qx, H, W)\n",
    "                dlist.append((d, qy, qx))\n",
    "            dlist.sort(key=lambda t: t[0])\n",
    "            for i in range(min(self.k_enemies, len(dlist))):\n",
    "                d, qy, qx = dlist[i]\n",
    "                dy = ((qy - y) % H); dy = min(dy, H - dy) / max(1, H)\n",
    "                dx = ((qx - x) % W); dx = min(dx, W - dx) / max(1, W)\n",
    "                dn = float(min(d, H + W) / (H + W))\n",
    "                enemy_feats.extend([dy, dx, dn])\n",
    "        need_e = self.k_enemies * 3\n",
    "        while len(enemy_feats) < need_e:\n",
    "            enemy_feats.extend([0.0, 0.0, 1.0])\n",
    "\n",
    "        return np.concatenate([base, np.array(enemy_feats, dtype=np.float32).reshape(1, -1)], axis=1)\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden=128, num_actions=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),    nn.ReLU(),\n",
    "            nn.Linear(hidden, num_actions)\n",
    "        )\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net[:-1](x)  # to последнего Linear",
    "        logits = self.net[-1](h)\n",
    "        value  = self.value_head(h)\n",
    "        return logits, value\n",
    "\n",
    "class NetAgentShared:\n",
    "    \"\"\"\n",
    "    Shared-MLP for N hunterов: один and тот же набор весов,\n",
    "    батчим фичи по всем predatorам and получаем список actions.\n",
    "    Добавлен небольшой BC-replay for устойчивости на малой бете.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_builder: FeatureBuilder, num_actions=5, lr=LR, device=None, team_size=5):\n",
    "        self.f = feature_builder\n",
    "        self.team_size = team_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._input_dim = None\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.ce = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "\n",
    "        # --- простой replay ---",
    "        self.rb_feats  = deque(maxlen=50000)  # храним поштучно: [D]",
    "        self.rb_labels = deque(maxlen=50000)  # int",
    "        self._rng = np.random.default_rng(0)\n",
    "\n",
    "    def _ensure_model(self, sample_feats: np.ndarray, num_actions=5):\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        D = sample_feats.shape[1]\n",
    "        self._input_dim = D\n",
    "        self.model = PolicyNet(D, hidden=128, num_actions=num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
    "\n",
    "    def _feats_all_preds(self, info: dict, world_map) -> np.ndarray:\n",
    "        feats_list = [self.f(info, world_map, pred_idx=k) for k in range(self.team_size)]\n",
    "        return np.concatenate(feats_list, axis=0)  # [K, D]",
    "\n",
    "    def get_actions(self, info: dict, world_map, training=False, greedy=False):\n",
    "        feats = self._feats_all_preds(info, world_map)   # [K, D]",
    "        self._ensure_model(feats[:1, :])\n",
    "        x = torch.tensor(feats, dtype=torch.float32, device=self.device)\n",
    "        logits, values = self.model(x)  # [K, A], [K,1]",
    "    \n",
    "        # --- СФОРМИРУЕМ МАСКУ available actions For КАЖДОГО predatorА ---",
    "        # действия: 0=стоим, 1=вправо(x+1), 2=влево(x-1), 3=вверх(y-1), 4=вниз(y+1)",
    "        wm = np.array(world_map)\n",
    "        if wm.ndim == 3:\n",
    "            walls = (wm[:, :, 0] == -1) & (wm[:, :, 1] == -1)\n",
    "        else:\n",
    "            walls = (wm == -1)\n",
    "        H, W = walls.shape\n",
    "        preds = info.get(\"predators\", [])\n",
    "        # prev позиции на случай запрета flipflop/stay (опционально)",
    "        prev_preds = None\n",
    "        # (if хочешь: можно передать prev_info сюда; сейчас ограничимся стенами)",
    "    \n",
    "        # mask: 1 = можно, 0 = нельзя",
    "        # --- torus-mask available actions (шагаем сквозь края maps) ---",
    "        wm = np.array(world_map)\n",
    "        if wm.ndim == 3:\n",
    "            walls = (wm[:, :, 0] == -1) & (wm[:, :, 1] == -1)\n",
    "        else:\n",
    "            walls = (wm == -1)\n",
    "        H, W = walls.shape\n",
    "        preds = info.get(\"predators\", [])\n",
    "\n",
    "        def passable(y, x):\n",
    "            return (not walls[y % H, x % W])\n",
    "\n",
    "        mask = torch.ones_like(logits, dtype=torch.bool)  # [K,5]",
    "        for k, pr in enumerate(preds):\n",
    "            y, x_ = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "            # вправо (x+1), влево (x-1), вверх (y-1), вниз (y+1) — всё по модулю",
    "            if not passable(y, x_ + 1): mask[k, 1] = False\n",
    "            if not passable(y, x_ - 1): mask[k, 2] = False\n",
    "            if not passable(y - 1, x_): mask[k, 3] = False\n",
    "            if not passable(y + 1, x_): mask[k, 4] = False\n",
    "\n",
    "        # if все движения запрещены — оставим хотя бы \"стоим\"",
    "        all_forbidden = (~mask[:, 1:]).all(dim=1)\n",
    "        mask[all_forbidden, 0] = True\n",
    "\n",
    "        # применим маску к логитам",
    "        masked_logits = logits.masked_fill(~mask, -1e9)\n",
    "    \n",
    "        if greedy or not training:\n",
    "            a = torch.argmax(masked_logits, dim=-1)  # [K]",
    "            actions = [int(i) for i in a.detach().cpu().tolist()]\n",
    "            # заглушки",
    "            logprobs = [torch.tensor(0.0, device=self.device) for _ in range(self.team_size)]\n",
    "            ents     = [torch.tensor(0.0, device=self.device) for _ in range(self.team_size)]\n",
    "            vals     = [v.squeeze(-1) for v in values]  # [K,1] -> списком тензоров скаляров",
    "            return actions, logprobs, vals, ents\n",
    "        else:\n",
    "            probs = torch.softmax(masked_logits, dim=-1)\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
    "            dist  = torch.distributions.Categorical(probs)\n",
    "            at    = dist.sample()\n",
    "            actions = [int(i) for i in at.detach().cpu().tolist()]\n",
    "            logprobs = dist.log_prob(at).unbind()\n",
    "            ents     = dist.entropy().unbind()\n",
    "            vals     = [v.squeeze(-1) for v in values]\n",
    "            return actions, logprobs, vals, ents\n",
    "\n",
    "    def train_step_bc_multi(self, info: dict, world_map, teacher_actions: list[int]):\n",
    "        feats = self._feats_all_preds(info, world_map)   # [K, D]",
    "        self._ensure_model(feats[:1, :])\n",
    "        x = torch.tensor(feats, dtype=torch.float32, device=self.device)\n",
    "        y = torch.tensor([int(a) for a in teacher_actions], dtype=torch.long, device=self.device)  # [K]",
    "        logits, _ = self.model(x)\n",
    "        loss = self.ce(logits, y)\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        # вернём, чтобы положить в replay",
    "        return float(loss.detach().cpu()), feats, [int(a) for a in teacher_actions]\n",
    "\n",
    "    # --- replay ---",
    "    def add_replay(self, feats: np.ndarray, labels: list[int]):\n",
    "        for i in range(min(len(feats), len(labels))):\n",
    "            self.rb_feats.append(feats[i].astype(np.float32, copy=True))\n",
    "            self.rb_labels.append(int(labels[i]))\n",
    "\n",
    "    def replay_step(self, steps: int = 1, batch: int = 128):\n",
    "        if len(self.rb_feats) < 512 or self.model is None:\n",
    "            return 0.0\n",
    "        losses = []\n",
    "        for _ in range(steps):\n",
    "            idx = self._rng.integers(0, len(self.rb_feats), size=min(batch, len(self.rb_feats)))\n",
    "            x = torch.tensor(np.stack([self.rb_feats[i] for i in idx], axis=0),\n",
    "                             dtype=torch.float32, device=self.device)\n",
    "            y = torch.tensor([self.rb_labels[i] for i in idx], dtype=torch.long, device=self.device)\n",
    "            logits, _ = self.model(x)\n",
    "            loss = self.ce(logits, y)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            losses.append(float(loss.detach().cpu()))\n",
    "        return float(np.mean(losses)) if losses else 0.0\n",
    "\n",
    "class AssignedClosestTargetAgent(ClosestTargetAgent):\n",
    "    \"\"\"\n",
    "    Назначаем hunterам уникальные targets (жадный матчинг), но:\n",
    "    - keep текущее assignment хотя бы hold_steps шагов,\n",
    "    - меняем target только if новая короче на switch_margin and/or истёк hold.\n",
    "    Это резко снижает \"thrashing\" and crowding.\n",
    "    coordinates — as в субфайлах: state[x,y,*], но у нас всё переводится в линейные индексы.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_predators: int, hold_steps: int = 4, switch_margin: float = 2.0):\n",
    "        super().__init__(num_predators=num_predators)\n",
    "        self.hold_steps = int(hold_steps)\n",
    "        self.switch_margin = float(switch_margin)\n",
    "        self._assign_tlin = {}   # pred_idx -> t_lin (линейный индекс targets)",
    "        self._assign_age  = {}   # pred_idx -> сколько шагов keep эту target",
    "\n",
    "    def reset(self, state, team):\n",
    "        # важная часть: initialization карт distances у базового агента",
    "        super().reset(state, team)\n",
    "        # сброс нашей памяти",
    "        self._assign_tlin.clear()\n",
    "        self._assign_age.clear()\n",
    "\n",
    "    def get_actions(self, state, team):\n",
    "        actions = [0 for _ in range(self.num_predators)]\n",
    "        predators = {}  # idx -> (x, y)",
    "        preys = []\n",
    "\n",
    "        preys_team = np.max(state[:, :, 0])\n",
    "        if preys_team == team:\n",
    "            preys_team = None\n",
    "\n",
    "        # as в исходнике ClosestTargetAgent (x=строка, y=столбец)",
    "        for x in range(state.shape[0]):\n",
    "            for y in range(state.shape[1]):\n",
    "                if state[x, y, 0] == team:\n",
    "                    predators[state[x, y, 1]] = (x, y)\n",
    "                    continue\n",
    "                if (preys_team is None and state[x, y, 0] > 0) or (state[x, y, 0] == preys_team):\n",
    "                    preys.append((x, y))\n",
    "\n",
    "        if not predators or not preys:\n",
    "            return actions\n",
    "\n",
    "        H, W = state.shape[0], state.shape[1]\n",
    "        alive_tlin = {px * W + py for (px, py) in preys}\n",
    "\n",
    "        # Собираем все пары (pred, prey) с дистанцией",
    "        pairs = []\n",
    "        for pred_idx, (px, py) in predators.items():\n",
    "            p_lin = px * W + py\n",
    "            for (tx, ty) in preys:\n",
    "                t_lin = tx * W + ty\n",
    "                d = float(self.distance_map[p_lin, t_lin])\n",
    "                if np.isfinite(d):\n",
    "                    pairs.append((pred_idx, p_lin, (tx, ty), t_lin, d))\n",
    "\n",
    "        if not pairs:\n",
    "            return actions\n",
    "\n",
    "        # greedy уникальное assignment (без коллизий)",
    "        pairs.sort(key=lambda z: z[4])  # по расстоянию",
    "        used_pred, used_tlin = set(), set()\n",
    "        proposed = {}  # pred_idx -> (p_lin, t_lin, d_new)",
    "        for pred_idx, p_lin, (tx, ty), t_lin, d in pairs:\n",
    "            if pred_idx in used_pred or t_lin in used_tlin:\n",
    "                continue\n",
    "            used_pred.add(pred_idx)\n",
    "            used_tlin.add(t_lin)\n",
    "            proposed[pred_idx] = (p_lin, t_lin, d)\n",
    "            if len(proposed) == self.num_predators:\n",
    "                break\n",
    "\n",
    "        # Применяем hysteresis: решаем, сохранять старую target or принять новую",
    "        chosen = {}  # pred_idx -> (p_lin, t_lin_final)",
    "        for pred_idx, (px, py) in predators.items():\n",
    "            p_lin = px * W + py\n",
    "\n",
    "            # новая рекомендация матчинга (может отсутствовать)",
    "            prop = proposed.get(pred_idx, None)\n",
    "            new_t_lin = prop[1] if prop is not None else None\n",
    "            new_d     = prop[2] if prop is not None else np.inf\n",
    "\n",
    "            # прежнее assignment (if есть and target ещё жива)",
    "            old_t_lin = self._assign_tlin.get(pred_idx, None)\n",
    "            have_old  = (old_t_lin is not None) and (old_t_lin in alive_tlin)\n",
    "            old_d     = float(self.distance_map[p_lin, old_t_lin]) if have_old else np.inf\n",
    "            age       = self._assign_age.get(pred_idx, 0)\n",
    "\n",
    "            keep_old = False\n",
    "            if have_old:\n",
    "                # keep хотя бы hold_steps шагов",
    "                if age < self.hold_steps:\n",
    "                    keep_old = True\n",
    "                else:\n",
    "                    # можно переключаться, но только if новая target реально лучше",
    "                    # \"лучше\" = короче на switch_margin (or старой дистанции нет)",
    "                    if not (np.isfinite(new_d) and (new_d + self.switch_margin < old_d)):\n",
    "                        keep_old = True\n",
    "\n",
    "            if keep_old:\n",
    "                t_lin_final = old_t_lin\n",
    "                self._assign_age[pred_idx] = age + 1\n",
    "            else:\n",
    "                # принимаем новую, if она есть; if нет — падаем на ближайшую из всех",
    "                if new_t_lin is not None and np.isfinite(new_d):\n",
    "                    t_lin_final = new_t_lin\n",
    "                else:\n",
    "                    # fallback: ближайшая вообще (as в базовом)",
    "                    bestd, best_t = np.inf, None\n",
    "                    for (tx, ty) in preys:\n",
    "                        t_lin = tx * W + ty\n",
    "                        d = float(self.distance_map[p_lin, t_lin])\n",
    "                        if d < bestd:\n",
    "                            bestd, best_t = d, t_lin\n",
    "                    t_lin_final = best_t\n",
    "\n",
    "                self._assign_tlin[pred_idx] = t_lin_final\n",
    "                self._assign_age[pred_idx]  = 0\n",
    "\n",
    "            chosen[pred_idx] = (p_lin, t_lin_final)\n",
    "\n",
    "        # Первое действие по action_map (as and раньше)",
    "        for k, (px, py) in predators.items():\n",
    "            p_lin, t_lin = chosen.get(k, (px * W + py, None))\n",
    "            if t_lin is None or not np.isfinite(self.distance_map[p_lin, t_lin]):\n",
    "                actions[k] = 0  # на всякий случай — стоять",
    "            else:\n",
    "                actions[k] = int(self.action_map[p_lin, t_lin])\n",
    "        return actions\n",
    "        \n",
    "# === ЛЁГКАЯ Model And АГЕНТ (for ограничения по времени) ===",
    "class PolicyNetLite(nn.Module):\n",
    "    \"\"\"\n",
    "    lightweight policy: 1 скрытый слой поменьше + value_head (for совместимости интерфейса).\n",
    "    Входной size такой же, as у тяжёлой модели — это позволяет дистиллировать без смены фич.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden=64, num_actions=5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.logits = nn.Linear(hidden, num_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.fc1(x))\n",
    "        return self.logits(h), self.value_head(h)\n",
    "\n",
    "\n",
    "class NetAgentSharedLite(NetAgentShared):\n",
    "    \"\"\"\n",
    "    Тот же интерфейс, что у NetAgentShared, но с PolicyNetLite and урезанным реплеем.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_builder: FeatureBuilder, num_actions=5, lr=LR, device=None, team_size=5):\n",
    "        super().__init__(feature_builder, num_actions, lr, device, team_size)\n",
    "        # меньше replay and попроще сглаживание CE",
    "        self.rb_feats  = deque(maxlen=20000)\n",
    "        self.rb_labels = deque(maxlen=20000)\n",
    "        self.ce = nn.CrossEntropyLoss(label_smoothing=0.03)\n",
    "\n",
    "    def _ensure_model(self, sample_feats: np.ndarray, num_actions=5):\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        D = sample_feats.shape[1]\n",
    "        self._input_dim = D\n",
    "        self.model = PolicyNetLite(D, hidden=64, num_actions=num_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_wall(raw_map, y, x):\n",
    "    # 2D: -1 = стена; 3D: канал 0 and 1 равны -1 for walls",
    "    if raw_map.ndim == 3:\n",
    "        return (raw_map[y, x, 0] == -1) and (raw_map[y, x, 1] == -1)\n",
    "    else:\n",
    "        return (raw_map[y, x] == -1)\n",
    "\n",
    "def manhattan_torus(y1, x1, y2, x2, H, W):\n",
    "    dy = min((y1 - y2) % H, (y2 - y1) % H)\n",
    "    dx = min((x1 - x2) % W, (x2 - x1) % W)\n",
    "    return dy + dx\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def shortest_path_grid_from_torus(raw_map, start_yx):\n",
    "    \"\"\"\n",
    "    BFS with toroidal wrapping (4-neighborhood), walls учитываем as обычно.\n",
    "    Возвращает dist[y,x] (np.inf if недостижимо из-за стен/островов).\n",
    "    \"\"\"\n",
    "    sy, sx = start_yx\n",
    "    H, W = raw_map.shape[:2]\n",
    "    dist = np.full((H, W), np.float32(np.inf), dtype=np.float32)\n",
    "    if _is_wall(raw_map, sy % H, sx % W):\n",
    "        return dist\n",
    "    dq = deque()\n",
    "    dq.append((sy % H, sx % W))\n",
    "    dist[sy % H, sx % W] = 0.0\n",
    "    while dq:\n",
    "        y, x = dq.popleft()\n",
    "        d = dist[y, x] + 1.0\n",
    "        for ny, nx in ((y-1, x), (y+1, x), (y, x-1), (y, x+1)):\n",
    "            ny %= H; nx %= W\n",
    "            if dist[ny, nx] > d and not _is_wall(raw_map, ny, nx):\n",
    "                dist[ny, nx] = d\n",
    "                dq.append((ny, nx))\n",
    "    return dist\n",
    "\n",
    "def shortest_path_grid_from(raw_map, start_yx):\n",
    "    \"\"\"\n",
    "    Returns a distance matrix dist[y,x] from start_yx to всех cells (np.inf if недостижимо).\n",
    "    4-neighborhood, учитывает walls (-1). coordinates [y,x].\n",
    "    \"\"\"\n",
    "    sy, sx = start_yx\n",
    "    H, W = raw_map.shape[:2]\n",
    "    dist = np.full((H, W), np.inf, dtype=np.float32)\n",
    "    if _is_wall(raw_map, sy, sx):\n",
    "        return dist\n",
    "    dq = deque()\n",
    "    dq.append((sy, sx))\n",
    "    dist[sy, sx] = 0.0\n",
    "    while dq:\n",
    "        y, x = dq.popleft()\n",
    "        d = dist[y, x] + 1.0\n",
    "        # порядок соседей: U, D, L, R (не принципиально)",
    "        for ny, nx in ((y-1, x), (y+1, x), (y, x-1), (y, x+1)):\n",
    "            if 0 <= ny < H and 0 <= nx < W and dist[ny, nx] > d and not _is_wall(raw_map, ny, nx):\n",
    "                dist[ny, nx] = d\n",
    "                dq.append((ny, nx))\n",
    "    return dist\n",
    "\n",
    "\n",
    "def extract_toroidal_patch(passable_map: np.ndarray, y: int, x: int, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Возвращает локальный патч size×size вокруг [y,x] с torus-зацикливанием по краям.\n",
    "    Important: coordinates везде [y, x].\n",
    "    \"\"\"\n",
    "    assert size % 2 == 1, \"PATCH_SIZE должен быть нечетным\"\n",
    "    H, W = passable_map.shape\n",
    "    r = size // 2\n",
    "    patch = np.zeros((size, size), dtype=np.float32)\n",
    "    for dy in range(-r, r + 1):\n",
    "        for dx in range(-r, r + 1):\n",
    "            yy = (y + dy) % H\n",
    "            xx = (x + dx) % W\n",
    "            patch[dy + r, dx + r] = passable_map[yy, xx]\n",
    "    return patch\n",
    "\n",
    "\n",
    "def shortest_path_dist(raw_map, src_yx, dst_yx):\n",
    "    \"\"\"shortest path по 4-соседству с учётом стен (-1). Возвращает np.inf if недостижимо.\"\"\"\n",
    "    sy, sx = src_yx\n",
    "    ty, tx = dst_yx\n",
    "    if (sy, sx) == (ty, tx):\n",
    "        return 0\n",
    "    H, W = raw_map.shape[:2]\n",
    "    seen = np.zeros((H, W), dtype=bool)\n",
    "    dq = deque([(sy, sx, 0)])\n",
    "    seen[sy, sx] = True\n",
    "    while dq:\n",
    "        y, x, d = dq.popleft()\n",
    "        for ny, nx in ((y-1, x), (y+1, x), (y, x-1), (y, x+1)):\n",
    "            if 0 <= ny < H and 0 <= nx < W and not seen[ny, nx] and not _is_wall(raw_map, ny, nx):\n",
    "                if (ny, nx) == (ty, tx):\n",
    "                    return d + 1\n",
    "                seen[ny, nx] = True\n",
    "                dq.append((ny, nx, d + 1))\n",
    "    return np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_singleteam_mixed_loader(\n",
    "    size=40, preys_num=100, spawn_points=5,\n",
    "    rocks_grid=None,\n",
    "    lab_links=None,\n",
    "    pregenerated_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a MixedMapLoader from multiple generators:\n",
    "      - Rocks: 8 configurations по сетке вероятностей\n",
    "      - Labyrinth: 8 configurations по числу доп. links/passages\n",
    "      - (опционально) pregenerated .npy из папки\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "\n",
    "    # 8 карт со rocks: proba в духе README (0.01..0.15, доп. 0..0.21)",
    "    if rocks_grid is None:\n",
    "        rocks_grid = [\n",
    "            (0.01, 0.00), (0.03, 0.05), (0.05, 0.10), (0.07, 0.12),\n",
    "            (0.09, 0.15), (0.11, 0.18), (0.13, 0.20), (0.15, 0.21),\n",
    "        ]\n",
    "    for p, ap in rocks_grid:\n",
    "        loaders.append(\n",
    "            SingleTeamRocksMapLoader(\n",
    "                size=size, preys_num=preys_num, spawn_points=spawn_points,\n",
    "                rock_spawn_proba=p, additional_rock_spawn_proba=ap\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 8 карт с labyrinths: from \"рыхлых\" к \"узким\"",
    "    if lab_links is None:\n",
    "        # (max, min)",
    "        lab_links = [(24,12), (20,10), (16,8), (12,6),\n",
    "                     (10,4),  (8,3),  (6,2),  (3,1)]\n",
    "    for lmax, lmin in lab_links:\n",
    "        loaders.append(\n",
    "            SingleTeamLabyrinthMapLoader(\n",
    "                size=size, preys_num=preys_num, spawn_points=spawn_points,\n",
    "                additional_links_max=lmax, additional_links_min=lmin\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4 “hand-crafted” — if есть folder с .npy (не обязательно)",
    "    if pregenerated_dir and PregeneratedMapLoader is not None:\n",
    "        if os.path.isdir(pregenerated_dir) and len(os.listdir(pregenerated_dir)) > 0:\n",
    "            loaders.append(PregeneratedMapLoader(pregenerated_dir))\n",
    "\n",
    "    return MixedMapLoader(loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_gif(env_wrapper, save_path, resize_factor=10, fps=8):\n",
    "    \"\"\"\n",
    "    GIF Generation в numpy-ориентации (map[y, x]).\n",
    "    Без транспонирования and отражений — полностью совпадает с visited_map.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for frame in env_wrapper.frames:\n",
    "        h, w = frame.shape\n",
    "        img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "        img[frame == -1] = env_wrapper.road_color\n",
    "        img[frame == -2] = env_wrapper.stone_color\n",
    "        img[frame == -3] = env_wrapper.bonus_color\n",
    "        for j in range(env_wrapper.realm.world.playable_teams_num):\n",
    "            img[frame == j] = env_wrapper.team_colors[j]\n",
    "        img[frame == env_wrapper.realm.world.playable_teams_num] = env_wrapper.prey_color\n",
    "\n",
    "        # --- без transpose and flip ---  теперь всё совпадает с visited_map",
    "        img = np.repeat(np.repeat(img, resize_factor, axis=0), resize_factor, axis=1)\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        frames.append(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    imageio.mimsave(save_path, frames, duration=1/fps)\n",
    "    print(f\"GIF сохранён: {save_path}\")\n",
    "\n",
    "\n",
    "def visualize_visited_map(ep, info, visited_map, predator_path, save_path):\n",
    "    \"\"\"\n",
    "    Heatmap and все объекты в numpy-порядке (y, x).\n",
    "    styling — as в стабильной версии (больше поля, узкая legend),\n",
    "    логика совмещения размеров — из исправленной версии.\n",
    "    \"\"\"\n",
    "    # --- 0) Размеры источников ---",
    "    Hv, Wv = visited_map.shape\n",
    "\n",
    "    raw_map = None\n",
    "    if (\"map\" in info) and (info[\"map\"] is not None):\n",
    "        raw_map = np.array(info[\"map\"])\n",
    "    elif hasattr(env_wrapper.base_env.realm.world, \"map\"):\n",
    "        raw_map = np.array(env_wrapper.base_env.realm.world.map)\n",
    "\n",
    "    if raw_map is not None and raw_map.ndim >= 2:\n",
    "        Hr, Wr = raw_map.shape[:2]\n",
    "    else:\n",
    "        Hr, Wr = Hv, Wv\n",
    "\n",
    "    # Общая отображаемая область = пересечение размеров",
    "    dispH, dispW = min(Hv, Hr), min(Wv, Wr)\n",
    "    vmap = visited_map[:dispH, :dispW]\n",
    "\n",
    "    # --- 1) Фигура and компоновка (чуть больше теплокарта + узкая цветовая scale) ---",
    "    plt.style.use(\"dark_background\")\n",
    "    long_side = max(dispH, dispW)\n",
    "    scale = max(6.8, min(9.0, 6.2 * long_side / 20.0))  # слегка больше 6\", адаптивно from размера maps",
    "\n",
    "    fig = plt.figure(figsize=(scale, scale), facecolor=\"black\")\n",
    "    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=[1.0, 0.045], wspace=0.035)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    cax = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # --- 2) Тепловая подложка ---",
    "    im = ax.imshow(vmap, cmap=\"plasma\", origin=\"upper\", interpolation=\"nearest\")\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.set_label(\"Посещения клетки\",fontsize = 14)\n",
    "    cax.set_facecolor(\"black\")\n",
    "    cax.tick_params(colors=\"white\", length=0)\n",
    "    for spine in cax.spines.values():\n",
    "        spine.set_color(\"black\")\n",
    "\n",
    "    # --- 3) Геометрия coordinates ---",
    "    ax.set_xlim(-0.5, dispW - 0.5)\n",
    "    ax.set_ylim(dispH - 0.5, -0.5)  # origin='upper'",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.autoscale(False)\n",
    "\n",
    "    # --- 4) walls ---",
    "    if raw_map is not None:\n",
    "        if raw_map.ndim == 3:\n",
    "            walls_full = np.logical_and(raw_map[:, :, 0] == -1, raw_map[:, :, 1] == -1)\n",
    "        else:\n",
    "            walls_full = (raw_map == -1)  # -1 = стена в 2D",
    "        walls = walls_full[:dispH, :dispW]\n",
    "        ax.imshow(np.ma.masked_where(~walls, walls),\n",
    "                  cmap=plt.cm.Greys_r, alpha=0.85, origin=\"upper\", zorder=1)\n",
    "        ax.contour(walls.astype(float), levels=[0.5],\n",
    "                   colors=\"#00FFFF\", linewidths=1.0, alpha=0.8, zorder=2)\n",
    "\n",
    "    # --- 5) hunter path (с учётом torusа and клипом в display area) ---",
    "    if predator_path and len(predator_path) > 1:\n",
    "        path = [(int(y), int(x))\n",
    "                for (y, x) in predator_path\n",
    "                if 0 <= y < dispH and 0 <= x < dispW]\n",
    "        if len(path) > 1:\n",
    "            path = np.array(path, dtype=np.int32)\n",
    "            ys, xs = path[:, 0], path[:, 1]\n",
    "            seg_y, seg_x = [[ys[0]]], [[xs[0]]]\n",
    "            for i in range(1, len(xs)):\n",
    "                dx, dy = abs(xs[i] - xs[i - 1]), abs(ys[i] - ys[i - 1])\n",
    "                if dx > dispW // 2 or dy > dispH // 2:\n",
    "                    seg_y.append([ys[i]]); seg_x.append([xs[i]])\n",
    "                else:\n",
    "                    seg_y[-1].append(ys[i]); seg_x[-1].append(xs[i])\n",
    "            for sy, sx in zip(seg_y, seg_x):\n",
    "                if len(sx) > 1:\n",
    "                    ax.plot(sx, sy, color=\"#FFFFFF\", linewidth=1.7, alpha=0.9, zorder=5, clip_on=True)\n",
    "\n",
    "    # --- 6) preys ---",
    "    alive, dead = [], []\n",
    "    if \"preys\" in info:\n",
    "        for p in info[\"preys\"]:\n",
    "            y, x = int(p.get(\"y\", -1)), int(p.get(\"x\", -1))\n",
    "            if 0 <= y < dispH and 0 <= x < dispW:\n",
    "                (alive if p.get(\"alive\", False) else dead).append((y, x))\n",
    "\n",
    "    if alive:\n",
    "        ys, xs = zip(*alive)\n",
    "        ax.scatter(xs, ys, c=\"#7CFC00\", marker=\"o\", s=75,\n",
    "                   edgecolors=\"black\", linewidths=0.5, zorder=6, clip_on=True)\n",
    "    if dead:\n",
    "        ys, xs = zip(*dead)\n",
    "        ax.scatter(xs, ys, c=\"#FF6347\", marker=\"X\", s=95,\n",
    "                   edgecolors=\"black\", linewidths=0.6, zorder=6, clip_on=True)\n",
    "\n",
    "    # --- 7) Финальная позиция hunterа ---",
    "    if \"predators\" in info and len(info[\"predators\"]) > 0:\n",
    "        pr = info[\"predators\"][0]\n",
    "        py, px = int(pr.get(\"y\", -1)), int(pr.get(\"x\", -1))\n",
    "        if 0 <= py < dispH and 0 <= px < dispW:\n",
    "            ax.scatter(px, py, c=\"#00FFFF\", marker=\"D\", s=135,\n",
    "                       edgecolors=\"black\", linewidths=0.6, zorder=7, clip_on=True)\n",
    "\n",
    "    # --- 8) styling ---",
    "    ax.set_title(\n",
    "        f\"Эпизод {ep} — исследовано: {int(np.count_nonzero(vmap))} клеток\",\n",
    "        fontsize=15, color=\"#EEEEEE\", pad=10\n",
    "    )\n",
    "    ax.grid(True, alpha=0.18, color=\"#444444\", linestyle=\":\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Компактные поля and сохранение",
    "    fig.subplots_adjust(left=0.02, right=0.985, top=0.93, bottom=0.02)\n",
    "    fig.savefig(save_path, dpi=220, bbox_inches=\"tight\", facecolor=\"black\", pad_inches=0.02)\n",
    "    plt.close(fig)\n",
    "    print(f\"Карта посещений сохранена: {save_path}\")\n",
    "    \n",
    "\n",
    "def visualize_team_map(ep, info, visited_map_team, predator_paths, save_path):\n",
    "    \"\"\"\n",
    "    Team Visit Heatmap + треки всех hunterов.\n",
    "    - visited_map_team: HxW, инкрементим на каждом шаге for КАЖДОГО hunterа\n",
    "    - predator_paths: список из K списков [(y,x), ...] for каждого hunterа\n",
    "    \"\"\"\n",
    "    # 0) размеры",
    "    Hv, Wv = visited_map_team.shape\n",
    "\n",
    "    raw_map = None\n",
    "    if (\"map\" in info) and (info[\"map\"] is not None):\n",
    "        raw_map = np.array(info[\"map\"])\n",
    "    elif hasattr(env_wrapper.base_env.realm.world, \"map\"):\n",
    "        raw_map = np.array(env_wrapper.base_env.realm.world.map)\n",
    "\n",
    "    if raw_map is not None and raw_map.ndim >= 2:\n",
    "        Hr, Wr = raw_map.shape[:2]\n",
    "    else:\n",
    "        Hr, Wr = Hv, Wv\n",
    "\n",
    "    dispH, dispW = min(Hv, Hr), min(Wv, Wr)\n",
    "    vmap = visited_map_team[:dispH, :dispW]\n",
    "\n",
    "    # цвета for треков hunterов (0..4)",
    "    track_colors = [\"#00FFFF\", \"#FF8C00\", \"#32CD32\", \"#FF1493\", \"#FFD700\"]\n",
    "\n",
    "    # 1) фигура",
    "    plt.style.use(\"dark_background\")\n",
    "    long_side = max(dispH, dispW)\n",
    "    scale = max(7.0, min(10.0, 6.8 * long_side / 20.0))\n",
    "    fig = plt.figure(figsize=(scale, scale), facecolor=\"black\")\n",
    "    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=[1.0, 0.045], wspace=0.035)\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    cax = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # 2) тепловая подложка visits КОМАНДОЙ",
    "    im = ax.imshow(vmap, cmap=\"plasma\", origin=\"upper\", interpolation=\"nearest\")\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.set_label(\"Посещения (команда)\", fontsize=14)\n",
    "    cax.set_facecolor(\"black\")\n",
    "    cax.tick_params(colors=\"white\", length=0)\n",
    "    for spine in cax.spines.values():\n",
    "        spine.set_color(\"black\")\n",
    "\n",
    "    # 3) walls",
    "    if raw_map is not None:\n",
    "        if raw_map.ndim == 3:\n",
    "            walls_full = np.logical_and(raw_map[:, :, 0] == -1, raw_map[:, :, 1] == -1)\n",
    "        else:\n",
    "            walls_full = (raw_map == -1)\n",
    "        walls = walls_full[:dispH, :dispW]\n",
    "        ax.imshow(np.ma.masked_where(~walls, walls),\n",
    "                  cmap=plt.cm.Greys_r, alpha=0.85, origin=\"upper\", zorder=1)\n",
    "        ax.contour(walls.astype(float), levels=[0.5],\n",
    "                   colors=\"#00FFFF\", linewidths=1.0, alpha=0.6, zorder=2)\n",
    "\n",
    "    # 4) пути всех hunterов (с разрывами по torusу)",
    "    for k, pathk in enumerate(predator_paths):\n",
    "        if not pathk or len(pathk) < 2:\n",
    "            continue\n",
    "        path = [(int(y), int(x)) for (y, x) in pathk if 0 <= y < dispH and 0 <= x < dispW]\n",
    "        if len(path) < 2:\n",
    "            continue\n",
    "        path = np.array(path, dtype=np.int32)\n",
    "        ys, xs = path[:, 0], path[:, 1]\n",
    "        seg_y, seg_x = [[ys[0]]], [[xs[0]]]\n",
    "        for i in range(1, len(xs)):\n",
    "            dx, dy = abs(xs[i] - xs[i-1]), abs(ys[i] - ys[i-1])\n",
    "            if dx > dispW // 2 or dy > dispH // 2:\n",
    "                seg_y.append([ys[i]]); seg_x.append([xs[i]])\n",
    "            else:\n",
    "                seg_y[-1].append(ys[i]); seg_x[-1].append(xs[i])\n",
    "        for sy, sx in zip(seg_y, seg_x):\n",
    "            if len(sx) > 1:\n",
    "                ax.plot(sx, sy, color=track_colors[k % len(track_colors)],\n",
    "                        linewidth=1.7, alpha=0.95, zorder=5, clip_on=True)\n",
    "\n",
    "        # пометки старта/финиша hunterа k",
    "        sy0, sx0 = path[0]\n",
    "        sy1, sx1 = path[-1]\n",
    "        ax.scatter([sx0], [sy0], c=track_colors[k % len(track_colors)],\n",
    "                   marker=\"o\", s=55, edgecolors=\"black\", linewidths=0.6, zorder=6)\n",
    "        ax.scatter([sx1], [sy1], c=track_colors[k % len(track_colors)],\n",
    "                   marker=\"D\", s=85, edgecolors=\"black\", linewidths=0.7, zorder=7)\n",
    "        ax.text(sx1 + 0.3, sy1 - 0.3, f\"{k}\", color=track_colors[k % len(track_colors)],\n",
    "                fontsize=12, weight=\"bold\", zorder=8)\n",
    "\n",
    "    # 5) preys",
    "    alive, dead = [], []\n",
    "    if \"preys\" in info:\n",
    "        for p in info[\"preys\"]:\n",
    "            y, x = int(p.get(\"y\", -1)), int(p.get(\"x\", -1))\n",
    "            if 0 <= y < dispH and 0 <= x < dispW:\n",
    "                (alive if p.get(\"alive\", False) else dead).append((y, x))\n",
    "    if alive:\n",
    "        ys, xs = zip(*alive)\n",
    "        ax.scatter(xs, ys, c=\"#7CFC00\", marker=\"o\", s=70,\n",
    "                   edgecolors=\"black\", linewidths=0.5, zorder=6, clip_on=True, label=\"alive\")\n",
    "    if dead:\n",
    "        ys, xs = zip(*dead)\n",
    "        ax.scatter(xs, ys, c=\"#FF6347\", marker=\"X\", s=90,\n",
    "                   edgecolors=\"black\", linewidths=0.6, zorder=6, clip_on=True, label=\"caught\")\n",
    "\n",
    "    # 6) styling",
    "    ax.set_xlim(-0.5, dispW - 0.5)\n",
    "    ax.set_ylim(dispH - 0.5, -0.5)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.autoscale(False)\n",
    "    explored = int(np.count_nonzero(vmap))\n",
    "    caught_total = sum(1 for p in info.get(\"preys\", []) if not p.get(\"alive\", True))\n",
    "    ax.set_title(\n",
    "        f\"Эпизод {ep} — Поймано: {caught_total} из {len(info.get('preys', []))}\",\n",
    "        fontsize=15, color=\"#EEEEEE\", pad=10\n",
    "    )\n",
    "    ax.grid(True, alpha=0.18, color=\"#444444\", linestyle=\":\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    fig.subplots_adjust(left=0.02, right=0.985, top=0.93, bottom=0.02)\n",
    "    fig.savefig(save_path, dpi=220, bbox_inches=\"tight\", facecolor=\"black\", pad_inches=0.02)\n",
    "    plt.close(fig)\n",
    "    print(f\"Командная карта посещений сохранена: {save_path}\")\n",
    "\n",
    "def display_side_by_side(gif_path, heatmap_path, width=256):\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
    "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
    "            <img src=\"{gif_path}\" width=\"{width}\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">",
    "        </div>\n",
    "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
    "            <img src=\"{heatmap_path}\" width=\"{width}\" style=\"border:1px solid #444; aspect-ratio:1/1;\">",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Награда and логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _team_potential_preys(info, H, W):\n",
    "    \"\"\"\n",
    "    Потенциал-команда: сумма torus-Manhattanских distances from КАЖДОГО нашего predatorа\n",
    "    to ближайшей живой preys (if нет жертв — 0). Чем меньше — тем лучше.\n",
    "    \"\"\"\n",
    "    preds = info.get(\"predators\", [])\n",
    "    preys = [q for q in info.get(\"preys\", []) if q.get(\"alive\", False)]\n",
    "    if not preds or not preys:\n",
    "        return 0.0\n",
    "\n",
    "    pot = 0.0\n",
    "    for pr in preds:\n",
    "        py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "        best = np.inf\n",
    "        for q in preys:\n",
    "            qy, qx = int(q[\"y\"]), int(q[\"x\"])\n",
    "            d = manhattan_torus(py, px, qy, qx, H, W)\n",
    "            if d < best:\n",
    "                best = d\n",
    "        pot += (0.0 if not np.isfinite(best) else float(best))\n",
    "    return float(pot)\n",
    "\n",
    "def compute_reward_static(prev_info, info, caught_preys,\n",
    "                          step_idx, episode_idx, visited_map, phase,\n",
    "                          actions_exec=None, actions_teacher=None):\n",
    "    \"\"\"\n",
    "    Возвращает: rewards[K], caught_preys\n",
    "    Логирование сохранено (одна строка на predatorа).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    preds = info.get(\"predators\", [])\n",
    "    K = len(preds)\n",
    "    rewards = np.zeros((K,), dtype=np.float32)\n",
    "\n",
    "    # базовый небольшой penalty всем",
    "    r_base = -0.01\n",
    "    rewards += r_base\n",
    "\n",
    "    # поимки на шаге (делим на всех)",
    "    prev_alive = {(p[\"y\"], p[\"x\"]) for p in prev_info[\"preys\"] if p[\"alive\"]} if prev_info is not None else set()\n",
    "    curr_alive = {(p[\"y\"], p[\"x\"]) for p in info[\"preys\"]  if p[\"alive\"]}\n",
    "    new_caught = prev_alive - curr_alive\n",
    "    new_caught -= caught_preys\n",
    "    r_capture_each = np.zeros((K,), dtype=np.float32)\n",
    "    if new_caught:\n",
    "        total_bonus = 10.0 * len(new_caught)\n",
    "        r_capture_each += (total_bonus / max(1, K))\n",
    "        rewards += r_capture_each\n",
    "        caught_preys |= new_caught\n",
    "\n",
    "    # подготовка maps",
    "    raw_map = None\n",
    "    if (\"map\" in info) and (info[\"map\"] is not None):\n",
    "        raw_map = np.array(info[\"map\"])\n",
    "    elif \"env_wrapper\" in globals() and hasattr(env_wrapper.base_env.realm.world, \"map\"):\n",
    "        raw_map = np.array(env_wrapper.base_env.realm.world.map)\n",
    "    if raw_map is None:\n",
    "        H = W = 0\n",
    "    else:\n",
    "        H, W = raw_map.shape[:2]\n",
    "\n",
    "    # потенциалы команды to/после",
    "    pot_prev = _team_potential_preys(prev_info, H, W) if (raw_map is not None and prev_info is not None) else 0.0\n",
    "    pot_curr = _team_potential_preys(info,     H, W) if (raw_map is not None) else 0.0\n",
    "\n",
    "    # вспомогательные",
    "    alive_preys_list = [q for q in info.get(\"preys\", []) if q.get(\"alive\", False)]\n",
    "    prev_preds = prev_info.get(\"predators\", []) if prev_info is not None else []\n",
    "    curr_coords = [(int(pr[\"y\"]), int(pr[\"x\"])) for pr in preds]\n",
    "    curr_dirs = {}\n",
    "\n",
    "    # компоненты-логи",
    "    r_explore_each = np.zeros((K,), dtype=np.float32)\n",
    "    r_revisit_each = np.zeros((K,), dtype=np.float32)\n",
    "    r_stand_each   = np.zeros((K,), dtype=np.float32)\n",
    "    r_bfs_each     = np.zeros((K,), dtype=np.float32)\n",
    "\n",
    "    # динамика repulsionа",
    "    alive_cnt = sum(1 for p in info.get(\"preys\", []) if p.get(\"alive\", False))\n",
    "    total_cnt = max(1, len(info.get(\"preys\", [])))\n",
    "    repulse_scale = 1.0 + repulse_boost * (1.0 - (alive_cnt / total_cnt))\n",
    "\n",
    "    # ход по каждому predatorу",
    "    for k, pr in enumerate(preds):\n",
    "        py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "\n",
    "        # стояние and направление",
    "        if prev_preds and k < len(prev_preds):\n",
    "            py0, px0 = int(prev_preds[k][\"y\"]), int(prev_preds[k][\"x\"])\n",
    "            if (py0, px0) == (py, px):\n",
    "                r_stand_each[k] = stand_still_penalty\n",
    "                rewards[k] -= r_stand_each[k]\n",
    "            curr_dirs[k] = (py - py0, px - px0)\n",
    "\n",
    "        # исследование / revisit",
    "        if 0 <= py and 0 <= px and visited_map.shape[0] > py and visited_map.shape[1] > px:\n",
    "            if visited_map[py, px] < 3:\n",
    "                r = exploration_coef / (1 + visited_map[py, px])\n",
    "                if step_idx < 20:\n",
    "                    r *= 1.4\n",
    "                r_explore_each[k] = r\n",
    "                rewards[k] += r_explore_each[k]\n",
    "            visited_map[py, px] += 1\n",
    "            if visited_map[py, px] > 5:\n",
    "                r_revisit_each[k] = revisit_penalty\n",
    "                rewards[k] -= r_revisit_each[k]\n",
    "\n",
    "        # Δдистанции to ближайшей preys (torus-L1)",
    "        if alive_preys_list and raw_map is not None:\n",
    "            best_d = float(\"inf\"); best_yx = None\n",
    "            for q in alive_preys_list:\n",
    "                qy, qx = int(q[\"y\"]), int(q[\"x\"])\n",
    "                d = manhattan_torus(py, px, qy, qx, H, W)\n",
    "                if d < best_d:\n",
    "                    best_d = d; best_yx = (qy, qx)\n",
    "            if prev_preds and k < len(prev_preds) and best_yx is not None and np.isfinite(best_d):\n",
    "                py0, px0 = int(prev_preds[k][\"y\"]), int(prev_preds[k][\"x\"])\n",
    "                old_d = manhattan_torus(py0, px0, best_yx[0], best_yx[1], H, W)\n",
    "                new_d = best_d\n",
    "                r_bfs_each[k] = shaping_coef * float(old_d - new_d)\n",
    "                rewards[k] += r_bfs_each[k]\n",
    "\n",
    "        # bonus — только if не вредим прогрессу (pot не вырос)",
    "        if raw_map is not None and 0 <= py < H and 0 <= px < W:\n",
    "            is_bonus = False\n",
    "            if raw_map.ndim == 3:\n",
    "                is_bonus = (raw_map[py, px, 0] == -3) or (raw_map[py, px, -1] == -3)\n",
    "            else:\n",
    "                is_bonus = (raw_map[py, px] == -3)\n",
    "            if is_bonus:\n",
    "                key = (py, px)\n",
    "                if key not in BONUS_TAKEN and (pot_curr <= pot_prev + 1e-6):\n",
    "                    late_scale = 1.0 + 0.5*(1.0 - alive_cnt/total_cnt)\n",
    "                    rewards[k] += (bonus_reward_base * late_scale)\n",
    "                    BONUS_TAKEN.add(key)\n",
    "\n",
    "    # repulsion/стадность (после ранней фазы) с кэпом",
    "    r_repulse_total = np.zeros((K,), dtype=np.float32)\n",
    "    if step_idx >= REPULSE_KICKS_IN_AFTER:\n",
    "        for i in range(K):\n",
    "            yi, xi = curr_coords[i]\n",
    "            for j in range(i + 1, K):\n",
    "                yj, xj = curr_coords[j]\n",
    "                d = abs(yi - yj) + abs(xi - xj)\n",
    "                if d == 0:\n",
    "                    r = repulse_same_cell * repulse_scale\n",
    "                elif d == 1:\n",
    "                    r = repulse_adjacent * repulse_scale\n",
    "                elif d == 2:\n",
    "                    r = repulse_radius2 * repulse_scale\n",
    "                else:\n",
    "                    r = 0.0\n",
    "                if r > 0:\n",
    "                    r_repulse_total[i] += r; r_repulse_total[j] += r\n",
    "        r_repulse_total = np.minimum(r_repulse_total, REPULSE_CAP_PER_AGENT)\n",
    "        rewards -= r_repulse_total\n",
    "\n",
    "    # anti-flipflop / same-dir-close — только if нет прогресса по потенциалу",
    "    r_flipflop_each = np.zeros((K,), dtype=np.float32)\n",
    "    r_same_dir_close_each = np.zeros((K,), dtype=np.float32)\n",
    "    progress_ok = (pot_curr <= pot_prev + 1e-6)\n",
    "    if prev_preds and not progress_ok:\n",
    "        for k in range(K):\n",
    "            dy, dx = curr_dirs.get(k, (0, 0))\n",
    "            last = LAST_DIRS.get(k, (0, 0))\n",
    "            if (dy, dx) != (0, 0) and (dy == -last[0] and dx == -last[1]):\n",
    "                r_flipflop_each[k] = flipflop_penalty\n",
    "        rewards -= r_flipflop_each\n",
    "\n",
    "        for i in range(K):\n",
    "            yi, xi = curr_coords[i]; vi = curr_dirs.get(i, (0, 0))\n",
    "            for j in range(i + 1, K):\n",
    "                yj, xj = curr_coords[j]\n",
    "                if abs(yi - yj) + abs(xi - xj) <= 2:\n",
    "                    vj = curr_dirs.get(j, (0, 0))\n",
    "                    if vi != (0, 0) and vi == vj:\n",
    "                        r_same = same_dir_close_penalty * 0.5\n",
    "                        r_same_dir_close_each[i] += r_same\n",
    "                        r_same_dir_close_each[j] += r_same\n",
    "        rewards -= r_same_dir_close_each\n",
    "\n",
    "    # обновить кэш направлений",
    "    for k, v in curr_dirs.items():\n",
    "        LAST_DIRS[k] = v\n",
    "\n",
    "    # === ЛОГИ (as у тебя) ===",
    "    idle_preds = 0\n",
    "    if prev_preds:\n",
    "        prev_coords = [(int(pr[\"y\"]), int(pr[\"x\"])) for pr in prev_preds]\n",
    "        for (py0, px0), (py1, px1) in zip(prev_coords, curr_coords):\n",
    "            if (py0, px0) == (py1, px1):\n",
    "                idle_preds += 1\n",
    "\n",
    "    pair_d0 = 0; pair_d1 = 0; sum_d = 0.0; pairs = 0\n",
    "    for i in range(K):\n",
    "        for j in range(i + 1, K):\n",
    "            yi, xi = curr_coords[i]; yj, xj = curr_coords[j]\n",
    "            d = abs(yi - yj) + abs(xi - xj)\n",
    "            if d == 0: pair_d0 += 1\n",
    "            elif d == 1: pair_d1 += 1\n",
    "            sum_d += d; pairs += 1\n",
    "    team_disp = (sum_d / max(1, pairs)) if pairs else 0.0\n",
    "\n",
    "    alive_preys_cnt = sum(1 for q in info.get(\"preys\", []) if q.get(\"alive\", False))\n",
    "    prev_score = (prev_info[\"scores\"][0] if (prev_info is not None and \"scores\" in prev_info) else 0)\n",
    "    curr_score = (info[\"scores\"][0] if \"scores\" in info else 0)\n",
    "    score_delta = curr_score - prev_score\n",
    "\n",
    "    exec_list  = list(actions_exec)    if actions_exec    is not None else [-1] * K\n",
    "    teach_list = list(actions_teacher) if actions_teacher is not None else [-1] * K\n",
    "\n",
    "    for k, pr in enumerate(preds):\n",
    "        py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "        # ближайшая жертва for логов",
    "        ny = nx = -1; dy = dx = 0.0\n",
    "        best_d = float(\"inf\")\n",
    "        for q in alive_preys_list:\n",
    "            qy, qx = int(q[\"y\"]), int(q[\"x\"])\n",
    "            d = manhattan_torus(py, px, qy, qx, H, W)\n",
    "            if d < best_d:\n",
    "                best_d = d; ny, nx = qy, qx; dy = float(ny - py); dx = float(nx - px)\n",
    "\n",
    "        STEP_LOG_BUFFER.append([\n",
    "            phase, int(episode_idx), int(step_idx),\n",
    "            int(k), int(py), int(px),\n",
    "            int(exec_list[k]) if k < len(exec_list) else -1,\n",
    "            int(teach_list[k]) if k < len(teach_list) else -1,\n",
    "            int(alive_preys_cnt), int(len(caught_preys)), int(len(new_caught)),\n",
    "            float(curr_score), float(score_delta),\n",
    "            float(rewards[k]), float(r_base),\n",
    "            float(r_capture_each[k] if k < len(r_capture_each) else 0.0),\n",
    "            float(r_explore_each[k]), float(r_stand_each[k]), float(r_revisit_each[k]),\n",
    "            float(r_bfs_each[k]), float(r_repulse_total[k]),\n",
    "            float(r_flipflop_each[k]), float(r_same_dir_close_each[k]),\n",
    "            int(idle_preds), int(pair_d0), int(pair_d1), float(team_disp),\n",
    "            int(ny), int(nx), float(dy), float(dx),\n",
    "            list(new_caught)\n",
    "        ])\n",
    "\n",
    "    return rewards, caught_preys\n",
    "\n",
    "\n",
    "def compute_reward_pvp(prev_info, info, caught_preys, step_idx, episode_idx, visited_map, phase,\n",
    "                       actions_exec=None, actions_teacher=None):\n",
    "    \"\"\"\n",
    "    PvP-надстройка поверх базовых наград.\n",
    "    \"\"\"\n",
    "    rewards, caught_preys = compute_reward_static(prev_info, info, caught_preys,\n",
    "                                                  step_idx, episode_idx, visited_map, phase,\n",
    "                                                  actions_exec, actions_teacher)\n",
    "    preds = info.get(\"predators\", [])\n",
    "    enemies = info.get(\"enemies\", []) or info.get(\"enemy\", [])\n",
    "    if not preds or not enemies:\n",
    "        return rewards, caught_preys\n",
    "\n",
    "    raw_map = None\n",
    "    if (\"map\" in info) and (info[\"map\"] is not None):\n",
    "        raw_map = np.array(info[\"map\"])\n",
    "    elif \"env_wrapper\" in globals() and hasattr(env_wrapper.base_env.realm.world, \"map\"):\n",
    "        raw_map = np.array(env_wrapper.base_env.realm.world.map)\n",
    "    if raw_map is None:\n",
    "        return rewards, caught_preys\n",
    "    H, W = raw_map.shape[:2]\n",
    "\n",
    "    # перехват противника: сокращаем torus-дистанцию to ближ. врага",
    "    r_intercept = 0.005\n",
    "    prev_preds = prev_info.get(\"predators\", []) if prev_info is not None else []\n",
    "    for k, pr in enumerate(preds):\n",
    "        py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "        best_d = min(manhattan_torus(py, px, int(en[\"y\"]), int(en[\"x\"]), H, W) for en in enemies)\n",
    "        if prev_preds and k < len(prev_preds):\n",
    "            py0, px0 = int(prev_preds[k][\"y\"]), int(prev_preds[k][\"x\"])\n",
    "            best_d0 = min(manhattan_torus(py0, px0, int(en[\"y\"]), int(en[\"x\"]), H, W) for en in enemies)\n",
    "            if np.isfinite(best_d) and np.isfinite(best_d0):\n",
    "                rewards[k] += r_intercept * float(best_d0 - best_d)\n",
    "\n",
    "    # гонка к нашей ближайшей жертве: плохо, if враг явнo ближе",
    "    r_race_penalty = 0.004\n",
    "    alive_preys = [q for q in info.get(\"preys\", []) if q.get(\"alive\", False)]\n",
    "    for k, pr in enumerate(preds):\n",
    "        py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "        my_best = (float(\"inf\"), None)\n",
    "        for q in alive_preys:\n",
    "            qy, qx = int(q[\"y\"]), int(q[\"x\"])\n",
    "            d = manhattan_torus(py, px, qy, qx, H, W)\n",
    "            if d < my_best[0]:\n",
    "                my_best = (d, (qy, qx))\n",
    "        if my_best[1] is None:\n",
    "            continue\n",
    "        qy, qx = my_best[1]\n",
    "        enemy_best = min(manhattan_torus(int(en[\"y\"]), int(en[\"x\"]), qy, qx, H, W) for en in enemies)\n",
    "        if np.isfinite(my_best[0]) and np.isfinite(enemy_best) and (enemy_best + 1 < my_best[0]):\n",
    "            rewards[k] -= r_race_penalty\n",
    "\n",
    "        # маленькое «gate-keeping»: стоим у preys ближе, чем враг",
    "        r_gate_keep = 0.003\n",
    "        if my_best[0] <= 1 and enemy_best > my_best[0] + 0.5:\n",
    "            rewards[k] += r_gate_keep\n",
    "\n",
    "    return rewards, caught_preys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PvP окружение против бота ClosestTarget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PvP окружение против бота ClosestTarget на МИКСЕ карт ===",
    "# Использует VersusBotEnv, чтобы шагать ОДНИМ списком actions (бот живёт в realm.bots)",
    "BEST_PVP = None\n",
    "TEAM_SIZE = 5\n",
    "BEST_PVP_EVAL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()",
    "def train_dagger_multi(agent: NetAgentShared, env_wrapper,\n",
    "                       episodes=24,\n",
    "                       beta_start=0.90, beta_end=0.05, render_every=6,\n",
    "                       seed_base: int = 12345):\n",
    "    \"\"\"\n",
    "    DAgger на смешанных картах (solo):\n",
    "      • sticky-DAgger (учитель/студент),\n",
    "      • τ-врата уверенности + короткий action-repeat,\n",
    "      • лёгкая стохастика ученика (EPS),\n",
    "      • anti-разворот на первом тике после отхода from учителя,\n",
    "      • anti-idle + action-mask + anti «одна сtorusона»,\n",
    "      • логи поведенческих метрик.\n",
    "    Учитель используется только as оракул (лейблы).\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    teacher = AssignedClosestTargetAgent(num_predators=agent.team_size)\n",
    "\n",
    "    beta_decay = 1.0 if episodes <= 1 else (beta_end / beta_start) ** (1.0 / (episodes - 1))\n",
    "    beta = beta_start\n",
    "\n",
    "    DIRS = {0:(0,0), 1:(0,1), 2:(0,-1), 3:(-1,0), 4:(1,0)}  # 0=stay,1=→,2=←,3=↑,4=↓",
    "\n",
    "    pbar = tqdm(range(episodes), desc=\"DAgger-5\", leave=True)\n",
    "    for ep in pbar:\n",
    "        # сброс кэша направлений (for flip/same) and bonusов",
    "        global LAST_DIRS, BONUS_TAKEN\n",
    "        LAST_DIRS = {}\n",
    "        BONUS_TAKEN = set()\n",
    "\n",
    "        # новая map/сид",
    "        seed_ep = seed_base + ep\n",
    "        random.seed(seed_ep); np.random.seed(seed_ep)\n",
    "\n",
    "        base_env = getattr(env_wrapper, \"base_env\", None)\n",
    "        can_seed_base = False\n",
    "        if base_env is not None and hasattr(base_env, \"reset\"):\n",
    "            try:\n",
    "                if \"seed\" in inspect.signature(base_env.reset).parameters:\n",
    "                    state, info = base_env.reset(seed=seed_ep)\n",
    "                    env_wrapper.frames = []\n",
    "                    env_wrapper.last_info = info\n",
    "                    can_seed_base = True\n",
    "            except Exception:\n",
    "                can_seed_base = False\n",
    "        if not can_seed_base:\n",
    "            state, info = env_wrapper.reset()\n",
    "\n",
    "        teacher.reset(state, team=0)\n",
    "\n",
    "        # map/passability (фиксируем один раз; if у тебя map статична в эпизоде — этого достаточно)",
    "        world_map = env_wrapper.base_env.realm.world.map\n",
    "        wm_np = np.array(world_map)\n",
    "        if wm_np.ndim == 3:\n",
    "            walls = (wm_np[:, :, 0] == -1) & (wm_np[:, :, 1] == -1)\n",
    "        else:\n",
    "            walls = (wm_np == -1)\n",
    "        H, W = walls.shape\n",
    "        def passable(y, x): return not walls[y % H, x % W]\n",
    "\n",
    "        # аккумулируем визиты/пути",
    "        visited_map_ep   = np.zeros((H, W), dtype=np.int32)\n",
    "        visited_map_team = np.zeros((H, W), dtype=np.int32)\n",
    "        predator_paths   = [[] for _ in range(agent.team_size)]\n",
    "        caught_preys     = set()\n",
    "\n",
    "        # sticky/τ/repeat/idle",
    "        STICK_K  = 4\n",
    "        TAU_CONF = 0.60\n",
    "        KEEP_K   = 1\n",
    "        src_hold = np.zeros(agent.team_size, dtype=np.int32)\n",
    "        src_is_teacher = np.ones(agent.team_size, dtype=bool)\n",
    "        keep_hold = np.zeros(agent.team_size, dtype=np.int32)\n",
    "        prev_exec_actions = [0] * agent.team_size\n",
    "        stay_streak = [0] * agent.team_size\n",
    "\n",
    "        # поведенческие метрики",
    "        cluster_steps = idle_steps = 0\n",
    "        max_idle_streak = _idle_streak = 0\n",
    "        prev_pred_coords = None\n",
    "        flip_events = same_dir_events = 0\n",
    "        last_dirs_ep = [(0,0)] * agent.team_size\n",
    "\n",
    "        # anti-«одна сtorusона»",
    "        repeat_cnt = [0] * agent.team_size\n",
    "        last_action_seen = [None] * agent.team_size\n",
    "\n",
    "        bc_losses = []\n",
    "        first_capture_step = None\n",
    "\n",
    "        # утилиты под расстояния to ближайшей targets",
    "        def nearest_prey_dist(py, px):\n",
    "            preys_alive = [q for q in info[\"preys\"] if q.get(\"alive\", False)]\n",
    "            if not preys_alive: \n",
    "                return 0.0\n",
    "            return min(manhattan_torus(py, px, int(t[\"y\"]), int(t[\"x\"]), H, W) for t in preys_alive)\n",
    "\n",
    "        def best_step_toward_nearest(k: int) -> int:\n",
    "            \"\"\"Жадно сокращаем torus-L1 to ближайшей живой preys из ДОПУСТИМЫХ ходов.\"\"\"\n",
    "            preys_alive = [q for q in info[\"preys\"] if q.get(\"alive\", False)]\n",
    "            if not preys_alive:\n",
    "                return 0\n",
    "            py = int(info[\"predators\"][k][\"y\"]); px = int(info[\"predators\"][k][\"x\"])\n",
    "            best_a, best_d = 0, float(\"inf\")\n",
    "            for a, (dy, dx) in DIRS.items():\n",
    "                ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                if not passable(ny, nx): \n",
    "                    continue\n",
    "                d = min(manhattan_torus(ny, nx, int(t[\"y\"]), int(t[\"x\"]), H, W) for t in preys_alive)\n",
    "                if d < best_d:\n",
    "                    best_d, best_a = d, a\n",
    "            return best_a\n",
    "\n",
    "        # основной цикл эпизода",
    "        for step in range(env_wrapper.base_env.realm.step_limit):\n",
    "            # 1) действия учителя",
    "            t_actions = teacher.get_actions(state, team=0)\n",
    "\n",
    "            # 2) BC",
    "            loss, feats_batch, y_batch = agent.train_step_bc_multi(info, world_map, t_actions)\n",
    "            bc_losses.append(loss)\n",
    "            agent.add_replay(feats_batch, y_batch)\n",
    "            if beta > 0.20 or (step % 5) == 0:\n",
    "                agent.replay_step(steps=1, batch=128)\n",
    "\n",
    "            # 3) микширование",
    "            exec_actions = [0] * agent.team_size\n",
    "\n",
    "            # один общий вызов ученика",
    "            student_actions = None\n",
    "            student_logprobs = None\n",
    "            use_student_mask = (np.random.rand() >= beta)\n",
    "            if use_student_mask:\n",
    "                student_actions, student_logprobs, _, _ = agent.get_actions(\n",
    "                    info, world_map, training=True, greedy=False\n",
    "                )\n",
    "\n",
    "            # лёгкая стохастика ученика (exploration)",
    "            EPS = 0.03\n",
    "            if use_student_mask and (np.random.rand() < EPS) and student_actions is not None:\n",
    "                k_rand = np.random.randint(0, agent.team_size)\n",
    "                py = int(info[\"predators\"][k_rand][\"y\"])\n",
    "                px = int(info[\"predators\"][k_rand][\"x\"])\n",
    "                legal = []\n",
    "                for a, (dy, dx) in DIRS.items():\n",
    "                    ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                    if a == 0 or passable(ny, nx):\n",
    "                        legal.append(a)\n",
    "                student_actions[k_rand] = int(np.random.choice(legal if legal else list(DIRS.keys())))\n",
    "\n",
    "            for k in range(agent.team_size):\n",
    "                if src_hold[k] <= 0:\n",
    "                    src_is_teacher[k] = (np.random.rand() < beta)\n",
    "                    src_hold[k] = STICK_K\n",
    "\n",
    "                if src_is_teacher[k]:\n",
    "                    a = int(t_actions[k])\n",
    "                    exec_actions[k] = a\n",
    "                    keep_hold[k] = 0  # сброс удержания, т.к. действие from учителя",
    "                else:\n",
    "                    if student_actions is None:\n",
    "                        sa, slp, _, _ = agent.get_actions(info, world_map, training=True, greedy=False)\n",
    "                        cand = int(sa[k]); lp = float(slp[k].max().detach().cpu().item())\n",
    "                    else:\n",
    "                        cand = int(student_actions[k])\n",
    "                        lp = float(student_logprobs[k].max().detach().cpu().item())\n",
    "\n",
    "                    prob = float(np.exp(lp))\n",
    "                    if (prob < TAU_CONF) or (keep_hold[k] > 0):\n",
    "                        exec_actions[k] = prev_exec_actions[k]\n",
    "                        keep_hold[k] = max(0, keep_hold[k] - 1)\n",
    "                    else:\n",
    "                        exec_actions[k] = cand\n",
    "                        keep_hold[k] = KEEP_K\n",
    "\n",
    "                src_hold[k] -= 1\n",
    "\n",
    "            # 3.1 anti-разворот в момент отхода from учителя",
    "            def _is_reverse(v_new, v_last):\n",
    "                return (v_new[0] == -v_last[0]) and (v_new[1] == -v_last[1]) and (v_new != (0,0))\n",
    "            for k in range(agent.team_size):\n",
    "                just_switched = (not src_is_teacher[k]) and (src_hold[k] == STICK_K-1)\n",
    "                if not just_switched: \n",
    "                    continue\n",
    "                v_last = last_dirs_ep[k] if k < len(last_dirs_ep) else (0,0)\n",
    "                v_new  = DIRS.get(exec_actions[k], (0,0))\n",
    "                if _is_reverse(v_new, v_last):\n",
    "                    alt = int(t_actions[k])\n",
    "                    if not _is_reverse(DIRS.get(alt,(0,0)), v_last):\n",
    "                        exec_actions[k] = alt\n",
    "                        src_is_teacher[k] = True\n",
    "                        src_hold[k] = max(src_hold[k], 1)\n",
    "                        keep_hold[k] = 0\n",
    "                    else:\n",
    "                        exec_actions[k] = 0\n",
    "                        keep_hold[k] = 1  # постоим тик",
    "\n",
    "            # 3.2 anti-idle (if стоим ≥3 тиков — форсим альтернативу)",
    "            for k in range(agent.team_size):\n",
    "                if exec_actions[k] == 0:\n",
    "                    stay_streak[k] += 1\n",
    "                else:\n",
    "                    stay_streak[k] = 0\n",
    "\n",
    "                if stay_streak[k] >= 3:\n",
    "                    alt = int(t_actions[k]) if int(t_actions[k]) != 0 else best_step_toward_nearest(k)\n",
    "                    if alt != 0:\n",
    "                        exec_actions[k] = alt\n",
    "                        src_is_teacher[k] = True\n",
    "                        src_hold[k] = max(src_hold[k], 1)\n",
    "                        keep_hold[k] = 0\n",
    "                        stay_streak[k] = 0\n",
    "\n",
    "            # 3.3 ACTION MASK: не шагаем в стену + anti «одна сtorusона»",
    "            for k in range(agent.team_size):\n",
    "                a = int(exec_actions[k])\n",
    "                dy, dx = DIRS.get(a, (0,0))\n",
    "                py = int(info[\"predators\"][k][\"y\"]); px = int(info[\"predators\"][k][\"x\"])\n",
    "                ny, nx = (py + dy) % H, (px + dx) % W\n",
    "\n",
    "                # (а) if step в стену — заменяем (сначала на учителя, else greedy к targets)",
    "                if a != 0 and not passable(ny, nx):\n",
    "                    a_t = int(t_actions[k])\n",
    "                    dy_t, dx_t = DIRS.get(a_t, (0,0))\n",
    "                    ny_t, nx_t = (py + dy_t) % H, (px + dx_t) % W\n",
    "                    if a_t != 0 and passable(ny_t, nx_t):\n",
    "                        exec_actions[k] = a_t\n",
    "                        src_is_teacher[k] = True\n",
    "                        src_hold[k] = max(src_hold[k], 1)\n",
    "                        keep_hold[k] = 0\n",
    "                    else:\n",
    "                        exec_actions[k] = best_step_toward_nearest(k)\n",
    "\n",
    "                # (б) anti «одна сtorusона»: одно and то же действие N раз без прогресса — nudging",
    "                a = int(exec_actions[k])  # мог поменяться",
    "                if last_action_seen[k] == a:\n",
    "                    repeat_cnt[k] += 1\n",
    "                else:\n",
    "                    repeat_cnt[k] = 1\n",
    "                    last_action_seen[k] = a\n",
    "\n",
    "                N_REPEAT = 6\n",
    "                if repeat_cnt[k] >= N_REPEAT and a != 0:\n",
    "                    py = int(info[\"predators\"][k][\"y\"]); px = int(info[\"predators\"][k][\"x\"])\n",
    "                    d0 = nearest_prey_dist(py, px)\n",
    "                    dy, dx = DIRS.get(a, (0,0))\n",
    "                    ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                    d1 = nearest_prey_dist(ny, nx)\n",
    "                    if not passable(ny, nx) or d1 >= d0:\n",
    "                        alt = int(t_actions[k])\n",
    "                        dyt, dxt = DIRS.get(alt, (0,0))\n",
    "                        nyt, nxt = (py + dyt) % H, (px + dxt) % W\n",
    "                        if alt != 0 and passable(nyt, nxt):\n",
    "                            exec_actions[k] = alt\n",
    "                            src_is_teacher[k] = True\n",
    "                            src_hold[k] = max(src_hold[k], 1)\n",
    "                            keep_hold[k] = 0\n",
    "                        else:\n",
    "                            exec_actions[k] = best_step_toward_nearest(k)\n",
    "                        repeat_cnt[k] = 0  # сброс после вмешательства",
    "\n",
    "            # 3.4 зафиксировать прошлые фактические действия (после всех правок)",
    "            for k in range(agent.team_size):\n",
    "                prev_exec_actions[k] = exec_actions[k]\n",
    "\n",
    "            # 4) step среды",
    "            state, done, new_info = env_wrapper.step(exec_actions)\n",
    "\n",
    "            # --- поведенческие метрики ---",
    "            coords = [(int(pr[\"y\"]), int(pr[\"x\"])) for pr in new_info[\"predators\"]]\n",
    "            idle_now = 0\n",
    "            if prev_pred_coords is not None:\n",
    "                for (py0, px0), (py1, px1) in zip(prev_pred_coords, coords):\n",
    "                    if (py0, px0) == (py1, px1):\n",
    "                        idle_now += 1\n",
    "            pair0 = pair1 = 0; sumd = 0.0; pairs = 0\n",
    "            for i in range(len(coords)):\n",
    "                for j in range(i + 1, len(coords)):\n",
    "                    yi, xi = coords[i]; yj, xj = coords[j]\n",
    "                    d = abs(yi - yj) + abs(xi - xj)\n",
    "                    if d == 0: pair0 += 1\n",
    "                    elif d == 1: pair1 += 1\n",
    "                    sumd += d; pairs += 1\n",
    "            disp = (sumd / max(1, pairs))\n",
    "            if (disp <= 1.5) or (pair0 > 0) or (pair1 >= 2):\n",
    "                cluster_steps += 1\n",
    "            if idle_now >= (agent.team_size - 1):\n",
    "                idle_steps += 1; _idle_streak += 1\n",
    "            else:\n",
    "                _idle_streak = 0\n",
    "            max_idle_streak = max(max_idle_streak, _idle_streak)\n",
    "\n",
    "            # flip/same",
    "            dirs_now = []\n",
    "            if prev_pred_coords is not None:\n",
    "                for k, (y1, x1) in enumerate(coords):\n",
    "                    y0, x0 = prev_pred_coords[k]\n",
    "                    dirs_now.append((y1 - y0, x1 - x0))\n",
    "                for k, (dy, dx) in enumerate(dirs_now):\n",
    "                    if (dy, dx) != (0,0) and (dy, dx) == (-last_dirs_ep[k][0], -last_dirs_ep[k][1]):\n",
    "                        flip_events += 1\n",
    "                for i in range(len(coords)):\n",
    "                    for j in range(i+1, len(coords)):\n",
    "                        d = abs(coords[i][0]-coords[j][0]) + abs(coords[i][1]-coords[j][1])\n",
    "                        if d <= 2 and i < len(dirs_now) and j < len(dirs_now):\n",
    "                            if dirs_now[i] != (0,0) and dirs_now[i] == dirs_now[j]:\n",
    "                                same_dir_events += 1\n",
    "                for k in range(len(dirs_now)):\n",
    "                    last_dirs_ep[k] = dirs_now[k]\n",
    "            else:\n",
    "                for k in range(len(last_dirs_ep)):\n",
    "                    last_dirs_ep[k] = (0,0)\n",
    "\n",
    "            prev_pred_coords = coords\n",
    "\n",
    "            # 5) треки/посещения",
    "            preds = new_info[\"predators\"]\n",
    "            for k, pr in enumerate(preds):\n",
    "                py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "                predator_paths[k].append((py, px))\n",
    "                if 0 <= py < H and 0 <= px < W:\n",
    "                    visited_map_team[py, px] += 1\n",
    "\n",
    "            # 6) награда/логирование (for аналитики)",
    "            _, caught_preys = compute_reward_static(\n",
    "                prev_info=info,\n",
    "                info=new_info,\n",
    "                caught_preys=caught_preys,\n",
    "                step_idx=step,\n",
    "                episode_idx=ep,\n",
    "                visited_map=visited_map_ep,\n",
    "                phase=\"BC-DAgger\",\n",
    "                actions_exec=exec_actions,\n",
    "                actions_teacher=t_actions\n",
    "            )\n",
    "\n",
    "            # первая поимка",
    "            caught_now = sum(1 for p in new_info[\"preys\"] if not p.get(\"alive\", True))\n",
    "            if first_capture_step is None and caught_now > 0:\n",
    "                first_capture_step = step\n",
    "\n",
    "            info = new_info\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # конец эпизода",
    "        beta = max(beta_end, beta * beta_decay)\n",
    "\n",
    "        # запись буфера шагов",
    "        if STEP_LOG_BUFFER:\n",
    "            with open(LOG_STEP_PATH, \"a\", newline=\"\") as f:\n",
    "                csv.writer(f).writerows(STEP_LOG_BUFFER)\n",
    "            STEP_LOG_BUFFER.clear()\n",
    "\n",
    "        caught_total = sum(1 for p in info[\"preys\"] if not p.get(\"alive\", True))\n",
    "        avg_bc = float(np.mean(bc_losses)) if bc_losses else 0.0\n",
    "\n",
    "        # визуал по расписанию",
    "        if render_every != 0 and ((ep + 1) % render_every == 0 or ep == 0 or ep == episodes - 1):\n",
    "            gif_path = os.path.join(FRAME_DIR, f\"dagger5_ep_{ep:03d}.gif\")\n",
    "            team_map_path = os.path.join(MAP_DIR, f\"dagger5_team_ep{ep:03d}.png\")\n",
    "            make_color_gif(env_wrapper, gif_path, resize_factor=10, fps=8)\n",
    "            visualize_team_map(ep, info, visited_map_team, predator_paths, team_map_path)\n",
    "            display_side_by_side(gif_path, team_map_path, width=300)\n",
    "\n",
    "        cluster_pct = cluster_steps / max(1, step + 1)\n",
    "        idle_pct    = idle_steps    / max(1, step + 1)\n",
    "        flip_rate   = flip_events   / max(1, step + 1)\n",
    "        same_rate   = same_dir_events / max(1, step + 1)\n",
    "\n",
    "        # мягкий decay LR к концу DAgger",
    "        if beta <= 0.20:\n",
    "            for pg in agent.optimizer.param_groups:\n",
    "                pg[\"lr\"] = max(pg[\"lr\"] * 0.95, LR * 0.3)\n",
    "\n",
    "        print(f\"[DAgger-5] ep={ep:03d} finished at step={step:03d}, beta={beta:.3f}, \"\n",
    "              f\"caught={caught_total:03d}, first_cap={first_capture_step}, bc={avg_bc:.4f}, \"\n",
    "              f\"cluster={cluster_pct:.2f}, idle={idle_pct:.2f}, flip={flip_rate:.2f}, same={same_rate:.2f}\")\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"beta\":   f\"{beta:.3f}\",\n",
    "            \"caught\": f\"{caught_total:03d}\",\n",
    "            \"first\":  first_capture_step if first_capture_step is not None else \"-\",\n",
    "            \"bc\":     f\"{avg_bc:.3f}\",\n",
    "            \"clu\":    f\"{cluster_pct:.2f}\",\n",
    "            \"idle\":   f\"{idle_pct:.2f}\",\n",
    "            \"flip\":   f\"{flip_rate:.2f}\",\n",
    "            \"same\":   f\"{same_rate:.2f}\",\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция обучения PvP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dagger_pvp(agent: NetAgentShared, env_wrapper,\n",
    "                     episodes=60,\n",
    "                     beta_start=0.60, beta_end=0.05, render_every=10,\n",
    "                     seed_base: int = 98765):\n",
    "    \"\"\"\n",
    "    DAgger в двухкомандной среде (наш агент vs ClosestTarget на врагах).\n",
    "    Фичи: sticky-DAgger, τ-врата уверенности, короткий action-repeat,\n",
    "    anti-разворот, anti-idle, action-mask, лёгкая стохастика ученика.\n",
    "    Печатает счёт and результат (WIN/DRAW/LOSS) «мы-бот».\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    teacher = AssignedClosestTargetAgent(num_predators=agent.team_size)\n",
    "\n",
    "    beta_decay = 1.0 if episodes <= 1 else (beta_end / beta_start) ** (1.0 / (episodes - 1))\n",
    "    beta = beta_start\n",
    "\n",
    "    pbar = tqdm(range(episodes), desc=\"DAgger-PvP\", leave=True)\n",
    "    for ep in pbar:\n",
    "        # разнообразие карт/сидов",
    "        seed_ep = seed_base + ep\n",
    "        random.seed(seed_ep); np.random.seed(seed_ep)\n",
    "\n",
    "        # reset среды с сидом, if поддерживается",
    "        base_env = getattr(env_wrapper, \"base_env\", None)\n",
    "        did = False\n",
    "        if base_env is not None and hasattr(base_env, \"reset\"):\n",
    "            try:\n",
    "                if \"seed\" in inspect.signature(base_env.reset).parameters:\n",
    "                    state, info = base_env.reset(seed=seed_ep)\n",
    "                    env_wrapper.frames = []\n",
    "                    env_wrapper.last_info = info\n",
    "                    did = True\n",
    "            except Exception:\n",
    "                did = False\n",
    "        if not did:\n",
    "            state, info = env_wrapper.reset()\n",
    "\n",
    "        # кэши/глобали",
    "        global LAST_DIRS, BONUS_TAKEN\n",
    "        LAST_DIRS = {}\n",
    "        BONUS_TAKEN = set()\n",
    "\n",
    "        teacher.reset(state, team=0)\n",
    "\n",
    "        world_map = env_wrapper.base_env.realm.world.map\n",
    "        H, W = np.array(world_map).shape[:2]\n",
    "        visited_map_ep   = np.zeros((H, W), dtype=np.int32)\n",
    "        visited_map_team = np.zeros((H, W), dtype=np.int32)\n",
    "        predator_paths   = [[] for _ in range(agent.team_size)]\n",
    "        caught_preys     = set()\n",
    "\n",
    "        # метрики поведения",
    "        cluster_steps = idle_steps = 0\n",
    "        max_idle_streak = _idle_streak = 0\n",
    "        prev_pred_coords = None\n",
    "        flip_events = same_dir_events = 0\n",
    "        last_dirs_ep = [(0,0)] * agent.team_size\n",
    "\n",
    "        # Sticky/τ/repeat/idle",
    "        STICK_K  = 4\n",
    "        TAU_CONF = 0.60\n",
    "        KEEP_K   = 1\n",
    "        src_hold = np.zeros(agent.team_size, dtype=np.int32)\n",
    "        src_is_teacher = np.ones(agent.team_size, dtype=bool)\n",
    "        keep_hold = np.zeros(agent.team_size, dtype=np.int32)\n",
    "        prev_exec_actions = [0] * agent.team_size\n",
    "        stay_streak = [0] * agent.team_size\n",
    "\n",
    "        bc_losses = []\n",
    "        first_capture_step = None\n",
    "\n",
    "        for step in range(env_wrapper.base_env.realm.step_limit):\n",
    "            world_map = env_wrapper.base_env.realm.world.map\n",
    "\n",
    "            # 1) учитель только for наших",
    "            t_actions = teacher.get_actions(state, team=0)\n",
    "\n",
    "            # 2) BC",
    "            loss, feats_batch, y_batch = agent.train_step_bc_multi(info, world_map, t_actions)\n",
    "            bc_losses.append(loss)\n",
    "            agent.add_replay(feats_batch, y_batch)\n",
    "            if beta > 0.20 or (step % 5) == 0:\n",
    "                agent.replay_step(steps=1, batch=128)\n",
    "\n",
    "            # 3) микширование",
    "            exec_actions = [0] * agent.team_size\n",
    "\n",
    "            # один общий вызов ученика",
    "            student_actions = None\n",
    "            student_logprobs = None\n",
    "            use_student_mask = (np.random.rand() >= beta)\n",
    "            if use_student_mask:\n",
    "                student_actions, student_logprobs, _, _ = agent.get_actions(\n",
    "                    info, world_map, training=True, greedy=False\n",
    "                )\n",
    "\n",
    "            # лёгкая стохастика ученика (exploration)",
    "            EPS = 0.03\n",
    "            DIRS = {0:(0,0), 1:(0,1), 2:(0,-1), 3:(-1,0), 4:(1,0)}  # 0=stay,1=→,2=←,3=↑,4=↓",
    "            if use_student_mask and (np.random.rand() < EPS) and student_actions is not None:\n",
    "                k_rand = np.random.randint(0, agent.team_size)\n",
    "                try:\n",
    "                    wm_np = np.array(world_map)\n",
    "                    walls = (wm_np[:, :, 0] == -1) & (wm_np[:, :, 1] == -1) if wm_np.ndim == 3 else (wm_np == -1)\n",
    "                    def passable(y, x): return not walls[y % H, x % W]\n",
    "                    py = int(info[\"predators\"][k_rand][\"y\"]); px = int(info[\"predators\"][k_rand][\"x\"])\n",
    "                    legal = []\n",
    "                    for a, (dy, dx) in DIRS.items():\n",
    "                        ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                        if a == 0 or passable(ny, nx):\n",
    "                            legal.append(a)\n",
    "                    student_actions[k_rand] = int(np.random.choice(legal if legal else list(DIRS.keys())))\n",
    "                except Exception:\n",
    "                    student_actions[k_rand] = int(np.random.choice(list(DIRS.keys())))\n",
    "\n",
    "            for k in range(agent.team_size):\n",
    "                if src_hold[k] <= 0:\n",
    "                    src_is_teacher[k] = (np.random.rand() < beta)\n",
    "                    src_hold[k] = STICK_K\n",
    "\n",
    "                if src_is_teacher[k]:\n",
    "                    a = int(t_actions[k])\n",
    "                    exec_actions[k] = a\n",
    "                    keep_hold[k] = 0\n",
    "                else:\n",
    "                    if student_actions is None:\n",
    "                        sa, slp, _, _ = agent.get_actions(info, world_map, training=True, greedy=False)\n",
    "                        cand = int(sa[k]); lp = float(slp[k].max().detach().cpu().item())\n",
    "                    else:\n",
    "                        cand = int(student_actions[k])\n",
    "                        lp = float(student_logprobs[k].max().detach().cpu().item())\n",
    "                    prob = float(np.exp(lp))\n",
    "                    if (prob < TAU_CONF) or (keep_hold[k] > 0):\n",
    "                        exec_actions[k] = prev_exec_actions[k]\n",
    "                        keep_hold[k] = max(0, keep_hold[k] - 1)\n",
    "                    else:\n",
    "                        exec_actions[k] = cand\n",
    "                        keep_hold[k] = KEEP_K\n",
    "\n",
    "                src_hold[k] -= 1\n",
    "\n",
    "            # 3.1 anti-разворот в момент отхода from учителя",
    "            def _is_reverse(v_new, v_last):\n",
    "                return (v_new[0] == -v_last[0]) and (v_new[1] == -v_last[1]) and (v_new != (0,0))\n",
    "            for k in range(agent.team_size):\n",
    "                just_switched = (not src_is_teacher[k]) and (src_hold[k] == STICK_K-1)\n",
    "                if not just_switched: \n",
    "                    continue\n",
    "                v_last = last_dirs_ep[k] if k < len(last_dirs_ep) else (0,0)\n",
    "                v_new  = DIRS.get(exec_actions[k], (0,0))\n",
    "                if _is_reverse(v_new, v_last):\n",
    "                    alt = int(t_actions[k])\n",
    "                    if not _is_reverse(DIRS.get(alt,(0,0)), v_last):\n",
    "                        exec_actions[k] = alt\n",
    "                        src_is_teacher[k] = True\n",
    "                        src_hold[k] = max(src_hold[k], 1)\n",
    "                        keep_hold[k] = 0\n",
    "                    else:\n",
    "                        exec_actions[k] = 0\n",
    "                        keep_hold[k] = 1\n",
    "\n",
    "            # 3.2 anti-idle (≥3 стояний подряд)",
    "            wm_np = np.array(world_map)\n",
    "            walls = (wm_np[:, :, 0] == -1) & (wm_np[:, :, 1] == -1) if wm_np.ndim == 3 else (wm_np == -1)\n",
    "            def passable(y, x): return not walls[y % H, x % W]\n",
    "            def best_step_toward_nearest(k: int) -> int:\n",
    "                preys_alive = [q for q in info[\"preys\"] if q.get(\"alive\", False)]\n",
    "                if not preys_alive: return 0\n",
    "                py = int(info[\"predators\"][k][\"y\"]); px = int(info[\"predators\"][k][\"x\"])\n",
    "                best_a, best_d = 0, float(\"inf\")\n",
    "                for a, (dy, dx) in DIRS.items():\n",
    "                    ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                    if not passable(ny, nx): continue\n",
    "                    d = min(manhattan_torus(ny, nx, int(t[\"y\"]), int(t[\"x\"]), H, W) for t in preys_alive)\n",
    "                    if d < best_d: best_d, best_a = d, a\n",
    "                return best_a\n",
    "\n",
    "            for k in range(agent.team_size):\n",
    "                stay_streak[k] = stay_streak[k] + 1 if exec_actions[k] == 0 else 0\n",
    "                if stay_streak[k] >= 3:\n",
    "                    alt = int(t_actions[k]) if int(t_actions[k]) != 0 else best_step_toward_nearest(k)\n",
    "                    if alt != 0:\n",
    "                        exec_actions[k] = alt\n",
    "                        src_is_teacher[k] = True\n",
    "                        src_hold[k] = max(src_hold[k], 1)\n",
    "                        keep_hold[k] = 0\n",
    "                        stay_streak[k] = 0\n",
    "\n",
    "            # зафиксировать прошлые действия после anti-idle",
    "            for k in range(agent.team_size):\n",
    "                prev_exec_actions[k] = exec_actions[k]\n",
    "\n",
    "            # 3.3 ACTION MASK + anti «в одну сtorusону»",
    "            def nearest_prey_dist(py, px):\n",
    "                preys_alive = [q for q in info[\"preys\"] if q.get(\"alive\", False)]\n",
    "                if not preys_alive: return 0.0\n",
    "                return min(manhattan_torus(py, px, int(t[\"y\"]), int(t[\"x\"]), H, W) for t in preys_alive)\n",
    "\n",
    "            for k in range(agent.team_size):\n",
    "                a = int(exec_actions[k])\n",
    "                dy, dx = DIRS.get(a, (0,0))\n",
    "                py = int(info[\"predators\"][k][\"y\"]); px = int(info[\"predators\"][k][\"x\"])\n",
    "                ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                if a != 0 and not passable(ny, nx):\n",
    "                    a_t = int(t_actions[k])\n",
    "                    dy_t, dx_t = DIRS.get(a_t, (0,0))\n",
    "                    ny_t, nx_t = (py + dy_t) % H, (px + dx_t) % W\n",
    "                    if a_t != 0 and passable(ny_t, nx_t):\n",
    "                        exec_actions[k] = a_t\n",
    "                        src_is_teacher[k] = True\n",
    "                        src_hold[k] = max(src_hold[k], 1)\n",
    "                        keep_hold[k] = 0\n",
    "                    else:\n",
    "                        exec_actions[k] = best_step_toward_nearest(k)\n",
    "\n",
    "            N_REPEAT = 6\n",
    "            if \"repeat_cnt\" not in locals():\n",
    "                repeat_cnt = [0]*agent.team_size\n",
    "                last_action_seen = [None]*agent.team_size\n",
    "            for k in range(agent.team_size):\n",
    "                a = int(exec_actions[k])\n",
    "                if last_action_seen[k] == a:\n",
    "                    repeat_cnt[k] += 1\n",
    "                else:\n",
    "                    repeat_cnt[k] = 1\n",
    "                    last_action_seen[k] = a\n",
    "\n",
    "                if repeat_cnt[k] >= N_REPEAT and a != 0:\n",
    "                    py = int(info[\"predators\"][k][\"y\"]); px = int(info[\"predators\"][k][\"x\"])\n",
    "                    d0 = nearest_prey_dist(py, px)\n",
    "                    dy, dx = DIRS.get(a, (0,0))\n",
    "                    ny, nx = (py + dy) % H, (px + dx) % W\n",
    "                    d1 = nearest_prey_dist(ny, nx)\n",
    "                    if not passable(ny, nx) or d1 >= d0:\n",
    "                        alt = int(t_actions[k])\n",
    "                        dyt, dxt = DIRS.get(alt, (0,0))\n",
    "                        nyt, nxt = (py + dyt) % H, (px + dxt) % W\n",
    "                        if alt != 0 and passable(nyt, nxt):\n",
    "                            exec_actions[k] = alt\n",
    "                            src_is_teacher[k] = True\n",
    "                            src_hold[k] = max(src_hold[k], 1)\n",
    "                            keep_hold[k] = 0\n",
    "                        else:\n",
    "                            exec_actions[k] = best_step_toward_nearest(k)\n",
    "                        repeat_cnt[k] = 0\n",
    "\n",
    "            # 4) step среды",
    "            state, done, new_info = env_wrapper.step(exec_actions)\n",
    "\n",
    "            # --- метрики",
    "            coords = [(int(pr[\"y\"]), int(pr[\"x\"])) for pr in new_info[\"predators\"]]\n",
    "            idle_now = 0\n",
    "            if prev_pred_coords is not None:\n",
    "                for (py0, px0), (py1, px1) in zip(prev_pred_coords, coords):\n",
    "                    if (py0, px0) == (py1, px1): idle_now += 1\n",
    "            pair0 = pair1 = 0; sumd = 0.0; pairs = 0\n",
    "            for i in range(len(coords)):\n",
    "                for j in range(i + 1, len(coords)):\n",
    "                    yi, xi = coords[i]; yj, xj = coords[j]\n",
    "                    d = abs(yi - yj) + abs(xi - xj)\n",
    "                    if d == 0: pair0 += 1\n",
    "                    elif d == 1: pair1 += 1\n",
    "                    sumd += d; pairs += 1\n",
    "            disp = (sumd / max(1, pairs))\n",
    "            if (disp <= 1.5) or (pair0 > 0) or (pair1 >= 2): cluster_steps += 1\n",
    "            if idle_now >= (agent.team_size - 1):\n",
    "                idle_steps += 1; _idle_streak += 1\n",
    "            else:\n",
    "                _idle_streak = 0\n",
    "            max_idle_streak = max(max_idle_streak, _idle_streak)\n",
    "\n",
    "            dirs_now = []\n",
    "            if prev_pred_coords is not None:\n",
    "                for k, (y1, x1) in enumerate(coords):\n",
    "                    y0, x0 = prev_pred_coords[k]\n",
    "                    dirs_now.append((y1 - y0, x1 - x0))\n",
    "                for k, (dy, dx) in enumerate(dirs_now):\n",
    "                    if (dy, dx) != (0,0) and (dy, dx) == (-last_dirs_ep[k][0], -last_dirs_ep[k][1]):\n",
    "                        flip_events += 1\n",
    "                for i in range(len(coords)):\n",
    "                    for j in range(i+1, len(coords)):\n",
    "                        d = abs(coords[i][0]-coords[j][0]) + abs(coords[i][1]-coords[j][1])\n",
    "                        if d <= 2 and i < len(dirs_now) and j < len(dirs_now):\n",
    "                            if dirs_now[i] != (0,0) and dirs_now[i] == dirs_now[j]:\n",
    "                                same_dir_events += 1\n",
    "                for k in range(len(dirs_now)): last_dirs_ep[k] = dirs_now[k]\n",
    "            else:\n",
    "                for k in range(len(last_dirs_ep)): last_dirs_ep[k] = (0,0)\n",
    "            prev_pred_coords = coords\n",
    "\n",
    "            # 5) треки/посещения",
    "            preds = new_info[\"predators\"]\n",
    "            for k, pr in enumerate(preds):\n",
    "                py, px = int(pr[\"y\"]), int(pr[\"x\"])\n",
    "                predator_paths[k].append((py, px))\n",
    "                if 0 <= py < H and 0 <= px < W:\n",
    "                    visited_map_team[py, px] += 1\n",
    "\n",
    "            # 6) PvP-reward + лог",
    "            _, caught_preys = compute_reward_pvp(\n",
    "                prev_info=info,\n",
    "                info=new_info,\n",
    "                caught_preys=caught_preys,\n",
    "                step_idx=step,\n",
    "                episode_idx=ep,\n",
    "                visited_map=visited_map_ep,\n",
    "                phase=\"BC-DAgger-PvP\",\n",
    "                actions_exec=exec_actions,\n",
    "                actions_teacher=t_actions\n",
    "            )\n",
    "\n",
    "            # первая поимка",
    "            caught_now = sum(1 for p in new_info[\"preys\"] if not p.get(\"alive\", True))\n",
    "            if first_capture_step is None and caught_now > 0:\n",
    "                first_capture_step = step\n",
    "\n",
    "            info = new_info\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        beta = max(beta_end, beta * beta_decay)\n",
    "\n",
    "        # сброс лог-буфера",
    "        if STEP_LOG_BUFFER:\n",
    "            with open(LOG_STEP_PATH, \"a\", newline=\"\") as f:\n",
    "                csv.writer(f).writerows(STEP_LOG_BUFFER)\n",
    "            STEP_LOG_BUFFER.clear()\n",
    "\n",
    "        caught_total = sum(1 for p in info[\"preys\"] if not p.get(\"alive\", True))\n",
    "        avg_bc = float(np.mean(bc_losses)) if bc_losses else 0.0\n",
    "\n",
    "        # счёт/результат (мы = team 0, бот = team 1)",
    "        sc0 = float(info.get(\"scores\", [0.0, 0.0])[0])\n",
    "        sc1 = float(info.get(\"scores\", [0.0, 0.0])[1])  # VersusBotEnv кладёт scores в info. :contentReference[oaicite:2]{index=2}",
    "        if sc0 > sc1:   result = \"WIN\"\n",
    "        elif sc0 < sc1: result = \"LOSS\"\n",
    "        else:           result = \"DRAW\"\n",
    "\n",
    "        if (ep + 1) % render_every == 0 or ep == 0 or ep == episodes-1:\n",
    "            gif_path = os.path.join(FRAME_DIR, f\"pvp_dagger_ep_{ep:03d}.gif\")\n",
    "            team_map_path = os.path.join(MAP_DIR, f\"pvp_team_ep{ep:03d}.png\")\n",
    "            make_color_gif(env_wrapper, gif_path, resize_factor=10, fps=8)\n",
    "            visualize_team_map(ep, info, visited_map_team, predator_paths, team_map_path)\n",
    "            display_side_by_side(gif_path, team_map_path, width=300)\n",
    "\n",
    "        cluster_pct = cluster_steps / max(1, step + 1)\n",
    "        idle_pct    = idle_steps    / max(1, step + 1)\n",
    "        flip_rate   = flip_events   / max(1, step + 1)\n",
    "        same_rate   = same_dir_events / max(1, step + 1)\n",
    "\n",
    "        print(f\"[DAgger-PvP] ep={ep:03d} step={step:03d} beta={beta:.3f} \"\n",
    "              f\"caught={caught_total:03d} first_cap={first_capture_step} bc={avg_bc:.4f} \"\n",
    "              f\"cluster={cluster_pct:.2f} idle={idle_pct:.2f} flip={flip_rate:.2f} same={same_rate:.2f} \"\n",
    "              f\"score_us-bot={sc0:.1f}-{sc1:.1f} result={result}\")\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"beta\":   f\"{beta:.3f}\",\n",
    "            \"caught\": f\"{caught_total:03d}\",\n",
    "            \"first\":  first_capture_step if first_capture_step is not None else \"-\",\n",
    "            \"bc\":     f\"{avg_bc:.3f}\",\n",
    "            \"clu\":    f\"{cluster_pct:.2f}\",\n",
    "            \"idle\":   f\"{idle_pct:.2f}\",\n",
    "            \"flip\":   f\"{flip_rate:.2f}\",\n",
    "            \"same\":   f\"{same_rate:.2f}\",\n",
    "            \"score\":  f\"{sc0:.0f}-{sc1:.0f} {result}\",\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение and экспорт "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPORT: сохраняем лучшую model в agent.pkl (CPU) ===",
    "\n",
    "def export_agent_pkl(agent, path=\"agent.pkl\"):\n",
    "    assert agent.model is not None, \"Модель ещё не инициализирована\"\n",
    "    obj = {\n",
    "        \"state_dict\": copy.deepcopy(agent.model.state_dict()),\n",
    "        \"meta\": {\n",
    "            \"n_actions\": 5,\n",
    "            \"input_dim\": int(agent._input_dim),  # длина векtorusа признаков D",
    "            \"patch_size\": int(PATCH_SIZE),\n",
    "            \"k_nearest\": int(K_NEAREST),\n",
    "            \"k_mates\": 2,   # в нашем FeatureBuilder по умолчанию",
    "        }\n",
    "    }\n",
    "    torch.save(obj, path)\n",
    "    print(f\"Сохранено: {os.path.abspath(path)}\")\n",
    "\n",
    "# (опционально) мини-эвал for выбора «лучшего» перед сохранением:",
    "# if у тебя уже отучено — просто вызови export_agent_pkl(agent5)",
    "\n",
    "\n",
    "def _export_agent_pkl(agent, path):\n",
    "    \"\"\"\n",
    "    Сохранение чекпойнта модели с автосозданием родительских дирекtorusий.\n",
    "    Возвращает абсолютный path к файлу.\n",
    "    \"\"\"\n",
    "    import copy, torch, os\n",
    "\n",
    "    # гарantiруем, что folder существует",
    "    parent = os.path.dirname(path)\n",
    "    if parent:\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "    meta = getattr(agent, \"meta\", {}) if hasattr(agent, \"meta\") else {}\n",
    "    # if нужно, можно добавить input_dim из exampleа входа:",
    "    # sample = torch.zeros(1, agent.model.input_dim, device=next(agent.model.parameters()).device)",
    "    # meta[\"input_dim\"] = int(sample.shape[1])",
    "\n",
    "    obj = {\n",
    "        \"state_dict\": copy.deepcopy(agent.model.state_dict()),\n",
    "        \"optimizer\": agent.optimizer.state_dict() if getattr(agent, \"optimizer\", None) else None,\n",
    "        \"meta\": meta\n",
    "    }\n",
    "    torch.save(obj, path)\n",
    "    return os.path.abspath(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions обучения учитель-ученик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _teacher_logits(agent_teacher: NetAgentShared, feats_np: np.ndarray, device):\n",
    "    \"\"\"\n",
    "    Вычисляет teacher-logits for батча фич (np -> torch).\n",
    "    Возвращает тензор [B, A].\n",
    "    \"\"\"\n",
    "    x = torch.tensor(feats_np, dtype=torch.float32, device=device)\n",
    "    agent_teacher.model.eval()\n",
    "    logits, _ = agent_teacher.model(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def distill_to_light(agent_teacher: NetAgentShared,\n",
    "                     agent_student: NetAgentSharedLite,\n",
    "                     steps: int = 2000,\n",
    "                     batch: int = 256,\n",
    "                     temperature: float = 1.5,\n",
    "                     alpha_kd: float = 0.8):\n",
    "    \"\"\"\n",
    "    Дистилляция из «тяжёлой» модели в «лёгкую».\n",
    "    Источник данных — общий BC-replay тяжёлого агента (feats/labels уже собирались when DAgger).\n",
    "    Потери: KD (KL между softmax(teacher/T) and softmax(student/T)) + небольшая CE по teacher-argmax.\n",
    "\n",
    "    parameters можно уменьшать/увеличивать под лимит времени.\n",
    "    \"\"\"\n",
    "    assert agent_teacher.model is not None, \"Teacher не инициализирован\"\n",
    "    # убедимся, что у студента заведен граф с теми же D",
    "    if agent_student.model is None:\n",
    "        # возьмём любую фичу из реплея, чтобы создать model",
    "        assert len(agent_teacher.rb_feats) > 0, \"Реплей пуст — надо сначала пройти SOLO/PvP DAgger\"\n",
    "        agent_student._ensure_model(agent_teacher.rb_feats[0][None, :])\n",
    "\n",
    "    device_s = next(agent_student.model.parameters()).device\n",
    "    agent_student.model.train()\n",
    "\n",
    "    T = float(temperature)\n",
    "    kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    for _ in range(steps):\n",
    "        if len(agent_teacher.rb_feats) == 0:\n",
    "            break\n",
    "        idx = rng.integers(0, len(agent_teacher.rb_feats), size=min(batch, len(agent_teacher.rb_feats)))\n",
    "        feats = np.stack([agent_teacher.rb_feats[i] for i in idx], axis=0)  # [B, D]",
    "\n",
    "        # teacher",
    "        with torch.no_grad():\n",
    "            t_logits = _teacher_logits(agent_teacher, feats, device_s) / T\n",
    "            t_prob   = torch.softmax(t_logits, dim=-1)\n",
    "\n",
    "        # student",
    "        x = torch.tensor(feats, dtype=torch.float32, device=device_s)\n",
    "        s_logits, _ = agent_student.model(x)\n",
    "        s_logits_T = s_logits / T\n",
    "        s_logprob_T = torch.log_softmax(s_logits_T, dim=-1)\n",
    "\n",
    "        # KD + чуть-чуть CE по teacher argmax",
    "        loss_kd = kl(s_logprob_T, t_prob) * (T * T)\n",
    "        hard = torch.argmax(t_prob, dim=-1)\n",
    "        loss_ce = ce(s_logits, hard)\n",
    "        loss = alpha_kd * loss_kd + (1.0 - alpha_kd) * loss_ce\n",
    "\n",
    "        agent_student.optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(agent_student.model.parameters(), 1.0)\n",
    "        agent_student.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader нескольких карт for команд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pvp_env_mixed(\n",
    "    team_size: int = TEAM_SIZE,\n",
    "    step_limit: int = 300,\n",
    "    spawn_bonus_every: int = 5,\n",
    "    use_pregenerated_dir: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    VersusBotEnv с MixedMapLoader:\n",
    "      - Две команды hunterов (мы = team 0, бот = team 1)\n",
    "      - Список разнообразных генераtorusов for ротации типов карт\n",
    "      - When желании можно добавить pregenerated .npy (if есть folder)\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "\n",
    "    # --- ДЮЖИНА РАЗНООБРАЗНЫХ ДВУХКОМАНДНЫХ ГЕНЕРАtorusОВ ---",
    "    # Rocks: from разреженных to плотных",
    "    for p in [0.01, 0.03, 0.06, 0.10, 0.15]:\n",
    "        loaders.append(TwoTeamRocksMapLoader(\n",
    "            size=40, spawn_radius=8, preys_num=100, spawn_points=10,\n",
    "            rock_spawn_proba=p, additional_rock_spawn_proba=0.20\n",
    "        ))\n",
    "\n",
    "    # Labyrinth: from более связных к узким",
    "    for add_links in [12, 9, 6, 3, 1]:\n",
    "        loaders.append(TwoTeamLabyrinthMapLoader(\n",
    "            size=40, spawn_radius=8, preys_num=100, spawn_points=10,\n",
    "            additional_links_max=add_links, additional_links_min=max(0, add_links-2)\n",
    "        ))\n",
    "\n",
    "    # (опционально) Однокомандные миксы + зеркалка не нужна — просто добавим for разнообразия структуры links/passages.",
    "    # Они будут автоматически адаптированы realm'ом; important, что в two-team логике спавн команд уже предусмотрен",
    "    # Но чтобы не ломать семantiку two-team задач, по умолчанию не добавляем single-team в PvP.",
    "\n",
    "    # (опционально) folder с pregenerated картами (формат .npy), if есть",
    "    if use_pregenerated_dir:\n",
    "        try:\n",
    "            loaders.append(PregeneratedMapLoader(dir=use_pregenerated_dir))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Pregenerated skip: {e}\")\n",
    "\n",
    "    mix_loader = MixedMapLoader(loaders)\n",
    "\n",
    "    # Бот-оппонент (идёт к ближайшим целям)",
    "    bot = ClosestTargetAgent(num_predators=team_size)\n",
    "\n",
    "    realm = Realm(\n",
    "        map_loader=mix_loader,\n",
    "        playable_teams_num=2,             # мы + бот",
    "        bots={1: bot},                    # бот = team 1",
    "        playable_team_size=team_size,\n",
    "        step_limit=step_limit,\n",
    "        spawn_bonus_every=spawn_bonus_every\n",
    "    )\n",
    "\n",
    "    base_env = VersusBotEnv(realm)\n",
    "    env_pvp = RenderedEnvWrapper(base_env)  # твоя же обёртка for кадров",
    "    return env_pvp\n",
    "\n",
    "# === NEW: mixed loader for двух команд (PvP) ===",
    "def build_twoteam_mixed_loader(size=40, preys_num=100, team_size=5,\n",
    "                               rocks_grid=None, lab_links=None, spawn_points=None):\n",
    "    \"\"\"\n",
    "    MixedMapLoader for двух команд: rocks + labyrinth.\n",
    "    ВНИМАНИЕ: у two-team лоадеров НЕТ аргумента team_size — используем spawn_points.\n",
    "    spawn_points трактуем as число точек спавна НА КОМАНДУ (обычно = team_size).\n",
    "    \"\"\"\n",
    "    sp = team_size if spawn_points is None else int(spawn_points)\n",
    "    loaders = []\n",
    "\n",
    "    # rocks-наборы",
    "    if rocks_grid is None:\n",
    "        rocks_grid = [(0.03, 0.05), (0.07, 0.12), (0.11, 0.18), (0.15, 0.21)]\n",
    "    for p, ap in rocks_grid:\n",
    "        loaders.append(\n",
    "            TwoTeamRocksMapLoader(\n",
    "                size=size,\n",
    "                preys_num=preys_num,\n",
    "                spawn_points=sp,                   # ← вместо team_size",
    "                rock_spawn_proba=p,\n",
    "                additional_rock_spawn_proba=ap\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # labyrinth-наборы",
    "    if lab_links is None:\n",
    "        lab_links = [(20, 10), (12, 6), (8, 3), (3, 1)]\n",
    "    for lmax, lmin in lab_links:\n",
    "        loaders.append(\n",
    "            TwoTeamLabyrinthMapLoader(\n",
    "                size=size,\n",
    "                preys_num=preys_num,\n",
    "                spawn_points=sp,                   # ← вместо team_size",
    "                additional_links_max=lmax,\n",
    "                additional_links_min=lmin\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return MixedMapLoader(loaders)\n",
    "\n",
    "# === VARIANT B: SOLO loader с полной вариативностью размера and типа maps ===",
    "def build_singleteam_mixed_loader_B(\n",
    "    sizes=(32, 40, 48, 56),\n",
    "    preys_num_grid=(80, 100, 120),\n",
    "    spawn_points_grid=None,     # None -> = TEAM_SIZE",
    "    rocks_grid=None,\n",
    "    lab_links=None,\n",
    "    pregenerated_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    MixedMapLoader for одиночной команды, но:\n",
    "      • Несколько размеров карт (sizes)\n",
    "      • Разные числа жертв (preys_num_grid)\n",
    "      • Разные spawn_points (if нужно, else = TEAM_SIZE)\n",
    "      • Богатые сетки параметров for Rocks and Labyrinth\n",
    "      • (опционально) подмешивание папки с pregenerated .npy\n",
    "\n",
    "    NB: Realm/Env нормально переносят смену размера между эпизодами — в твоём тренинге\n",
    "        visited_map_* пересоздаются под текущую карту.\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "\n",
    "    # Сетки по умолчанию (достаточно разнообразные)",
    "    if rocks_grid is None:\n",
    "        # (rock_spawn_proba, additional_rock_spawn_proba)",
    "        rocks_grid = [(0.01, 0.00), (0.03, 0.05), (0.05, 0.10), (0.07, 0.12),\n",
    "                      (0.09, 0.15), (0.11, 0.18), (0.13, 0.20), (0.15, 0.21)]\n",
    "    if lab_links is None:\n",
    "        # (additional_links_max, additional_links_min)",
    "        lab_links = [(24,12), (20,10), (16,8), (12,6),\n",
    "                     (10,4),  (8,3),  (6,2),  (3,1)]\n",
    "\n",
    "    for S in sizes:\n",
    "        for PNUM in preys_num_grid:\n",
    "            sp_candidates = (spawn_points_grid if spawn_points_grid is not None\n",
    "                             else (TEAM_SIZE,))\n",
    "            for SP in sp_candidates:\n",
    "                # --- Rocks ---",
    "                for p, ap in rocks_grid:\n",
    "                    loaders.append(\n",
    "                        SingleTeamRocksMapLoader(\n",
    "                            size=S, preys_num=PNUM, spawn_points=int(SP),\n",
    "                            rock_spawn_proba=p, additional_rock_spawn_proba=ap\n",
    "                        )\n",
    "                    )\n",
    "                # --- Labyrinth ---",
    "                for lmax, lmin in lab_links:\n",
    "                    loaders.append(\n",
    "                        SingleTeamLabyrinthMapLoader(\n",
    "                            size=S, preys_num=PNUM, spawn_points=int(SP),\n",
    "                            additional_links_max=lmax, additional_links_min=lmin\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    # pregenerated (if есть что подмешать)",
    "    if pregenerated_dir and os.path.isdir(pregenerated_dir) and len(os.listdir(pregenerated_dir)) > 0:\n",
    "        loaders.append(PregeneratedMapLoader(pregenerated_dir))\n",
    "\n",
    "    return MixedMapLoader(loaders)\n",
    "\n",
    "# === VARIANT B: PvP loader с полной вариативностью размеров and параметров ===",
    "def build_twoteam_mixed_loader_B(\n",
    "    sizes=(32, 40, 48, 56),\n",
    "    preys_num_grid=(80, 100, 120),\n",
    "    team_size=5,\n",
    "    spawn_points_grid=None,     # None -> = team_size",
    "    rocks_grid=None,\n",
    "    lab_links=None\n",
    "):\n",
    "    \"\"\"\n",
    "    MixedMapLoader for двухкомандной PvP-среды с множеством configurations:\n",
    "      • Несколько размеров карт\n",
    "      • Разные числа жертв\n",
    "      • Разные spawn_points на КОМАНДУ (обычно = team_size)\n",
    "      • Наборы rocks/labyrinth\n",
    "    \"\"\"\n",
    "    sp_default = (team_size,)\n",
    "    sp_candidates = spawn_points_grid if spawn_points_grid is not None else sp_default\n",
    "\n",
    "    if rocks_grid is None:\n",
    "        rocks_grid = [(0.03, 0.05), (0.05, 0.10), (0.07, 0.12), (0.09, 0.15),\n",
    "                      (0.11, 0.18), (0.13, 0.20), (0.15, 0.21)]\n",
    "    if lab_links is None:\n",
    "        lab_links = [(20,10), (16,8), (12,6), (8,3), (3,1)]\n",
    "\n",
    "    loaders = []\n",
    "    for S in sizes:\n",
    "        for PNUM in preys_num_grid:\n",
    "            for SP in sp_candidates:\n",
    "                # Rocks",
    "                for p, ap in rocks_grid:\n",
    "                    loaders.append(\n",
    "                        TwoTeamRocksMapLoader(\n",
    "                            size=S,\n",
    "                            preys_num=PNUM,\n",
    "                            spawn_points=int(SP),    # <= важное: здесь spawn_points, не team_size",
    "                            rock_spawn_proba=p,\n",
    "                            additional_rock_spawn_proba=ap\n",
    "                        )\n",
    "                    )\n",
    "                # Labyrinth",
    "                for lmax, lmin in lab_links:\n",
    "                    loaders.append(\n",
    "                        TwoTeamLabyrinthMapLoader(\n",
    "                            size=S,\n",
    "                            preys_num=PNUM,\n",
    "                            spawn_points=int(SP),\n",
    "                            additional_links_max=lmax,\n",
    "                            additional_links_min=lmin\n",
    "                        )\n",
    "                    )\n",
    "    return MixedMapLoader(loaders)\n",
    "\n",
    "# === VARIANT B: PvP окружение на полном миксе ===",
    "def build_pvp_env_mixed_B(\n",
    "    team_size: int = TEAM_SIZE,\n",
    "    step_limit: int = 300,\n",
    "    spawn_bonus_every: int = 5,\n",
    "    use_pregenerated_dir: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    VersusBotEnv с MixedMapLoader на полном миксе размеров/типов for PvP.\n",
    "    \"\"\"\n",
    "    # Подмешаем 2..3 сотни configurations за счёт разных размеров/сеток",
    "    mix_loader = build_twoteam_mixed_loader_B(\n",
    "        sizes=(32, 40, 48, 56),\n",
    "        preys_num_grid=(80, 100, 120),\n",
    "        team_size=team_size,\n",
    "        spawn_points_grid=(team_size, team_size+1),  # иногда спавним на 1 точку больше",
    "        rocks_grid=[(0.03, 0.05), (0.07, 0.12), (0.11, 0.18), (0.15, 0.21)],\n",
    "        lab_links=[(20,10), (12,6), (8,3), (3,1)],\n",
    "    )\n",
    "\n",
    "    # (опционально) добавим заранее сгенерённые maps одной строкой:",
    "    if use_pregenerated_dir and os.path.isdir(use_pregenerated_dir) and len(os.listdir(use_pregenerated_dir)) > 0:\n",
    "        # MixedMapLoader умеет принимать список лоадеров; расширим его внутренний пул:",
    "        # аккуратно добавим ещё один PregeneratedMapLoader",
    "        preg = PregeneratedMapLoader(use_pregenerated_dir)\n",
    "        # MixedMapLoader([... , preg]) — пересоздадим с добавкой",
    "        mix_loader = MixedMapLoader(mix_loader.loaders + [preg])  # у твоего MixedMapLoader есть .loaders",
    "\n",
    "    bot = ClosestTargetAgent(num_predators=team_size)\n",
    "\n",
    "    realm = Realm(\n",
    "        map_loader=mix_loader,\n",
    "        playable_teams_num=2,\n",
    "        bots={1: bot},\n",
    "        playable_team_size=team_size,\n",
    "        step_limit=step_limit,\n",
    "        spawn_bonus_every=spawn_bonus_every\n",
    "    )\n",
    "\n",
    "    base_env = VersusBotEnv(realm)\n",
    "    return RenderedEnvWrapper(base_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_module_prefix(state_dict: dict):\n",
    "    \"\"\"Убираем префикс 'module.' from DataParallel/DistributedDataParallel, if есть.\"\"\"\n",
    "    if not state_dict:\n",
    "        return state_dict\n",
    "    sample_key = next(iter(state_dict.keys()))\n",
    "    if sample_key.startswith(\"module.\"):\n",
    "        return {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "def load_agent_from_pkl(agent_obj, ckpt_path: str):\n",
    "    \"\"\"\n",
    "    Грузит веса модели/оптимизаtorusа в существующий agent_obj.\n",
    "    Поддерживает форматы:\n",
    "      - torch.save({'model': sd, 'optimizer': sd_opt, ...})\n",
    "      - torch.save({'state_dict': sd}) / {'model_state_dict': sd}\n",
    "      - torch.save(sd)  # сам state_dict",
    "    \"\"\"\n",
    "    assert os.path.exists(ckpt_path), f\"Нет файла: {ckpt_path}\"\n",
    "    device = next(agent_obj.model.parameters()).device\n",
    "\n",
    "    # 1) грузим payload через torch.load (а НЕ pickle.load)",
    "    payload = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # 2) находим state_dict модели в разных вариантах ключей",
    "    if isinstance(payload, dict):\n",
    "        sd = None\n",
    "        for k in (\"model\", \"state_dict\", \"model_state_dict\"):\n",
    "            if k in payload and isinstance(payload[k], dict):\n",
    "                sd = payload[k]\n",
    "                break\n",
    "        if sd is None and all(isinstance(k, str) for k in payload.keys()):\n",
    "            # возможно это and есть state_dict",
    "            sd = payload\n",
    "        if sd is None:\n",
    "            raise ValueError(f\"Не нашёл state_dict в чекпойнте: ключи={list(payload.keys())[:10]}\")\n",
    "        sd = _strip_module_prefix(sd)\n",
    "        missing, unexpected = agent_obj.model.load_state_dict(sd, strict=False)\n",
    "        if missing:\n",
    "            print(f\"[WARN] Missing keys: {len(missing)} (первые 5): {missing[:5]}\")\n",
    "        if unexpected:\n",
    "            print(f\"[WARN] Unexpected keys: {len(unexpected)} (первые 5): {unexpected[:5]}\")\n",
    "\n",
    "        # 3) оптимизаtorus (if есть and совместим)",
    "        if \"optimizer\" in payload and getattr(agent_obj, \"optimizer\", None) is not None:\n",
    "            try:\n",
    "                agent_obj.optimizer.load_state_dict(payload[\"optimizer\"])\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Optimizer state не загружен: {e}\")\n",
    "    else:\n",
    "        # целиком state_dict saved as объект",
    "        sd = _strip_module_prefix(payload)\n",
    "        missing, unexpected = agent_obj.model.load_state_dict(sd, strict=False)\n",
    "        if missing:\n",
    "            print(f\"[WARN] Missing keys: {len(missing)} (первые 5): {missing[:5]}\")\n",
    "        if unexpected:\n",
    "            print(f\"[WARN] Unexpected keys: {len(unexpected)} (первые 5): {unexpected[:5]}\")\n",
    "\n",
    "    print(f\"[OK] Модель загружена из: {ckpt_path} на устройство {device}\")\n",
    "    return agent_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE 1] SOLO DAgger...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a286e85b16bd4304a564b9936a587a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DAgger-5:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-5] ep=000 finished at step=095, beta=0.943, caught=080, first_cap=2, bc=1.5539, cluster=0.00, idle=0.00, flip=0.34, same=0.04\n",
      "[DAgger-5] ep=001 finished at step=099, beta=0.936, caught=080, first_cap=1, bc=1.4282, cluster=0.00, idle=0.00, flip=0.30, same=0.14\n",
      "[DAgger-5] ep=002 finished at step=091, beta=0.929, caught=080, first_cap=1, bc=1.4094, cluster=0.00, idle=0.00, flip=0.38, same=0.01\n",
      "[DAgger-5] ep=003 finished at step=085, beta=0.922, caught=080, first_cap=1, bc=1.4011, cluster=0.00, idle=0.00, flip=0.24, same=0.09\n",
      "[DAgger-5] ep=004 finished at step=108, beta=0.916, caught=080, first_cap=0, bc=1.3815, cluster=0.00, idle=0.00, flip=0.26, same=0.14\n",
      "[DAgger-5] ep=005 finished at step=086, beta=0.909, caught=080, first_cap=0, bc=1.3831, cluster=0.00, idle=0.00, flip=0.36, same=0.06\n",
      "[DAgger-5] ep=006 finished at step=129, beta=0.902, caught=080, first_cap=1, bc=1.3758, cluster=0.00, idle=0.00, flip=0.27, same=0.38\n",
      "[DAgger-5] ep=007 finished at step=092, beta=0.896, caught=080, first_cap=0, bc=1.3411, cluster=0.00, idle=0.00, flip=0.32, same=0.11\n",
      "[DAgger-5] ep=008 finished at step=090, beta=0.889, caught=080, first_cap=0, bc=1.2719, cluster=0.00, idle=0.00, flip=0.30, same=0.03\n",
      "[DAgger-5] ep=009 finished at step=105, beta=0.882, caught=080, first_cap=1, bc=1.1932, cluster=0.03, idle=0.00, flip=0.22, same=0.43\n",
      "[DAgger-5] ep=010 finished at step=093, beta=0.876, caught=080, first_cap=0, bc=1.2469, cluster=0.00, idle=0.00, flip=0.34, same=0.06\n",
      "[DAgger-5] ep=011 finished at step=094, beta=0.869, caught=080, first_cap=2, bc=1.2022, cluster=0.00, idle=0.00, flip=0.25, same=0.25\n",
      "[DAgger-5] ep=012 finished at step=107, beta=0.863, caught=080, first_cap=0, bc=1.1779, cluster=0.02, idle=0.00, flip=0.22, same=0.14\n",
      "[DAgger-5] ep=013 finished at step=097, beta=0.857, caught=080, first_cap=0, bc=1.2305, cluster=0.00, idle=0.00, flip=0.24, same=0.33\n",
      "[DAgger-5] ep=014 finished at step=122, beta=0.850, caught=080, first_cap=0, bc=1.1157, cluster=0.03, idle=0.00, flip=0.24, same=0.54\n",
      "[DAgger-5] ep=015 finished at step=140, beta=0.844, caught=080, first_cap=0, bc=1.0578, cluster=0.01, idle=0.00, flip=0.23, same=0.40\n",
      "[DAgger-5] ep=016 finished at step=119, beta=0.838, caught=100, first_cap=0, bc=1.3092, cluster=0.00, idle=0.00, flip=0.24, same=0.28\n",
      "[DAgger-5] ep=017 finished at step=109, beta=0.832, caught=100, first_cap=2, bc=1.3230, cluster=0.00, idle=0.00, flip=0.33, same=0.10\n",
      "[DAgger-5] ep=018 finished at step=125, beta=0.826, caught=100, first_cap=0, bc=1.2844, cluster=0.00, idle=0.00, flip=0.28, same=0.03\n",
      "[DAgger-5] ep=019 finished at step=115, beta=0.820, caught=100, first_cap=0, bc=1.3040, cluster=0.00, idle=0.00, flip=0.35, same=0.08\n",
      "[DAgger-5] ep=020 finished at step=102, beta=0.814, caught=100, first_cap=0, bc=1.3019, cluster=0.00, idle=0.00, flip=0.36, same=0.18\n",
      "[DAgger-5] ep=021 finished at step=100, beta=0.808, caught=100, first_cap=0, bc=1.2426, cluster=0.00, idle=0.00, flip=0.35, same=0.14\n",
      "[DAgger-5] ep=022 finished at step=134, beta=0.802, caught=100, first_cap=1, bc=1.2261, cluster=0.00, idle=0.00, flip=0.31, same=0.15\n",
      "[DAgger-5] ep=023 finished at step=134, beta=0.796, caught=100, first_cap=0, bc=1.2353, cluster=0.03, idle=0.01, flip=0.25, same=0.33\n",
      "[DAgger-5] ep=024 finished at step=096, beta=0.790, caught=100, first_cap=3, bc=1.1490, cluster=0.00, idle=0.00, flip=0.32, same=0.28\n",
      "[DAgger-5] ep=025 finished at step=094, beta=0.784, caught=100, first_cap=0, bc=1.1752, cluster=0.00, idle=0.00, flip=0.31, same=0.16\n",
      "[DAgger-5] ep=026 finished at step=104, beta=0.778, caught=100, first_cap=0, bc=1.1120, cluster=0.00, idle=0.00, flip=0.34, same=0.13\n",
      "[DAgger-5] ep=027 finished at step=116, beta=0.773, caught=100, first_cap=0, bc=1.1017, cluster=0.00, idle=0.00, flip=0.29, same=0.23\n",
      "[DAgger-5] ep=028 finished at step=120, beta=0.767, caught=100, first_cap=1, bc=1.0970, cluster=0.01, idle=0.00, flip=0.25, same=0.39\n",
      "[DAgger-5] ep=029 finished at step=119, beta=0.761, caught=100, first_cap=0, bc=1.0847, cluster=0.01, idle=0.00, flip=0.25, same=0.33\n",
      "[DAgger-5] ep=030 finished at step=106, beta=0.756, caught=100, first_cap=0, bc=1.0759, cluster=0.00, idle=0.00, flip=0.32, same=0.12\n",
      "[DAgger-5] ep=031 finished at step=139, beta=0.750, caught=100, first_cap=0, bc=1.1212, cluster=0.03, idle=0.00, flip=0.20, same=0.52\n",
      "[DAgger-5] ep=032 finished at step=108, beta=0.745, caught=080, first_cap=0, bc=1.2598, cluster=0.00, idle=0.00, flip=0.34, same=0.05\n",
      "[DAgger-5] ep=033 finished at step=104, beta=0.739, caught=080, first_cap=0, bc=1.2666, cluster=0.01, idle=0.00, flip=0.45, same=0.25\n",
      "[DAgger-5] ep=034 finished at step=108, beta=0.734, caught=080, first_cap=0, bc=1.2643, cluster=0.00, idle=0.00, flip=0.37, same=0.08\n",
      "[DAgger-5] ep=035 finished at step=089, beta=0.728, caught=080, first_cap=0, bc=1.2371, cluster=0.01, idle=0.00, flip=0.29, same=0.02\n",
      "[DAgger-5] ep=036 finished at step=104, beta=0.723, caught=080, first_cap=0, bc=1.2328, cluster=0.00, idle=0.00, flip=0.30, same=0.03\n",
      "[DAgger-5] ep=037 finished at step=085, beta=0.718, caught=080, first_cap=0, bc=1.2179, cluster=0.00, idle=0.00, flip=0.28, same=0.14\n",
      "[DAgger-5] ep=038 finished at step=096, beta=0.712, caught=080, first_cap=1, bc=1.1658, cluster=0.00, idle=0.01, flip=0.25, same=0.11\n",
      "[DAgger-5] ep=039 finished at step=103, beta=0.707, caught=080, first_cap=1, bc=1.1019, cluster=0.00, idle=0.00, flip=0.43, same=0.10\n",
      "[DAgger-5] ep=040 finished at step=114, beta=0.702, caught=080, first_cap=0, bc=1.1208, cluster=0.01, idle=0.00, flip=0.31, same=0.30\n",
      "[DAgger-5] ep=041 finished at step=131, beta=0.697, caught=080, first_cap=0, bc=1.0981, cluster=0.01, idle=0.00, flip=0.28, same=0.20\n",
      "[DAgger-5] ep=042 finished at step=136, beta=0.692, caught=080, first_cap=0, bc=1.0791, cluster=0.03, idle=0.00, flip=0.22, same=0.73\n",
      "[DAgger-5] ep=043 finished at step=094, beta=0.687, caught=080, first_cap=0, bc=1.0876, cluster=0.02, idle=0.00, flip=0.36, same=0.20\n",
      "[DAgger-5] ep=044 finished at step=100, beta=0.682, caught=080, first_cap=0, bc=1.0949, cluster=0.00, idle=0.00, flip=0.22, same=0.16\n",
      "[DAgger-5] ep=045 finished at step=134, beta=0.677, caught=080, first_cap=0, bc=1.0459, cluster=0.00, idle=0.00, flip=0.28, same=0.38\n",
      "[DAgger-5] ep=046 finished at step=134, beta=0.672, caught=080, first_cap=0, bc=1.0515, cluster=0.01, idle=0.00, flip=0.22, same=0.47\n",
      "[DAgger-5] ep=047 finished at step=136, beta=0.667, caught=080, first_cap=0, bc=1.0585, cluster=0.03, idle=0.00, flip=0.24, same=0.34\n",
      "[DAgger-5] ep=048 finished at step=131, beta=0.662, caught=100, first_cap=0, bc=1.2002, cluster=0.00, idle=0.00, flip=0.34, same=0.11\n",
      "[DAgger-5] ep=049 finished at step=106, beta=0.657, caught=100, first_cap=0, bc=1.2120, cluster=0.01, idle=0.00, flip=0.37, same=0.04\n",
      "[DAgger-5] ep=050 finished at step=123, beta=0.652, caught=100, first_cap=0, bc=1.1854, cluster=0.00, idle=0.00, flip=0.39, same=0.02\n",
      "[DAgger-5] ep=051 finished at step=123, beta=0.647, caught=100, first_cap=0, bc=1.1578, cluster=0.00, idle=0.00, flip=0.34, same=0.03\n",
      "[DAgger-5] ep=052 finished at step=118, beta=0.642, caught=100, first_cap=0, bc=1.1954, cluster=0.00, idle=0.00, flip=0.30, same=0.13\n",
      "[DAgger-5] ep=053 finished at step=127, beta=0.638, caught=100, first_cap=0, bc=1.1917, cluster=0.02, idle=0.00, flip=0.31, same=0.22\n",
      "[DAgger-5] ep=054 finished at step=155, beta=0.633, caught=100, first_cap=0, bc=1.2515, cluster=0.03, idle=0.00, flip=0.38, same=0.41\n",
      "[DAgger-5] ep=055 finished at step=121, beta=0.628, caught=100, first_cap=1, bc=1.1583, cluster=0.02, idle=0.01, flip=0.43, same=0.11\n",
      "[DAgger-5] ep=056 finished at step=127, beta=0.624, caught=100, first_cap=0, bc=1.1259, cluster=0.00, idle=0.00, flip=0.27, same=0.16\n",
      "[DAgger-5] ep=057 finished at step=115, beta=0.619, caught=100, first_cap=0, bc=1.0976, cluster=0.00, idle=0.00, flip=0.28, same=0.08\n",
      "[DAgger-5] ep=058 finished at step=142, beta=0.615, caught=100, first_cap=0, bc=1.0745, cluster=0.01, idle=0.00, flip=0.26, same=0.46\n",
      "[DAgger-5] ep=059 finished at step=130, beta=0.610, caught=100, first_cap=0, bc=1.0743, cluster=0.02, idle=0.00, flip=0.22, same=0.28\n",
      "[DAgger-5] ep=060 finished at step=137, beta=0.606, caught=100, first_cap=0, bc=1.0666, cluster=0.00, idle=0.00, flip=0.26, same=0.15\n",
      "[DAgger-5] ep=061 finished at step=128, beta=0.601, caught=100, first_cap=2, bc=1.0883, cluster=0.00, idle=0.00, flip=0.40, same=0.22\n",
      "[DAgger-5] ep=062 finished at step=138, beta=0.597, caught=100, first_cap=2, bc=1.0751, cluster=0.00, idle=0.00, flip=0.27, same=0.35\n",
      "[DAgger-5] ep=063 finished at step=176, beta=0.592, caught=100, first_cap=0, bc=0.9595, cluster=0.12, idle=0.01, flip=0.22, same=0.72\n",
      "[DAgger-5] ep=064 finished at step=139, beta=0.588, caught=080, first_cap=2, bc=1.1526, cluster=0.00, idle=0.00, flip=0.32, same=0.01\n",
      "[DAgger-5] ep=065 finished at step=156, beta=0.584, caught=080, first_cap=1, bc=1.1808, cluster=0.00, idle=0.00, flip=0.36, same=0.03\n",
      "[DAgger-5] ep=066 finished at step=133, beta=0.579, caught=080, first_cap=0, bc=1.1897, cluster=0.00, idle=0.00, flip=0.34, same=0.01\n",
      "[DAgger-5] ep=067 finished at step=160, beta=0.575, caught=080, first_cap=1, bc=1.1569, cluster=0.00, idle=0.00, flip=0.27, same=0.05\n",
      "[DAgger-5] ep=068 finished at step=148, beta=0.571, caught=080, first_cap=0, bc=1.1378, cluster=0.01, idle=0.00, flip=0.21, same=0.13\n",
      "[DAgger-5] ep=069 finished at step=141, beta=0.567, caught=080, first_cap=4, bc=1.1635, cluster=0.04, idle=0.00, flip=0.36, same=0.09\n",
      "[DAgger-5] ep=070 finished at step=160, beta=0.563, caught=080, first_cap=0, bc=1.1762, cluster=0.00, idle=0.00, flip=0.43, same=0.07\n",
      "[DAgger-5] ep=071 finished at step=157, beta=0.558, caught=080, first_cap=0, bc=1.1181, cluster=0.02, idle=0.00, flip=0.35, same=0.25\n",
      "[DAgger-5] ep=072 finished at step=154, beta=0.554, caught=080, first_cap=0, bc=1.0537, cluster=0.01, idle=0.00, flip=0.25, same=0.19\n",
      "[DAgger-5] ep=073 finished at step=141, beta=0.550, caught=080, first_cap=0, bc=1.0342, cluster=0.00, idle=0.00, flip=0.32, same=0.11\n",
      "[DAgger-5] ep=074 finished at step=164, beta=0.546, caught=080, first_cap=0, bc=1.0780, cluster=0.01, idle=0.00, flip=0.31, same=0.27\n",
      "[DAgger-5] ep=075 finished at step=185, beta=0.542, caught=080, first_cap=1, bc=1.0067, cluster=0.00, idle=0.00, flip=0.23, same=0.25\n",
      "[DAgger-5] ep=076 finished at step=233, beta=0.538, caught=080, first_cap=0, bc=1.0593, cluster=0.05, idle=0.00, flip=0.29, same=0.46\n",
      "[DAgger-5] ep=077 finished at step=179, beta=0.534, caught=080, first_cap=0, bc=1.0155, cluster=0.00, idle=0.00, flip=0.22, same=0.62\n",
      "[DAgger-5] ep=078 finished at step=200, beta=0.530, caught=080, first_cap=0, bc=0.9848, cluster=0.00, idle=0.00, flip=0.20, same=0.20\n",
      "[DAgger-5] ep=079 finished at step=196, beta=0.526, caught=080, first_cap=1, bc=0.9960, cluster=0.02, idle=0.00, flip=0.26, same=0.32\n",
      "[DAgger-5] ep=080 finished at step=166, beta=0.523, caught=100, first_cap=0, bc=1.1345, cluster=0.00, idle=0.00, flip=0.33, same=0.02\n",
      "[DAgger-5] ep=081 finished at step=162, beta=0.519, caught=100, first_cap=0, bc=1.1251, cluster=0.02, idle=0.00, flip=0.36, same=0.17\n",
      "[DAgger-5] ep=082 finished at step=143, beta=0.515, caught=100, first_cap=0, bc=1.0854, cluster=0.00, idle=0.00, flip=0.31, same=0.03\n",
      "[DAgger-5] ep=083 finished at step=143, beta=0.511, caught=100, first_cap=0, bc=1.0934, cluster=0.00, idle=0.00, flip=0.28, same=0.11\n",
      "[DAgger-5] ep=084 finished at step=152, beta=0.507, caught=100, first_cap=2, bc=1.1201, cluster=0.01, idle=0.00, flip=0.37, same=0.09\n",
      "[DAgger-5] ep=085 finished at step=174, beta=0.504, caught=100, first_cap=1, bc=1.1100, cluster=0.01, idle=0.00, flip=0.38, same=0.09\n",
      "[DAgger-5] ep=086 finished at step=159, beta=0.500, caught=100, first_cap=1, bc=1.1334, cluster=0.00, idle=0.00, flip=0.34, same=0.07\n",
      "[DAgger-5] ep=087 finished at step=184, beta=0.496, caught=100, first_cap=0, bc=1.0533, cluster=0.00, idle=0.00, flip=0.33, same=0.11\n",
      "[DAgger-5] ep=088 finished at step=146, beta=0.493, caught=100, first_cap=2, bc=1.0754, cluster=0.00, idle=0.00, flip=0.43, same=0.10\n",
      "[DAgger-5] ep=089 finished at step=144, beta=0.489, caught=100, first_cap=3, bc=1.0408, cluster=0.00, idle=0.00, flip=0.23, same=0.19\n",
      "[DAgger-5] ep=090 finished at step=196, beta=0.485, caught=100, first_cap=0, bc=1.0336, cluster=0.00, idle=0.00, flip=0.34, same=0.16\n",
      "[DAgger-5] ep=091 finished at step=152, beta=0.482, caught=100, first_cap=0, bc=1.0280, cluster=0.01, idle=0.00, flip=0.27, same=0.37\n",
      "[DAgger-5] ep=092 finished at step=189, beta=0.478, caught=100, first_cap=0, bc=1.0076, cluster=0.01, idle=0.00, flip=0.25, same=0.26\n",
      "[DAgger-5] ep=093 finished at step=165, beta=0.475, caught=100, first_cap=0, bc=1.0154, cluster=0.00, idle=0.00, flip=0.23, same=0.07\n",
      "[DAgger-5] ep=094 finished at step=179, beta=0.471, caught=100, first_cap=0, bc=1.0050, cluster=0.01, idle=0.00, flip=0.29, same=0.30\n",
      "[DAgger-5] ep=095 finished at step=195, beta=0.468, caught=100, first_cap=0, bc=1.0148, cluster=0.01, idle=0.01, flip=0.33, same=0.11\n",
      "[DAgger-5] ep=096 finished at step=144, beta=0.464, caught=080, first_cap=0, bc=1.0632, cluster=0.00, idle=0.00, flip=0.38, same=0.08\n",
      "[DAgger-5] ep=097 finished at step=156, beta=0.461, caught=080, first_cap=1, bc=1.0621, cluster=0.00, idle=0.00, flip=0.36, same=0.02\n",
      "[DAgger-5] ep=098 finished at step=177, beta=0.458, caught=080, first_cap=1, bc=1.0845, cluster=0.00, idle=0.00, flip=0.41, same=0.08\n",
      "[DAgger-5] ep=099 finished at step=124, beta=0.454, caught=080, first_cap=0, bc=1.0774, cluster=0.00, idle=0.00, flip=0.28, same=0.02\n",
      "[DAgger-5] ep=100 finished at step=148, beta=0.451, caught=080, first_cap=0, bc=1.1106, cluster=0.00, idle=0.00, flip=0.35, same=0.12\n",
      "[DAgger-5] ep=101 finished at step=159, beta=0.448, caught=080, first_cap=1, bc=1.0951, cluster=0.01, idle=0.00, flip=0.36, same=0.16\n",
      "[DAgger-5] ep=102 finished at step=157, beta=0.444, caught=080, first_cap=1, bc=1.1067, cluster=0.03, idle=0.00, flip=0.35, same=0.15\n",
      "[DAgger-5] ep=103 finished at step=209, beta=0.441, caught=080, first_cap=0, bc=1.1007, cluster=0.00, idle=0.00, flip=0.30, same=0.12\n",
      "[DAgger-5] ep=104 finished at step=122, beta=0.438, caught=080, first_cap=0, bc=1.0077, cluster=0.00, idle=0.00, flip=0.24, same=0.31\n",
      "[DAgger-5] ep=105 finished at step=144, beta=0.435, caught=080, first_cap=0, bc=1.0106, cluster=0.00, idle=0.00, flip=0.27, same=0.26\n",
      "[DAgger-5] ep=106 finished at step=121, beta=0.431, caught=080, first_cap=0, bc=1.0077, cluster=0.03, idle=0.01, flip=0.33, same=0.08\n",
      "[DAgger-5] ep=107 finished at step=169, beta=0.428, caught=080, first_cap=0, bc=1.0141, cluster=0.05, idle=0.00, flip=0.29, same=0.15\n",
      "[DAgger-5] ep=108 finished at step=207, beta=0.425, caught=080, first_cap=0, bc=0.9948, cluster=0.03, idle=0.00, flip=0.27, same=0.27\n",
      "[DAgger-5] ep=109 finished at step=169, beta=0.422, caught=080, first_cap=0, bc=0.9607, cluster=0.00, idle=0.00, flip=0.27, same=0.32\n",
      "[DAgger-5] ep=110 finished at step=208, beta=0.419, caught=080, first_cap=2, bc=0.9475, cluster=0.04, idle=0.00, flip=0.25, same=0.42\n",
      "[DAgger-5] ep=111 finished at step=182, beta=0.416, caught=080, first_cap=2, bc=0.9758, cluster=0.01, idle=0.01, flip=0.28, same=0.41\n",
      "[DAgger-5] ep=112 finished at step=171, beta=0.413, caught=100, first_cap=0, bc=1.0389, cluster=0.02, idle=0.01, flip=0.37, same=0.03\n",
      "[DAgger-5] ep=113 finished at step=169, beta=0.410, caught=100, first_cap=3, bc=1.0214, cluster=0.00, idle=0.01, flip=0.34, same=0.01\n",
      "[DAgger-5] ep=114 finished at step=182, beta=0.407, caught=100, first_cap=3, bc=1.0760, cluster=0.00, idle=0.01, flip=0.33, same=0.05\n",
      "[DAgger-5] ep=115 finished at step=160, beta=0.404, caught=100, first_cap=2, bc=1.0074, cluster=0.00, idle=0.00, flip=0.34, same=0.04\n",
      "[DAgger-5] ep=116 finished at step=151, beta=0.401, caught=100, first_cap=0, bc=1.0764, cluster=0.01, idle=0.00, flip=0.31, same=0.09\n",
      "[DAgger-5] ep=117 finished at step=177, beta=0.398, caught=100, first_cap=3, bc=1.0553, cluster=0.00, idle=0.01, flip=0.35, same=0.03\n",
      "[DAgger-5] ep=118 finished at step=177, beta=0.395, caught=100, first_cap=0, bc=1.0984, cluster=0.00, idle=0.00, flip=0.37, same=0.05\n",
      "[DAgger-5] ep=119 finished at step=210, beta=0.392, caught=100, first_cap=1, bc=1.0750, cluster=0.05, idle=0.00, flip=0.37, same=0.27\n",
      "[DAgger-5] ep=120 finished at step=145, beta=0.389, caught=100, first_cap=0, bc=1.0000, cluster=0.00, idle=0.00, flip=0.31, same=0.00\n",
      "[DAgger-5] ep=121 finished at step=206, beta=0.386, caught=100, first_cap=1, bc=0.9624, cluster=0.00, idle=0.00, flip=0.23, same=0.38\n",
      "[DAgger-5] ep=122 finished at step=172, beta=0.383, caught=100, first_cap=2, bc=1.0133, cluster=0.00, idle=0.01, flip=0.36, same=0.08\n",
      "[DAgger-5] ep=123 finished at step=216, beta=0.380, caught=100, first_cap=0, bc=0.9355, cluster=0.00, idle=0.00, flip=0.32, same=0.13\n",
      "[DAgger-5] ep=124 finished at step=173, beta=0.378, caught=100, first_cap=0, bc=0.9370, cluster=0.05, idle=0.01, flip=0.26, same=0.45\n",
      "[DAgger-5] ep=125 finished at step=198, beta=0.375, caught=100, first_cap=0, bc=0.9630, cluster=0.00, idle=0.00, flip=0.27, same=0.21\n",
      "[DAgger-5] ep=126 finished at step=229, beta=0.372, caught=100, first_cap=0, bc=0.9983, cluster=0.02, idle=0.00, flip=0.27, same=0.29\n",
      "[DAgger-5] ep=127 finished at step=244, beta=0.369, caught=100, first_cap=0, bc=0.9758, cluster=0.02, idle=0.00, flip=0.31, same=0.27\n",
      "[DAgger-5] ep=128 finished at step=175, beta=0.367, caught=080, first_cap=3, bc=0.9974, cluster=0.00, idle=0.00, flip=0.35, same=0.10\n",
      "[DAgger-5] ep=129 finished at step=195, beta=0.364, caught=080, first_cap=2, bc=1.0374, cluster=0.01, idle=0.00, flip=0.31, same=0.04\n",
      "[DAgger-5] ep=130 finished at step=172, beta=0.361, caught=080, first_cap=1, bc=1.0501, cluster=0.01, idle=0.01, flip=0.26, same=0.00\n",
      "[DAgger-5] ep=131 finished at step=185, beta=0.359, caught=080, first_cap=0, bc=1.0847, cluster=0.01, idle=0.00, flip=0.30, same=0.10\n",
      "[DAgger-5] ep=132 finished at step=174, beta=0.356, caught=080, first_cap=0, bc=1.0609, cluster=0.00, idle=0.00, flip=0.31, same=0.06\n",
      "[DAgger-5] ep=133 finished at step=164, beta=0.353, caught=080, first_cap=1, bc=1.0424, cluster=0.00, idle=0.00, flip=0.33, same=0.10\n",
      "[DAgger-5] ep=134 finished at step=178, beta=0.351, caught=080, first_cap=4, bc=1.1038, cluster=0.01, idle=0.01, flip=0.32, same=0.07\n",
      "[DAgger-5] ep=135 finished at step=161, beta=0.348, caught=080, first_cap=3, bc=1.0951, cluster=0.01, idle=0.00, flip=0.30, same=0.10\n",
      "[DAgger-5] ep=136 finished at step=230, beta=0.346, caught=080, first_cap=1, bc=1.0086, cluster=0.02, idle=0.00, flip=0.34, same=0.07\n",
      "[DAgger-5] ep=137 finished at step=261, beta=0.343, caught=080, first_cap=1, bc=0.9438, cluster=0.00, idle=0.00, flip=0.27, same=0.20\n",
      "[DAgger-5] ep=138 finished at step=216, beta=0.341, caught=080, first_cap=0, bc=0.9488, cluster=0.00, idle=0.00, flip=0.29, same=0.17\n",
      "[DAgger-5] ep=139 finished at step=232, beta=0.338, caught=080, first_cap=3, bc=0.9409, cluster=0.02, idle=0.00, flip=0.29, same=0.30\n",
      "[DAgger-5] ep=140 finished at step=207, beta=0.336, caught=080, first_cap=0, bc=0.9433, cluster=0.02, idle=0.00, flip=0.30, same=0.35\n",
      "[DAgger-5] ep=141 finished at step=277, beta=0.333, caught=080, first_cap=3, bc=0.9637, cluster=0.00, idle=0.00, flip=0.26, same=0.32\n",
      "[DAgger-5] ep=142 finished at step=286, beta=0.331, caught=080, first_cap=2, bc=0.9247, cluster=0.05, idle=0.00, flip=0.26, same=0.39\n",
      "[DAgger-5] ep=143 finished at step=299, beta=0.328, caught=077, first_cap=0, bc=0.9260, cluster=0.02, idle=0.00, flip=0.26, same=0.41\n",
      "[DAgger-5] ep=144 finished at step=231, beta=0.326, caught=100, first_cap=0, bc=0.9583, cluster=0.01, idle=0.00, flip=0.27, same=0.00\n",
      "[DAgger-5] ep=145 finished at step=207, beta=0.323, caught=100, first_cap=2, bc=0.9869, cluster=0.00, idle=0.00, flip=0.33, same=0.04\n",
      "[DAgger-5] ep=146 finished at step=213, beta=0.321, caught=100, first_cap=2, bc=1.0352, cluster=0.00, idle=0.00, flip=0.35, same=0.01\n",
      "[DAgger-5] ep=147 finished at step=204, beta=0.319, caught=100, first_cap=1, bc=1.0607, cluster=0.00, idle=0.00, flip=0.29, same=0.04\n",
      "[DAgger-5] ep=148 finished at step=215, beta=0.316, caught=100, first_cap=2, bc=1.0625, cluster=0.00, idle=0.00, flip=0.32, same=0.01\n",
      "[DAgger-5] ep=149 finished at step=214, beta=0.314, caught=100, first_cap=1, bc=1.1056, cluster=0.00, idle=0.00, flip=0.38, same=0.11\n",
      "[DAgger-5] ep=150 finished at step=286, beta=0.312, caught=100, first_cap=4, bc=1.0488, cluster=0.01, idle=0.00, flip=0.33, same=0.06\n",
      "[DAgger-5] ep=151 finished at step=218, beta=0.309, caught=100, first_cap=1, bc=1.0389, cluster=0.01, idle=0.00, flip=0.38, same=0.13\n",
      "[DAgger-5] ep=152 finished at step=207, beta=0.307, caught=100, first_cap=1, bc=0.9343, cluster=0.03, idle=0.01, flip=0.27, same=0.25\n",
      "[DAgger-5] ep=153 finished at step=247, beta=0.305, caught=100, first_cap=1, bc=0.9300, cluster=0.00, idle=0.00, flip=0.34, same=0.05\n",
      "[DAgger-5] ep=154 finished at step=299, beta=0.303, caught=096, first_cap=5, bc=0.9074, cluster=0.03, idle=0.00, flip=0.27, same=0.41\n",
      "[DAgger-5] ep=155 finished at step=299, beta=0.300, caught=099, first_cap=3, bc=0.8760, cluster=0.00, idle=0.00, flip=0.24, same=0.38\n",
      "[DAgger-5] ep=156 finished at step=294, beta=0.298, caught=100, first_cap=0, bc=0.9295, cluster=0.02, idle=0.00, flip=0.32, same=0.16\n",
      "[DAgger-5] ep=157 finished at step=283, beta=0.296, caught=100, first_cap=3, bc=0.9382, cluster=0.00, idle=0.00, flip=0.33, same=0.10\n",
      "[DAgger-5] ep=158 finished at step=299, beta=0.294, caught=099, first_cap=2, bc=0.9458, cluster=0.01, idle=0.00, flip=0.37, same=0.10\n",
      "[DAgger-5] ep=159 finished at step=299, beta=0.292, caught=083, first_cap=1, bc=0.9616, cluster=0.01, idle=0.00, flip=0.32, same=0.24\n",
      "[DAgger-5] ep=160 finished at step=130, beta=0.290, caught=080, first_cap=1, bc=1.0062, cluster=0.04, idle=0.00, flip=0.33, same=0.05\n",
      "[DAgger-5] ep=161 finished at step=120, beta=0.287, caught=080, first_cap=2, bc=1.0229, cluster=0.00, idle=0.01, flip=0.35, same=0.02\n",
      "[DAgger-5] ep=162 finished at step=122, beta=0.285, caught=080, first_cap=2, bc=0.9936, cluster=0.01, idle=0.01, flip=0.32, same=0.15\n",
      "[DAgger-5] ep=163 finished at step=115, beta=0.283, caught=080, first_cap=0, bc=0.9929, cluster=0.00, idle=0.00, flip=0.39, same=0.12\n",
      "[DAgger-5] ep=164 finished at step=109, beta=0.281, caught=080, first_cap=1, bc=1.0433, cluster=0.00, idle=0.00, flip=0.35, same=0.02\n",
      "[DAgger-5] ep=165 finished at step=130, beta=0.279, caught=080, first_cap=0, bc=1.0104, cluster=0.05, idle=0.00, flip=0.32, same=0.16\n",
      "[DAgger-5] ep=166 finished at step=141, beta=0.277, caught=080, first_cap=0, bc=1.0682, cluster=0.07, idle=0.00, flip=0.32, same=0.35\n",
      "[DAgger-5] ep=167 finished at step=177, beta=0.275, caught=080, first_cap=0, bc=0.9928, cluster=0.04, idle=0.00, flip=0.37, same=0.24\n",
      "[DAgger-5] ep=168 finished at step=129, beta=0.273, caught=080, first_cap=0, bc=0.8886, cluster=0.04, idle=0.00, flip=0.27, same=0.10\n",
      "[DAgger-5] ep=169 finished at step=125, beta=0.271, caught=080, first_cap=1, bc=0.9798, cluster=0.00, idle=0.00, flip=0.33, same=0.33\n",
      "[DAgger-5] ep=170 finished at step=099, beta=0.269, caught=080, first_cap=0, bc=0.8797, cluster=0.00, idle=0.00, flip=0.35, same=0.18\n",
      "[DAgger-5] ep=171 finished at step=155, beta=0.267, caught=080, first_cap=0, bc=0.9379, cluster=0.10, idle=0.01, flip=0.27, same=0.53\n",
      "[DAgger-5] ep=172 finished at step=104, beta=0.265, caught=080, first_cap=0, bc=0.9147, cluster=0.00, idle=0.00, flip=0.29, same=0.10\n",
      "[DAgger-5] ep=173 finished at step=159, beta=0.263, caught=080, first_cap=0, bc=0.9785, cluster=0.03, idle=0.00, flip=0.33, same=0.24\n",
      "[DAgger-5] ep=174 finished at step=180, beta=0.261, caught=080, first_cap=0, bc=0.8124, cluster=0.13, idle=0.02, flip=0.25, same=0.69\n",
      "[DAgger-5] ep=175 finished at step=143, beta=0.259, caught=080, first_cap=0, bc=0.8711, cluster=0.00, idle=0.00, flip=0.36, same=0.26\n",
      "[DAgger-5] ep=176 finished at step=157, beta=0.257, caught=100, first_cap=0, bc=0.9989, cluster=0.01, idle=0.00, flip=0.35, same=0.10\n",
      "[DAgger-5] ep=177 finished at step=158, beta=0.255, caught=100, first_cap=0, bc=0.9703, cluster=0.01, idle=0.01, flip=0.38, same=0.16\n",
      "[DAgger-5] ep=178 finished at step=149, beta=0.254, caught=100, first_cap=0, bc=1.0321, cluster=0.00, idle=0.00, flip=0.29, same=0.11\n",
      "[DAgger-5] ep=179 finished at step=130, beta=0.252, caught=100, first_cap=0, bc=1.0300, cluster=0.00, idle=0.01, flip=0.37, same=0.11\n",
      "[DAgger-5] ep=180 finished at step=134, beta=0.250, caught=100, first_cap=2, bc=1.0056, cluster=0.00, idle=0.01, flip=0.34, same=0.10\n",
      "[DAgger-5] ep=181 finished at step=141, beta=0.248, caught=100, first_cap=0, bc=1.0057, cluster=0.04, idle=0.01, flip=0.35, same=0.14\n",
      "[DAgger-5] ep=182 finished at step=112, beta=0.246, caught=100, first_cap=1, bc=1.0489, cluster=0.00, idle=0.00, flip=0.33, same=0.11\n",
      "[DAgger-5] ep=183 finished at step=207, beta=0.244, caught=100, first_cap=3, bc=0.9902, cluster=0.02, idle=0.00, flip=0.35, same=0.27\n",
      "[DAgger-5] ep=184 finished at step=129, beta=0.243, caught=100, first_cap=0, bc=0.9494, cluster=0.00, idle=0.00, flip=0.41, same=0.24\n",
      "[DAgger-5] ep=185 finished at step=142, beta=0.241, caught=100, first_cap=2, bc=0.9110, cluster=0.00, idle=0.01, flip=0.41, same=0.17\n",
      "[DAgger-5] ep=186 finished at step=123, beta=0.239, caught=100, first_cap=1, bc=0.9158, cluster=0.02, idle=0.00, flip=0.28, same=0.32\n",
      "[DAgger-5] ep=187 finished at step=127, beta=0.237, caught=100, first_cap=2, bc=0.9634, cluster=0.00, idle=0.00, flip=0.31, same=0.09\n",
      "[DAgger-5] ep=188 finished at step=147, beta=0.236, caught=100, first_cap=0, bc=0.8858, cluster=0.01, idle=0.00, flip=0.22, same=0.47\n",
      "[DAgger-5] ep=189 finished at step=139, beta=0.234, caught=100, first_cap=0, bc=0.9256, cluster=0.01, idle=0.00, flip=0.29, same=0.20\n",
      "[DAgger-5] ep=190 finished at step=147, beta=0.232, caught=100, first_cap=0, bc=0.8954, cluster=0.01, idle=0.00, flip=0.42, same=0.30\n",
      "[DAgger-5] ep=191 finished at step=136, beta=0.230, caught=100, first_cap=0, bc=0.9073, cluster=0.01, idle=0.00, flip=0.39, same=0.27\n",
      "[DAgger-5] ep=192 finished at step=107, beta=0.229, caught=080, first_cap=3, bc=0.9671, cluster=0.02, idle=0.01, flip=0.26, same=0.06\n",
      "[DAgger-5] ep=193 finished at step=100, beta=0.227, caught=080, first_cap=0, bc=0.9420, cluster=0.00, idle=0.00, flip=0.25, same=0.05\n",
      "[DAgger-5] ep=194 finished at step=123, beta=0.225, caught=080, first_cap=0, bc=1.0326, cluster=0.00, idle=0.00, flip=0.33, same=0.12\n",
      "[DAgger-5] ep=195 finished at step=131, beta=0.224, caught=080, first_cap=5, bc=1.0001, cluster=0.01, idle=0.00, flip=0.30, same=0.10\n",
      "[DAgger-5] ep=196 finished at step=127, beta=0.222, caught=080, first_cap=2, bc=0.9669, cluster=0.02, idle=0.00, flip=0.32, same=0.13\n",
      "[DAgger-5] ep=197 finished at step=118, beta=0.220, caught=080, first_cap=0, bc=1.0188, cluster=0.00, idle=0.00, flip=0.38, same=0.06\n",
      "[DAgger-5] ep=198 finished at step=155, beta=0.219, caught=080, first_cap=1, bc=1.0096, cluster=0.00, idle=0.01, flip=0.46, same=0.14\n",
      "[DAgger-5] ep=199 finished at step=125, beta=0.217, caught=080, first_cap=2, bc=1.0381, cluster=0.01, idle=0.01, flip=0.37, same=0.11\n",
      "[DAgger-5] ep=200 finished at step=114, beta=0.216, caught=080, first_cap=2, bc=0.9249, cluster=0.02, idle=0.01, flip=0.43, same=0.23\n",
      "[DAgger-5] ep=201 finished at step=149, beta=0.214, caught=080, first_cap=0, bc=0.8900, cluster=0.02, idle=0.00, flip=0.31, same=0.27\n",
      "[DAgger-5] ep=202 finished at step=137, beta=0.212, caught=080, first_cap=0, bc=0.8953, cluster=0.00, idle=0.00, flip=0.46, same=0.29\n",
      "[DAgger-5] ep=203 finished at step=134, beta=0.211, caught=080, first_cap=2, bc=0.9221, cluster=0.00, idle=0.00, flip=0.28, same=0.23\n",
      "[DAgger-5] ep=204 finished at step=152, beta=0.209, caught=080, first_cap=4, bc=0.9214, cluster=0.01, idle=0.00, flip=0.27, same=0.52\n",
      "[DAgger-5] ep=205 finished at step=162, beta=0.208, caught=080, first_cap=0, bc=0.9633, cluster=0.03, idle=0.00, flip=0.28, same=0.38\n",
      "[DAgger-5] ep=206 finished at step=154, beta=0.206, caught=080, first_cap=0, bc=0.9394, cluster=0.05, idle=0.00, flip=0.29, same=0.35\n",
      "[DAgger-5] ep=207 finished at step=202, beta=0.205, caught=080, first_cap=3, bc=0.8952, cluster=0.03, idle=0.00, flip=0.27, same=0.52\n",
      "[DAgger-5] ep=208 finished at step=172, beta=0.203, caught=100, first_cap=2, bc=1.0090, cluster=0.00, idle=0.01, flip=0.36, same=0.17\n",
      "[DAgger-5] ep=209 finished at step=144, beta=0.202, caught=100, first_cap=3, bc=1.0262, cluster=0.00, idle=0.01, flip=0.32, same=0.06\n",
      "[DAgger-5] ep=210 finished at step=155, beta=0.200, caught=100, first_cap=2, bc=0.9812, cluster=0.00, idle=0.00, flip=0.31, same=0.04\n",
      "[DAgger-5] ep=211 finished at step=148, beta=0.199, caught=100, first_cap=2, bc=1.0377, cluster=0.00, idle=0.01, flip=0.34, same=0.09\n",
      "[DAgger-5] ep=212 finished at step=143, beta=0.197, caught=100, first_cap=0, bc=1.0396, cluster=0.00, idle=0.01, flip=0.39, same=0.01\n",
      "[DAgger-5] ep=213 finished at step=161, beta=0.196, caught=100, first_cap=2, bc=1.0456, cluster=0.00, idle=0.01, flip=0.44, same=0.06\n",
      "[DAgger-5] ep=214 finished at step=141, beta=0.194, caught=100, first_cap=0, bc=0.9895, cluster=0.00, idle=0.00, flip=0.54, same=0.00\n",
      "[DAgger-5] ep=215 finished at step=152, beta=0.193, caught=100, first_cap=0, bc=1.0529, cluster=0.00, idle=0.00, flip=0.39, same=0.09\n",
      "[DAgger-5] ep=216 finished at step=145, beta=0.192, caught=100, first_cap=1, bc=1.0217, cluster=0.00, idle=0.00, flip=0.34, same=0.16\n",
      "[DAgger-5] ep=217 finished at step=137, beta=0.190, caught=100, first_cap=1, bc=0.9679, cluster=0.01, idle=0.00, flip=0.35, same=0.30\n",
      "[DAgger-5] ep=218 finished at step=187, beta=0.189, caught=100, first_cap=1, bc=0.9534, cluster=0.02, idle=0.01, flip=0.29, same=0.21\n",
      "[DAgger-5] ep=219 finished at step=143, beta=0.187, caught=100, first_cap=0, bc=0.9203, cluster=0.06, idle=0.00, flip=0.19, same=0.34\n",
      "[DAgger-5] ep=220 finished at step=197, beta=0.186, caught=100, first_cap=0, bc=1.0008, cluster=0.19, idle=0.00, flip=0.29, same=0.63\n",
      "[DAgger-5] ep=221 finished at step=127, beta=0.185, caught=100, first_cap=0, bc=0.9434, cluster=0.01, idle=0.00, flip=0.27, same=0.27\n",
      "[DAgger-5] ep=222 finished at step=154, beta=0.183, caught=100, first_cap=0, bc=0.9591, cluster=0.01, idle=0.00, flip=0.35, same=0.10\n",
      "[DAgger-5] ep=223 finished at step=173, beta=0.182, caught=100, first_cap=1, bc=0.9624, cluster=0.00, idle=0.01, flip=0.39, same=0.10\n",
      "[DAgger-5] ep=224 finished at step=183, beta=0.181, caught=080, first_cap=2, bc=0.9511, cluster=0.00, idle=0.01, flip=0.34, same=0.06\n",
      "[DAgger-5] ep=225 finished at step=187, beta=0.179, caught=080, first_cap=2, bc=1.0461, cluster=0.01, idle=0.00, flip=0.29, same=0.12\n",
      "[DAgger-5] ep=226 finished at step=178, beta=0.178, caught=080, first_cap=2, bc=0.9736, cluster=0.00, idle=0.00, flip=0.31, same=0.03\n",
      "[DAgger-5] ep=227 finished at step=160, beta=0.177, caught=080, first_cap=0, bc=1.0382, cluster=0.00, idle=0.01, flip=0.25, same=0.01\n",
      "[DAgger-5] ep=228 finished at step=156, beta=0.175, caught=080, first_cap=1, bc=1.0532, cluster=0.00, idle=0.00, flip=0.34, same=0.08\n",
      "[DAgger-5] ep=229 finished at step=136, beta=0.174, caught=080, first_cap=7, bc=1.0732, cluster=0.01, idle=0.00, flip=0.28, same=0.16\n",
      "[DAgger-5] ep=230 finished at step=176, beta=0.173, caught=080, first_cap=2, bc=1.0386, cluster=0.01, idle=0.00, flip=0.41, same=0.14\n",
      "[DAgger-5] ep=231 finished at step=230, beta=0.171, caught=080, first_cap=0, bc=1.0034, cluster=0.00, idle=0.00, flip=0.38, same=0.08\n",
      "[DAgger-5] ep=232 finished at step=173, beta=0.170, caught=080, first_cap=2, bc=0.9752, cluster=0.01, idle=0.01, flip=0.33, same=0.09\n",
      "[DAgger-5] ep=233 finished at step=253, beta=0.169, caught=080, first_cap=1, bc=0.9825, cluster=0.01, idle=0.00, flip=0.36, same=0.14\n",
      "[DAgger-5] ep=234 finished at step=205, beta=0.168, caught=080, first_cap=0, bc=0.8973, cluster=0.00, idle=0.00, flip=0.30, same=0.22\n",
      "[DAgger-5] ep=235 finished at step=187, beta=0.166, caught=080, first_cap=2, bc=0.9172, cluster=0.01, idle=0.00, flip=0.35, same=0.19\n",
      "[DAgger-5] ep=236 finished at step=222, beta=0.165, caught=080, first_cap=2, bc=0.9664, cluster=0.01, idle=0.00, flip=0.34, same=0.26\n",
      "[DAgger-5] ep=237 finished at step=198, beta=0.164, caught=080, first_cap=4, bc=0.9215, cluster=0.00, idle=0.00, flip=0.52, same=0.05\n",
      "[DAgger-5] ep=238 finished at step=228, beta=0.163, caught=080, first_cap=2, bc=0.9612, cluster=0.00, idle=0.00, flip=0.29, same=0.31\n",
      "[DAgger-5] ep=239 finished at step=189, beta=0.162, caught=080, first_cap=1, bc=0.8896, cluster=0.00, idle=0.00, flip=0.37, same=0.08\n",
      "[DAgger-5] ep=240 finished at step=184, beta=0.160, caught=100, first_cap=2, bc=1.0003, cluster=0.01, idle=0.01, flip=0.29, same=0.31\n",
      "[DAgger-5] ep=241 finished at step=207, beta=0.159, caught=100, first_cap=2, bc=0.9749, cluster=0.00, idle=0.00, flip=0.31, same=0.03\n",
      "[DAgger-5] ep=242 finished at step=246, beta=0.158, caught=100, first_cap=1, bc=0.9848, cluster=0.00, idle=0.00, flip=0.40, same=0.04\n",
      "[DAgger-5] ep=243 finished at step=176, beta=0.157, caught=100, first_cap=0, bc=1.0673, cluster=0.00, idle=0.00, flip=0.36, same=0.05\n",
      "[DAgger-5] ep=244 finished at step=184, beta=0.156, caught=100, first_cap=2, bc=1.0340, cluster=0.00, idle=0.01, flip=0.34, same=0.03\n",
      "[DAgger-5] ep=245 finished at step=190, beta=0.155, caught=100, first_cap=3, bc=1.0375, cluster=0.00, idle=0.00, flip=0.36, same=0.06\n",
      "[DAgger-5] ep=246 finished at step=205, beta=0.154, caught=100, first_cap=3, bc=1.0845, cluster=0.00, idle=0.00, flip=0.38, same=0.14\n",
      "[DAgger-5] ep=247 finished at step=244, beta=0.152, caught=100, first_cap=0, bc=0.9739, cluster=0.01, idle=0.00, flip=0.35, same=0.10\n",
      "[DAgger-5] ep=248 finished at step=192, beta=0.151, caught=100, first_cap=0, bc=0.9652, cluster=0.00, idle=0.01, flip=0.37, same=0.11\n",
      "[DAgger-5] ep=249 finished at step=215, beta=0.150, caught=100, first_cap=1, bc=0.9388, cluster=0.03, idle=0.00, flip=0.35, same=0.07\n",
      "[DAgger-5] ep=250 finished at step=203, beta=0.149, caught=100, first_cap=1, bc=0.9392, cluster=0.00, idle=0.00, flip=0.38, same=0.20\n",
      "[DAgger-5] ep=251 finished at step=204, beta=0.148, caught=100, first_cap=0, bc=0.9314, cluster=0.01, idle=0.00, flip=0.40, same=0.19\n",
      "[DAgger-5] ep=252 finished at step=189, beta=0.147, caught=100, first_cap=0, bc=0.8674, cluster=0.02, idle=0.00, flip=0.37, same=0.14\n",
      "[DAgger-5] ep=253 finished at step=220, beta=0.146, caught=100, first_cap=2, bc=0.9517, cluster=0.02, idle=0.00, flip=0.38, same=0.38\n",
      "[DAgger-5] ep=254 finished at step=280, beta=0.145, caught=100, first_cap=2, bc=0.9563, cluster=0.00, idle=0.00, flip=0.33, same=0.25\n",
      "[DAgger-5] ep=255 finished at step=279, beta=0.144, caught=100, first_cap=2, bc=0.9400, cluster=0.00, idle=0.00, flip=0.36, same=0.25\n",
      "[DAgger-5] ep=256 finished at step=180, beta=0.143, caught=080, first_cap=2, bc=0.9634, cluster=0.01, idle=0.01, flip=0.30, same=0.04\n",
      "[DAgger-5] ep=257 finished at step=174, beta=0.142, caught=080, first_cap=3, bc=0.9420, cluster=0.00, idle=0.01, flip=0.30, same=0.00\n",
      "[DAgger-5] ep=258 finished at step=167, beta=0.140, caught=080, first_cap=2, bc=0.9976, cluster=0.00, idle=0.01, flip=0.37, same=0.00\n",
      "[DAgger-5] ep=259 finished at step=178, beta=0.139, caught=080, first_cap=2, bc=1.0084, cluster=0.00, idle=0.00, flip=0.37, same=0.01\n",
      "[DAgger-5] ep=260 finished at step=160, beta=0.138, caught=080, first_cap=2, bc=1.0555, cluster=0.00, idle=0.01, flip=0.33, same=0.02\n",
      "[DAgger-5] ep=261 finished at step=190, beta=0.137, caught=080, first_cap=2, bc=0.9843, cluster=0.00, idle=0.00, flip=0.36, same=0.14\n",
      "[DAgger-5] ep=262 finished at step=166, beta=0.136, caught=080, first_cap=0, bc=1.0075, cluster=0.01, idle=0.00, flip=0.31, same=0.04\n",
      "[DAgger-5] ep=263 finished at step=185, beta=0.135, caught=080, first_cap=3, bc=1.0477, cluster=0.00, idle=0.00, flip=0.42, same=0.13\n",
      "[DAgger-5] ep=264 finished at step=207, beta=0.134, caught=080, first_cap=2, bc=0.9639, cluster=0.01, idle=0.00, flip=0.35, same=0.15\n",
      "[DAgger-5] ep=265 finished at step=228, beta=0.133, caught=080, first_cap=0, bc=0.9465, cluster=0.03, idle=0.00, flip=0.36, same=0.17\n",
      "[DAgger-5] ep=266 finished at step=191, beta=0.132, caught=080, first_cap=0, bc=0.9193, cluster=0.01, idle=0.01, flip=0.46, same=0.13\n",
      "[DAgger-5] ep=267 finished at step=190, beta=0.131, caught=080, first_cap=0, bc=0.8974, cluster=0.00, idle=0.00, flip=0.36, same=0.08\n",
      "[DAgger-5] ep=268 finished at step=172, beta=0.130, caught=080, first_cap=0, bc=0.9335, cluster=0.01, idle=0.00, flip=0.34, same=0.14\n",
      "[DAgger-5] ep=269 finished at step=246, beta=0.130, caught=080, first_cap=3, bc=0.9284, cluster=0.00, idle=0.00, flip=0.43, same=0.06\n",
      "[DAgger-5] ep=270 finished at step=208, beta=0.129, caught=080, first_cap=4, bc=0.9014, cluster=0.01, idle=0.00, flip=0.40, same=0.18\n",
      "[DAgger-5] ep=271 finished at step=241, beta=0.128, caught=080, first_cap=1, bc=0.9577, cluster=0.01, idle=0.00, flip=0.44, same=0.24\n",
      "[DAgger-5] ep=272 finished at step=197, beta=0.127, caught=100, first_cap=1, bc=1.0046, cluster=0.00, idle=0.00, flip=0.29, same=0.03\n",
      "[DAgger-5] ep=273 finished at step=193, beta=0.126, caught=100, first_cap=3, bc=0.9838, cluster=0.00, idle=0.01, flip=0.32, same=0.07\n",
      "[DAgger-5] ep=274 finished at step=171, beta=0.125, caught=100, first_cap=2, bc=0.9805, cluster=0.00, idle=0.01, flip=0.31, same=0.03\n",
      "[DAgger-5] ep=275 finished at step=230, beta=0.124, caught=100, first_cap=2, bc=0.9964, cluster=0.00, idle=0.00, flip=0.38, same=0.09\n",
      "[DAgger-5] ep=276 finished at step=224, beta=0.123, caught=100, first_cap=0, bc=1.0614, cluster=0.00, idle=0.00, flip=0.23, same=0.14\n",
      "[DAgger-5] ep=277 finished at step=195, beta=0.122, caught=100, first_cap=3, bc=1.0040, cluster=0.00, idle=0.01, flip=0.35, same=0.09\n",
      "[DAgger-5] ep=278 finished at step=221, beta=0.121, caught=100, first_cap=2, bc=1.0256, cluster=0.02, idle=0.00, flip=0.33, same=0.05\n",
      "[DAgger-5] ep=279 finished at step=182, beta=0.120, caught=100, first_cap=1, bc=0.9886, cluster=0.01, idle=0.00, flip=0.43, same=0.06\n",
      "[DAgger-5] ep=280 finished at step=209, beta=0.119, caught=100, first_cap=0, bc=0.8869, cluster=0.00, idle=0.00, flip=0.40, same=0.12\n",
      "[DAgger-5] ep=281 finished at step=220, beta=0.119, caught=100, first_cap=1, bc=0.9678, cluster=0.00, idle=0.00, flip=0.37, same=0.12\n",
      "[DAgger-5] ep=282 finished at step=209, beta=0.118, caught=100, first_cap=0, bc=0.9600, cluster=0.00, idle=0.00, flip=0.43, same=0.10\n",
      "[DAgger-5] ep=283 finished at step=216, beta=0.117, caught=100, first_cap=3, bc=0.9134, cluster=0.00, idle=0.00, flip=0.41, same=0.24\n",
      "[DAgger-5] ep=284 finished at step=283, beta=0.116, caught=100, first_cap=0, bc=0.9102, cluster=0.01, idle=0.00, flip=0.42, same=0.14\n",
      "[DAgger-5] ep=285 finished at step=193, beta=0.115, caught=100, first_cap=0, bc=0.9059, cluster=0.00, idle=0.00, flip=0.43, same=0.05\n",
      "[DAgger-5] ep=286 finished at step=218, beta=0.114, caught=100, first_cap=0, bc=0.8500, cluster=0.00, idle=0.00, flip=0.38, same=0.19\n",
      "[DAgger-5] ep=287 finished at step=294, beta=0.113, caught=100, first_cap=2, bc=0.9031, cluster=0.00, idle=0.00, flip=0.49, same=0.07\n",
      "[DAgger-5] ep=288 finished at step=221, beta=0.113, caught=080, first_cap=7, bc=0.9268, cluster=0.00, idle=0.00, flip=0.36, same=0.03\n",
      "[DAgger-5] ep=289 finished at step=212, beta=0.112, caught=080, first_cap=2, bc=0.9937, cluster=0.01, idle=0.00, flip=0.26, same=0.22\n",
      "[DAgger-5] ep=290 finished at step=202, beta=0.111, caught=080, first_cap=2, bc=0.9741, cluster=0.00, idle=0.00, flip=0.34, same=0.01\n",
      "[DAgger-5] ep=291 finished at step=192, beta=0.110, caught=080, first_cap=4, bc=0.9919, cluster=0.00, idle=0.01, flip=0.34, same=0.00\n",
      "[DAgger-5] ep=292 finished at step=198, beta=0.109, caught=080, first_cap=6, bc=1.0513, cluster=0.00, idle=0.01, flip=0.32, same=0.01\n",
      "[DAgger-5] ep=293 finished at step=202, beta=0.109, caught=080, first_cap=2, bc=0.9957, cluster=0.00, idle=0.00, flip=0.29, same=0.10\n",
      "[DAgger-5] ep=294 finished at step=262, beta=0.108, caught=080, first_cap=3, bc=1.0908, cluster=0.00, idle=0.00, flip=0.36, same=0.11\n",
      "[DAgger-5] ep=295 finished at step=277, beta=0.107, caught=080, first_cap=2, bc=1.0868, cluster=0.03, idle=0.00, flip=0.41, same=0.19\n",
      "[DAgger-5] ep=296 finished at step=228, beta=0.106, caught=080, first_cap=0, bc=0.9681, cluster=0.00, idle=0.00, flip=0.37, same=0.21\n",
      "[DAgger-5] ep=297 finished at step=260, beta=0.105, caught=080, first_cap=0, bc=0.9212, cluster=0.00, idle=0.00, flip=0.47, same=0.03\n",
      "[DAgger-5] ep=298 finished at step=299, beta=0.105, caught=076, first_cap=2, bc=0.9181, cluster=0.04, idle=0.00, flip=0.40, same=0.22\n",
      "[DAgger-5] ep=299 finished at step=299, beta=0.104, caught=072, first_cap=2, bc=0.8979, cluster=0.01, idle=0.00, flip=0.41, same=0.16\n",
      "[DAgger-5] ep=300 finished at step=280, beta=0.103, caught=080, first_cap=0, bc=0.9193, cluster=0.00, idle=0.00, flip=0.44, same=0.09\n",
      "[DAgger-5] ep=301 finished at step=291, beta=0.102, caught=080, first_cap=7, bc=0.9357, cluster=0.02, idle=0.00, flip=0.41, same=0.25\n",
      "[DAgger-5] ep=302 finished at step=299, beta=0.102, caught=079, first_cap=3, bc=0.9144, cluster=0.00, idle=0.00, flip=0.49, same=0.08\n",
      "[DAgger-5] ep=303 finished at step=299, beta=0.101, caught=077, first_cap=1, bc=0.9262, cluster=0.00, idle=0.00, flip=0.46, same=0.12\n",
      "[DAgger-5] ep=304 finished at step=258, beta=0.100, caught=100, first_cap=1, bc=0.9685, cluster=0.00, idle=0.00, flip=0.27, same=0.09\n",
      "[DAgger-5] ep=305 finished at step=215, beta=0.099, caught=100, first_cap=1, bc=1.0267, cluster=0.00, idle=0.00, flip=0.31, same=0.02\n",
      "[DAgger-5] ep=306 finished at step=219, beta=0.099, caught=100, first_cap=2, bc=1.0206, cluster=0.00, idle=0.00, flip=0.30, same=0.05\n",
      "[DAgger-5] ep=307 finished at step=246, beta=0.098, caught=100, first_cap=2, bc=1.0574, cluster=0.00, idle=0.00, flip=0.30, same=0.02\n",
      "[DAgger-5] ep=308 finished at step=225, beta=0.097, caught=100, first_cap=1, bc=1.0640, cluster=0.00, idle=0.00, flip=0.34, same=0.08\n",
      "[DAgger-5] ep=309 finished at step=249, beta=0.096, caught=100, first_cap=3, bc=1.0506, cluster=0.00, idle=0.00, flip=0.36, same=0.07\n",
      "[DAgger-5] ep=310 finished at step=242, beta=0.096, caught=100, first_cap=2, bc=1.0535, cluster=0.00, idle=0.00, flip=0.43, same=0.05\n",
      "[DAgger-5] ep=311 finished at step=276, beta=0.095, caught=100, first_cap=1, bc=1.0211, cluster=0.00, idle=0.00, flip=0.34, same=0.09\n",
      "[DAgger-5] ep=312 finished at step=299, beta=0.094, caught=096, first_cap=2, bc=0.9261, cluster=0.00, idle=0.00, flip=0.47, same=0.12\n",
      "[DAgger-5] ep=313 finished at step=256, beta=0.094, caught=100, first_cap=1, bc=0.9263, cluster=0.00, idle=0.00, flip=0.37, same=0.03\n",
      "[DAgger-5] ep=314 finished at step=299, beta=0.093, caught=096, first_cap=2, bc=1.0189, cluster=0.00, idle=0.00, flip=0.43, same=0.21\n",
      "[DAgger-5] ep=315 finished at step=285, beta=0.092, caught=100, first_cap=2, bc=0.9190, cluster=0.01, idle=0.00, flip=0.47, same=0.06\n",
      "[DAgger-5] ep=316 finished at step=299, beta=0.092, caught=096, first_cap=1, bc=0.9029, cluster=0.00, idle=0.00, flip=0.51, same=0.13\n",
      "[DAgger-5] ep=317 finished at step=299, beta=0.091, caught=089, first_cap=0, bc=0.9520, cluster=0.03, idle=0.00, flip=0.34, same=0.12\n",
      "[DAgger-5] ep=318 finished at step=299, beta=0.090, caught=094, first_cap=0, bc=0.8824, cluster=0.00, idle=0.00, flip=0.44, same=0.08\n",
      "[DAgger-5] ep=319 finished at step=299, beta=0.090, caught=097, first_cap=5, bc=0.9015, cluster=0.01, idle=0.00, flip=0.47, same=0.06\n",
      "[DAgger-5] ep=320 finished at step=162, beta=0.089, caught=080, first_cap=4, bc=0.9547, cluster=0.01, idle=0.01, flip=0.33, same=0.17\n",
      "[DAgger-5] ep=321 finished at step=161, beta=0.088, caught=080, first_cap=1, bc=0.9566, cluster=0.01, idle=0.00, flip=0.30, same=0.10\n",
      "[DAgger-5] ep=322 finished at step=147, beta=0.088, caught=080, first_cap=2, bc=0.9898, cluster=0.00, idle=0.01, flip=0.41, same=0.04\n",
      "[DAgger-5] ep=323 finished at step=148, beta=0.087, caught=080, first_cap=0, bc=1.0331, cluster=0.01, idle=0.00, flip=0.26, same=0.11\n",
      "[DAgger-5] ep=324 finished at step=166, beta=0.086, caught=080, first_cap=1, bc=1.0711, cluster=0.00, idle=0.01, flip=0.32, same=0.09\n",
      "[DAgger-5] ep=325 finished at step=133, beta=0.086, caught=080, first_cap=3, bc=1.0194, cluster=0.01, idle=0.01, flip=0.39, same=0.16\n",
      "[DAgger-5] ep=326 finished at step=148, beta=0.085, caught=080, first_cap=2, bc=1.0258, cluster=0.01, idle=0.01, flip=0.36, same=0.15\n",
      "[DAgger-5] ep=327 finished at step=172, beta=0.084, caught=080, first_cap=0, bc=1.0419, cluster=0.00, idle=0.01, flip=0.39, same=0.06\n",
      "[DAgger-5] ep=328 finished at step=155, beta=0.084, caught=080, first_cap=0, bc=0.9477, cluster=0.00, idle=0.00, flip=0.38, same=0.21\n",
      "[DAgger-5] ep=329 finished at step=177, beta=0.083, caught=080, first_cap=0, bc=0.9601, cluster=0.01, idle=0.00, flip=0.42, same=0.14\n",
      "[DAgger-5] ep=330 finished at step=120, beta=0.083, caught=080, first_cap=0, bc=0.9801, cluster=0.02, idle=0.00, flip=0.33, same=0.14\n",
      "[DAgger-5] ep=331 finished at step=167, beta=0.082, caught=080, first_cap=1, bc=0.9030, cluster=0.02, idle=0.00, flip=0.49, same=0.10\n",
      "[DAgger-5] ep=332 finished at step=158, beta=0.081, caught=080, first_cap=0, bc=0.8749, cluster=0.04, idle=0.00, flip=0.43, same=0.53\n",
      "[DAgger-5] ep=333 finished at step=178, beta=0.081, caught=080, first_cap=0, bc=0.9227, cluster=0.02, idle=0.00, flip=0.35, same=0.24\n",
      "[DAgger-5] ep=334 finished at step=183, beta=0.080, caught=080, first_cap=0, bc=0.9483, cluster=0.02, idle=0.00, flip=0.39, same=0.21\n",
      "[DAgger-5] ep=335 finished at step=176, beta=0.080, caught=080, first_cap=0, bc=0.9768, cluster=0.06, idle=0.00, flip=0.36, same=0.18\n",
      "[DAgger-5] ep=336 finished at step=146, beta=0.079, caught=100, first_cap=2, bc=0.9815, cluster=0.00, idle=0.01, flip=0.38, same=0.01\n",
      "[DAgger-5] ep=337 finished at step=141, beta=0.078, caught=100, first_cap=2, bc=1.0166, cluster=0.01, idle=0.01, flip=0.29, same=0.04\n",
      "[DAgger-5] ep=338 finished at step=157, beta=0.078, caught=100, first_cap=2, bc=1.0028, cluster=0.00, idle=0.01, flip=0.25, same=0.13\n",
      "[DAgger-5] ep=339 finished at step=157, beta=0.077, caught=100, first_cap=2, bc=1.0579, cluster=0.00, idle=0.00, flip=0.42, same=0.05\n",
      "[DAgger-5] ep=340 finished at step=158, beta=0.077, caught=100, first_cap=0, bc=1.0650, cluster=0.03, idle=0.01, flip=0.30, same=0.07\n",
      "[DAgger-5] ep=341 finished at step=153, beta=0.076, caught=100, first_cap=1, bc=1.0378, cluster=0.01, idle=0.00, flip=0.38, same=0.08\n",
      "[DAgger-5] ep=342 finished at step=160, beta=0.076, caught=100, first_cap=2, bc=0.9948, cluster=0.04, idle=0.00, flip=0.37, same=0.19\n",
      "[DAgger-5] ep=343 finished at step=181, beta=0.075, caught=100, first_cap=2, bc=1.0103, cluster=0.02, idle=0.00, flip=0.50, same=0.20\n",
      "[DAgger-5] ep=344 finished at step=154, beta=0.074, caught=100, first_cap=0, bc=0.9243, cluster=0.00, idle=0.01, flip=0.35, same=0.05\n",
      "[DAgger-5] ep=345 finished at step=237, beta=0.074, caught=100, first_cap=0, bc=1.0279, cluster=0.16, idle=0.01, flip=0.29, same=0.29\n",
      "[DAgger-5] ep=346 finished at step=207, beta=0.073, caught=100, first_cap=0, bc=0.9909, cluster=0.04, idle=0.01, flip=0.45, same=0.32\n",
      "[DAgger-5] ep=347 finished at step=152, beta=0.073, caught=100, first_cap=2, bc=0.9316, cluster=0.00, idle=0.01, flip=0.33, same=0.05\n",
      "[DAgger-5] ep=348 finished at step=183, beta=0.072, caught=100, first_cap=0, bc=0.9270, cluster=0.09, idle=0.00, flip=0.28, same=0.73\n",
      "[DAgger-5] ep=349 finished at step=160, beta=0.072, caught=100, first_cap=1, bc=0.8538, cluster=0.00, idle=0.01, flip=0.36, same=0.16\n",
      "[DAgger-5] ep=350 finished at step=153, beta=0.071, caught=100, first_cap=0, bc=0.9449, cluster=0.01, idle=0.00, flip=0.36, same=0.23\n",
      "[DAgger-5] ep=351 finished at step=147, beta=0.071, caught=100, first_cap=1, bc=0.9567, cluster=0.01, idle=0.00, flip=0.43, same=0.19\n",
      "[DAgger-5] ep=352 finished at step=163, beta=0.070, caught=080, first_cap=0, bc=0.9191, cluster=0.00, idle=0.00, flip=0.34, same=0.02\n",
      "[DAgger-5] ep=353 finished at step=138, beta=0.070, caught=080, first_cap=3, bc=1.0528, cluster=0.00, idle=0.00, flip=0.32, same=0.01\n",
      "[DAgger-5] ep=354 finished at step=123, beta=0.069, caught=080, first_cap=3, bc=0.9493, cluster=0.02, idle=0.01, flip=0.33, same=0.01\n",
      "[DAgger-5] ep=355 finished at step=147, beta=0.069, caught=080, first_cap=2, bc=1.0959, cluster=0.00, idle=0.00, flip=0.43, same=0.10\n",
      "[DAgger-5] ep=356 finished at step=133, beta=0.068, caught=080, first_cap=4, bc=0.9825, cluster=0.00, idle=0.01, flip=0.31, same=0.09\n",
      "[DAgger-5] ep=357 finished at step=141, beta=0.068, caught=080, first_cap=0, bc=1.0311, cluster=0.00, idle=0.00, flip=0.39, same=0.09\n",
      "[DAgger-5] ep=358 finished at step=219, beta=0.067, caught=080, first_cap=0, bc=0.9878, cluster=0.26, idle=0.24, flip=0.31, same=0.15\n",
      "[DAgger-5] ep=359 finished at step=223, beta=0.067, caught=080, first_cap=0, bc=1.0206, cluster=0.00, idle=0.00, flip=0.46, same=0.12\n",
      "[DAgger-5] ep=360 finished at step=148, beta=0.066, caught=080, first_cap=3, bc=0.9222, cluster=0.02, idle=0.01, flip=0.38, same=0.04\n",
      "[DAgger-5] ep=361 finished at step=141, beta=0.066, caught=080, first_cap=1, bc=0.9857, cluster=0.01, idle=0.00, flip=0.42, same=0.11\n",
      "[DAgger-5] ep=362 finished at step=147, beta=0.065, caught=080, first_cap=2, bc=0.9382, cluster=0.00, idle=0.01, flip=0.52, same=0.10\n",
      "[DAgger-5] ep=363 finished at step=147, beta=0.065, caught=080, first_cap=1, bc=0.9058, cluster=0.01, idle=0.01, flip=0.35, same=0.24\n",
      "[DAgger-5] ep=364 finished at step=138, beta=0.064, caught=080, first_cap=0, bc=0.9329, cluster=0.01, idle=0.01, flip=0.29, same=0.25\n",
      "[DAgger-5] ep=365 finished at step=144, beta=0.064, caught=080, first_cap=0, bc=0.9314, cluster=0.05, idle=0.01, flip=0.39, same=0.14\n",
      "[DAgger-5] ep=366 finished at step=141, beta=0.063, caught=080, first_cap=0, bc=0.8906, cluster=0.01, idle=0.00, flip=0.49, same=0.10\n",
      "[DAgger-5] ep=367 finished at step=190, beta=0.063, caught=080, first_cap=0, bc=0.9145, cluster=0.03, idle=0.01, flip=0.39, same=0.40\n",
      "[DAgger-5] ep=368 finished at step=158, beta=0.062, caught=100, first_cap=2, bc=0.9615, cluster=0.00, idle=0.01, flip=0.25, same=0.01\n",
      "[DAgger-5] ep=369 finished at step=149, beta=0.062, caught=100, first_cap=2, bc=0.9655, cluster=0.00, idle=0.01, flip=0.27, same=0.15\n",
      "[DAgger-5] ep=370 finished at step=173, beta=0.061, caught=100, first_cap=1, bc=1.0486, cluster=0.02, idle=0.01, flip=0.32, same=0.10\n",
      "[DAgger-5] ep=371 finished at step=152, beta=0.061, caught=100, first_cap=2, bc=1.0234, cluster=0.03, idle=0.01, flip=0.31, same=0.18\n",
      "[DAgger-5] ep=372 finished at step=165, beta=0.061, caught=100, first_cap=1, bc=1.0253, cluster=0.01, idle=0.00, flip=0.43, same=0.13\n",
      "[DAgger-5] ep=373 finished at step=175, beta=0.060, caught=100, first_cap=2, bc=1.0580, cluster=0.04, idle=0.01, flip=0.34, same=0.03\n",
      "[DAgger-5] ep=374 finished at step=160, beta=0.060, caught=100, first_cap=0, bc=1.0241, cluster=0.00, idle=0.00, flip=0.35, same=0.03\n",
      "[DAgger-5] ep=375 finished at step=180, beta=0.059, caught=100, first_cap=2, bc=1.0833, cluster=0.01, idle=0.01, flip=0.52, same=0.04\n",
      "[DAgger-5] ep=376 finished at step=170, beta=0.059, caught=100, first_cap=1, bc=0.9709, cluster=0.04, idle=0.01, flip=0.34, same=0.27\n",
      "[DAgger-5] ep=377 finished at step=146, beta=0.058, caught=100, first_cap=0, bc=0.8983, cluster=0.00, idle=0.00, flip=0.36, same=0.14\n",
      "[DAgger-5] ep=378 finished at step=144, beta=0.058, caught=100, first_cap=0, bc=0.8843, cluster=0.00, idle=0.00, flip=0.32, same=0.13\n",
      "[DAgger-5] ep=379 finished at step=186, beta=0.058, caught=100, first_cap=0, bc=0.9198, cluster=0.06, idle=0.01, flip=0.38, same=0.21\n",
      "[DAgger-5] ep=380 finished at step=179, beta=0.057, caught=100, first_cap=1, bc=0.9795, cluster=0.00, idle=0.00, flip=0.45, same=0.13\n",
      "[DAgger-5] ep=381 finished at step=211, beta=0.057, caught=100, first_cap=0, bc=0.9056, cluster=0.04, idle=0.01, flip=0.37, same=0.06\n",
      "[DAgger-5] ep=382 finished at step=205, beta=0.056, caught=100, first_cap=0, bc=0.8957, cluster=0.00, idle=0.00, flip=0.40, same=0.07\n",
      "[DAgger-5] ep=383 finished at step=202, beta=0.056, caught=100, first_cap=5, bc=0.8661, cluster=0.04, idle=0.01, flip=0.46, same=0.20\n",
      "[DAgger-5] ep=384 finished at step=187, beta=0.055, caught=080, first_cap=3, bc=0.9677, cluster=0.00, idle=0.01, flip=0.36, same=0.03\n",
      "[DAgger-5] ep=385 finished at step=162, beta=0.055, caught=080, first_cap=3, bc=1.0046, cluster=0.00, idle=0.01, flip=0.34, same=0.07\n",
      "[DAgger-5] ep=386 finished at step=190, beta=0.055, caught=080, first_cap=2, bc=0.9594, cluster=0.00, idle=0.01, flip=0.32, same=0.08\n",
      "[DAgger-5] ep=387 finished at step=183, beta=0.054, caught=080, first_cap=3, bc=1.0184, cluster=0.00, idle=0.01, flip=0.32, same=0.10\n",
      "[DAgger-5] ep=388 finished at step=207, beta=0.054, caught=080, first_cap=2, bc=1.0078, cluster=0.00, idle=0.00, flip=0.34, same=0.03\n",
      "[DAgger-5] ep=389 finished at step=174, beta=0.053, caught=080, first_cap=5, bc=1.0734, cluster=0.01, idle=0.00, flip=0.40, same=0.12\n",
      "[DAgger-5] ep=390 finished at step=177, beta=0.053, caught=080, first_cap=3, bc=1.0232, cluster=0.02, idle=0.01, flip=0.39, same=0.07\n",
      "[DAgger-5] ep=391 finished at step=178, beta=0.053, caught=080, first_cap=4, bc=1.0218, cluster=0.01, idle=0.00, flip=0.37, same=0.07\n",
      "[DAgger-5] ep=392 finished at step=223, beta=0.052, caught=080, first_cap=2, bc=0.9159, cluster=0.02, idle=0.00, flip=0.37, same=0.32\n",
      "[DAgger-5] ep=393 finished at step=249, beta=0.052, caught=080, first_cap=5, bc=0.9064, cluster=0.07, idle=0.01, flip=0.46, same=0.08\n",
      "[DAgger-5] ep=394 finished at step=265, beta=0.051, caught=080, first_cap=0, bc=0.8911, cluster=0.01, idle=0.00, flip=0.48, same=0.10\n",
      "[DAgger-5] ep=395 finished at step=257, beta=0.051, caught=080, first_cap=2, bc=0.8886, cluster=0.04, idle=0.00, flip=0.49, same=0.26\n",
      "[DAgger-5] ep=396 finished at step=299, beta=0.051, caught=079, first_cap=0, bc=1.0144, cluster=0.05, idle=0.00, flip=0.53, same=0.10\n",
      "[DAgger-5] ep=397 finished at step=298, beta=0.050, caught=080, first_cap=1, bc=0.8864, cluster=0.37, idle=0.01, flip=0.35, same=0.32\n",
      "[DAgger-5] ep=398 finished at step=268, beta=0.050, caught=080, first_cap=6, bc=1.0273, cluster=0.01, idle=0.00, flip=0.46, same=0.22\n",
      "[DAgger-5] ep=399 finished at step=299, beta=0.050, caught=078, first_cap=0, bc=0.9329, cluster=0.13, idle=0.00, flip=0.34, same=0.47\n",
      "Solo checkpoint: C:\\Users\\aveexela\\Desktop\\rl_project\\logs\\checkpoints\\agent_solo.pkl\n",
      "[STAGE 2] PvP DAgger vs ClosestTarget...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a132a6eb0694533996c74fda810ef82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DAgger-PvP:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_000.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep000.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_000.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep000.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=000 step=054 beta=0.595 caught=080 first_cap=2 bc=0.9730 cluster=0.04 idle=0.00 flip=0.35 same=0.22 score_us-bot=43.2-44.0 result=LOSS\n",
      "[DAgger-PvP] ep=001 step=066 beta=0.590 caught=080 first_cap=0 bc=1.0468 cluster=0.01 idle=0.01 flip=0.30 same=0.18 score_us-bot=51.0-35.0 result=WIN\n",
      "[DAgger-PvP] ep=002 step=071 beta=0.585 caught=080 first_cap=0 bc=1.0227 cluster=0.00 idle=0.00 flip=0.28 same=0.03 score_us-bot=56.0-34.8 result=WIN\n",
      "[DAgger-PvP] ep=003 step=054 beta=0.580 caught=080 first_cap=1 bc=1.0554 cluster=0.00 idle=0.00 flip=0.38 same=0.05 score_us-bot=44.5-44.0 result=WIN\n",
      "[DAgger-PvP] ep=004 step=061 beta=0.576 caught=080 first_cap=1 bc=1.0181 cluster=0.00 idle=0.00 flip=0.29 same=0.13 score_us-bot=40.0-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=005 step=066 beta=0.571 caught=080 first_cap=2 bc=0.9904 cluster=0.01 idle=0.01 flip=0.28 same=0.25 score_us-bot=45.8-53.2 result=LOSS\n",
      "[DAgger-PvP] ep=006 step=064 beta=0.566 caught=080 first_cap=0 bc=1.0324 cluster=0.00 idle=0.00 flip=0.26 same=0.05 score_us-bot=48.8-54.2 result=LOSS\n",
      "[DAgger-PvP] ep=007 step=063 beta=0.561 caught=080 first_cap=0 bc=1.0018 cluster=0.00 idle=0.00 flip=0.27 same=0.22 score_us-bot=50.8-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=008 step=055 beta=0.557 caught=080 first_cap=0 bc=0.9800 cluster=0.05 idle=0.00 flip=0.16 same=0.77 score_us-bot=45.0-38.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_009.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep009.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_009.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep009.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=009 step=060 beta=0.552 caught=080 first_cap=0 bc=0.9632 cluster=0.02 idle=0.00 flip=0.33 same=0.03 score_us-bot=48.0-35.0 result=WIN\n",
      "[DAgger-PvP] ep=010 step=057 beta=0.548 caught=080 first_cap=1 bc=0.9091 cluster=0.00 idle=0.00 flip=0.21 same=0.10 score_us-bot=41.0-42.8 result=LOSS\n",
      "[DAgger-PvP] ep=011 step=087 beta=0.543 caught=080 first_cap=0 bc=0.8896 cluster=0.02 idle=0.00 flip=0.45 same=0.16 score_us-bot=53.0-46.5 result=WIN\n",
      "[DAgger-PvP] ep=012 step=059 beta=0.539 caught=080 first_cap=0 bc=0.9252 cluster=0.00 idle=0.00 flip=0.23 same=0.32 score_us-bot=34.0-58.0 result=LOSS\n",
      "[DAgger-PvP] ep=013 step=060 beta=0.534 caught=080 first_cap=2 bc=0.9967 cluster=0.00 idle=0.00 flip=0.28 same=0.05 score_us-bot=52.5-46.0 result=WIN\n",
      "[DAgger-PvP] ep=014 step=053 beta=0.530 caught=080 first_cap=0 bc=0.9874 cluster=0.02 idle=0.00 flip=0.35 same=0.33 score_us-bot=46.0-40.0 result=WIN\n",
      "[DAgger-PvP] ep=015 step=073 beta=0.525 caught=080 first_cap=1 bc=1.0690 cluster=0.00 idle=0.00 flip=0.26 same=0.12 score_us-bot=47.0-41.0 result=WIN\n",
      "[DAgger-PvP] ep=016 step=055 beta=0.521 caught=080 first_cap=0 bc=0.9827 cluster=0.02 idle=0.00 flip=0.34 same=0.09 score_us-bot=50.0-44.0 result=WIN\n",
      "[DAgger-PvP] ep=017 step=053 beta=0.517 caught=080 first_cap=0 bc=1.0784 cluster=0.00 idle=0.00 flip=0.30 same=0.09 score_us-bot=48.2-41.0 result=WIN\n",
      "[DAgger-PvP] ep=018 step=095 beta=0.512 caught=080 first_cap=0 bc=1.0352 cluster=0.00 idle=0.00 flip=0.32 same=0.11 score_us-bot=64.0-30.2 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_019.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep019.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_019.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep019.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=019 step=052 beta=0.508 caught=080 first_cap=0 bc=0.9283 cluster=0.02 idle=0.00 flip=0.45 same=0.34 score_us-bot=45.8-44.5 result=WIN\n",
      "[DAgger-PvP] ep=020 step=068 beta=0.504 caught=080 first_cap=1 bc=0.9799 cluster=0.00 idle=0.00 flip=0.30 same=0.20 score_us-bot=52.5-46.5 result=WIN\n",
      "[DAgger-PvP] ep=021 step=049 beta=0.500 caught=080 first_cap=0 bc=0.9552 cluster=0.02 idle=0.02 flip=0.28 same=0.10 score_us-bot=41.0-45.0 result=LOSS\n",
      "[DAgger-PvP] ep=022 step=056 beta=0.496 caught=080 first_cap=0 bc=0.9376 cluster=0.09 idle=0.00 flip=0.30 same=0.46 score_us-bot=47.0-44.0 result=WIN\n",
      "[DAgger-PvP] ep=023 step=073 beta=0.492 caught=080 first_cap=0 bc=0.9401 cluster=0.05 idle=0.00 flip=0.23 same=0.84 score_us-bot=54.8-40.2 result=WIN\n",
      "[DAgger-PvP] ep=024 step=062 beta=0.487 caught=100 first_cap=1 bc=0.9722 cluster=0.00 idle=0.00 flip=0.17 same=0.11 score_us-bot=64.5-64.0 result=WIN\n",
      "[DAgger-PvP] ep=025 step=068 beta=0.483 caught=100 first_cap=0 bc=1.0828 cluster=0.00 idle=0.00 flip=0.26 same=0.03 score_us-bot=62.5-59.0 result=WIN\n",
      "[DAgger-PvP] ep=026 step=070 beta=0.479 caught=100 first_cap=0 bc=1.0442 cluster=0.00 idle=0.00 flip=0.31 same=0.08 score_us-bot=55.0-70.2 result=LOSS\n",
      "[DAgger-PvP] ep=027 step=068 beta=0.475 caught=100 first_cap=0 bc=0.9659 cluster=0.01 idle=0.00 flip=0.52 same=0.06 score_us-bot=59.0-64.0 result=LOSS\n",
      "[DAgger-PvP] ep=028 step=067 beta=0.471 caught=100 first_cap=1 bc=1.0116 cluster=0.00 idle=0.00 flip=0.44 same=0.10 score_us-bot=59.2-53.5 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_029.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep029.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_029.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep029.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=029 step=056 beta=0.468 caught=100 first_cap=0 bc=0.9968 cluster=0.00 idle=0.00 flip=0.51 same=0.32 score_us-bot=53.0-53.0 result=DRAW\n",
      "[DAgger-PvP] ep=030 step=066 beta=0.464 caught=100 first_cap=0 bc=1.0230 cluster=0.07 idle=0.01 flip=0.28 same=0.16 score_us-bot=54.2-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=031 step=070 beta=0.460 caught=100 first_cap=0 bc=0.9636 cluster=0.00 idle=0.00 flip=0.39 same=0.14 score_us-bot=54.2-54.0 result=WIN\n",
      "[DAgger-PvP] ep=032 step=071 beta=0.456 caught=100 first_cap=0 bc=1.0037 cluster=0.03 idle=0.01 flip=0.29 same=0.22 score_us-bot=61.0-62.8 result=LOSS\n",
      "[DAgger-PvP] ep=033 step=073 beta=0.452 caught=100 first_cap=0 bc=0.9017 cluster=0.03 idle=0.00 flip=0.27 same=0.23 score_us-bot=58.5-57.0 result=WIN\n",
      "[DAgger-PvP] ep=034 step=069 beta=0.449 caught=100 first_cap=0 bc=0.9456 cluster=0.04 idle=0.00 flip=0.44 same=0.33 score_us-bot=61.2-64.0 result=LOSS\n",
      "[DAgger-PvP] ep=035 step=083 beta=0.445 caught=100 first_cap=0 bc=0.8952 cluster=0.00 idle=0.00 flip=0.20 same=0.13 score_us-bot=72.0-61.2 result=WIN\n",
      "[DAgger-PvP] ep=036 step=075 beta=0.441 caught=100 first_cap=0 bc=0.9704 cluster=0.00 idle=0.00 flip=0.21 same=0.05 score_us-bot=63.2-61.0 result=WIN\n",
      "[DAgger-PvP] ep=037 step=064 beta=0.438 caught=100 first_cap=0 bc=0.9866 cluster=0.02 idle=0.00 flip=0.34 same=0.08 score_us-bot=50.0-56.2 result=LOSS\n",
      "[DAgger-PvP] ep=038 step=067 beta=0.434 caught=100 first_cap=1 bc=1.0515 cluster=0.00 idle=0.00 flip=0.28 same=0.04 score_us-bot=62.8-62.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_039.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep039.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_039.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep039.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=039 step=071 beta=0.430 caught=100 first_cap=0 bc=1.0889 cluster=0.00 idle=0.00 flip=0.39 same=0.10 score_us-bot=58.8-53.0 result=WIN\n",
      "[DAgger-PvP] ep=040 step=064 beta=0.427 caught=100 first_cap=0 bc=1.1219 cluster=0.00 idle=0.00 flip=0.48 same=0.06 score_us-bot=53.2-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=041 step=069 beta=0.423 caught=100 first_cap=1 bc=0.9678 cluster=0.03 idle=0.00 flip=0.29 same=0.07 score_us-bot=51.8-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=042 step=081 beta=0.420 caught=100 first_cap=0 bc=0.9807 cluster=0.00 idle=0.00 flip=0.35 same=0.10 score_us-bot=77.0-44.8 result=WIN\n",
      "[DAgger-PvP] ep=043 step=064 beta=0.416 caught=100 first_cap=0 bc=0.9674 cluster=0.00 idle=0.00 flip=0.40 same=0.18 score_us-bot=51.8-61.8 result=LOSS\n",
      "[DAgger-PvP] ep=044 step=087 beta=0.413 caught=100 first_cap=1 bc=0.9913 cluster=0.00 idle=0.00 flip=0.25 same=0.10 score_us-bot=68.0-48.0 result=WIN\n",
      "[DAgger-PvP] ep=045 step=067 beta=0.409 caught=100 first_cap=0 bc=0.9391 cluster=0.00 idle=0.00 flip=0.32 same=0.01 score_us-bot=65.5-61.0 result=WIN\n",
      "[DAgger-PvP] ep=046 step=068 beta=0.406 caught=100 first_cap=0 bc=0.9236 cluster=0.06 idle=0.01 flip=0.41 same=0.41 score_us-bot=61.2-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=047 step=069 beta=0.403 caught=100 first_cap=1 bc=0.9228 cluster=0.20 idle=0.01 flip=0.27 same=0.43 score_us-bot=40.0-78.0 result=LOSS\n",
      "[DAgger-PvP] ep=048 step=062 beta=0.399 caught=080 first_cap=1 bc=0.9677 cluster=0.00 idle=0.00 flip=0.22 same=0.05 score_us-bot=42.0-44.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_049.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep049.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_049.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep049.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=049 step=054 beta=0.396 caught=080 first_cap=0 bc=0.9806 cluster=0.00 idle=0.00 flip=0.56 same=0.05 score_us-bot=45.5-48.8 result=LOSS\n",
      "[DAgger-PvP] ep=050 step=051 beta=0.393 caught=080 first_cap=0 bc=0.9762 cluster=0.00 idle=0.00 flip=0.31 same=0.02 score_us-bot=50.8-50.0 result=WIN\n",
      "[DAgger-PvP] ep=051 step=063 beta=0.389 caught=080 first_cap=0 bc=1.0351 cluster=0.00 idle=0.00 flip=0.28 same=0.08 score_us-bot=42.0-45.5 result=LOSS\n",
      "[DAgger-PvP] ep=052 step=066 beta=0.386 caught=080 first_cap=0 bc=1.0589 cluster=0.00 idle=0.00 flip=0.42 same=0.19 score_us-bot=38.5-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=053 step=069 beta=0.383 caught=080 first_cap=1 bc=1.0445 cluster=0.00 idle=0.00 flip=0.37 same=0.41 score_us-bot=43.0-43.8 result=LOSS\n",
      "[DAgger-PvP] ep=054 step=068 beta=0.380 caught=080 first_cap=0 bc=0.9918 cluster=0.00 idle=0.00 flip=0.30 same=0.00 score_us-bot=44.0-43.0 result=WIN\n",
      "[DAgger-PvP] ep=055 step=060 beta=0.377 caught=080 first_cap=1 bc=0.9478 cluster=0.05 idle=0.00 flip=0.25 same=0.11 score_us-bot=41.0-58.5 result=LOSS\n",
      "[DAgger-PvP] ep=056 step=074 beta=0.374 caught=080 first_cap=0 bc=0.9748 cluster=0.00 idle=0.00 flip=0.37 same=0.11 score_us-bot=51.0-50.2 result=WIN\n",
      "[DAgger-PvP] ep=057 step=056 beta=0.371 caught=080 first_cap=0 bc=0.9594 cluster=0.09 idle=0.00 flip=0.21 same=0.42 score_us-bot=52.5-43.0 result=WIN\n",
      "[DAgger-PvP] ep=058 step=073 beta=0.367 caught=080 first_cap=1 bc=0.8971 cluster=0.00 idle=0.00 flip=0.31 same=0.19 score_us-bot=44.0-46.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_059.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep059.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_059.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep059.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=059 step=072 beta=0.364 caught=080 first_cap=0 bc=0.9395 cluster=0.08 idle=0.00 flip=0.37 same=0.45 score_us-bot=59.2-42.5 result=WIN\n",
      "[DAgger-PvP] ep=060 step=047 beta=0.361 caught=080 first_cap=0 bc=0.9596 cluster=0.00 idle=0.00 flip=0.46 same=0.04 score_us-bot=35.0-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=061 step=050 beta=0.358 caught=080 first_cap=0 bc=1.0039 cluster=0.00 idle=0.02 flip=0.31 same=0.04 score_us-bot=37.2-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=062 step=068 beta=0.355 caught=080 first_cap=0 bc=1.0305 cluster=0.03 idle=0.00 flip=0.30 same=0.16 score_us-bot=47.5-49.0 result=LOSS\n",
      "[DAgger-PvP] ep=063 step=051 beta=0.352 caught=080 first_cap=0 bc=1.0171 cluster=0.00 idle=0.00 flip=0.29 same=0.02 score_us-bot=35.0-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=064 step=052 beta=0.350 caught=080 first_cap=0 bc=1.0274 cluster=0.00 idle=0.00 flip=0.36 same=0.04 score_us-bot=33.0-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=065 step=068 beta=0.347 caught=080 first_cap=0 bc=1.0000 cluster=0.00 idle=0.01 flip=0.36 same=0.00 score_us-bot=35.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=066 step=066 beta=0.344 caught=080 first_cap=0 bc=1.0444 cluster=0.03 idle=0.01 flip=0.39 same=0.16 score_us-bot=46.2-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=067 step=062 beta=0.341 caught=080 first_cap=0 bc=0.9562 cluster=0.00 idle=0.00 flip=0.38 same=0.03 score_us-bot=40.2-53.0 result=LOSS\n",
      "[DAgger-PvP] ep=068 step=064 beta=0.338 caught=080 first_cap=0 bc=0.9597 cluster=0.00 idle=0.02 flip=0.32 same=0.14 score_us-bot=39.0-52.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_069.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep069.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_069.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep069.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=069 step=061 beta=0.335 caught=080 first_cap=0 bc=0.9840 cluster=0.03 idle=0.00 flip=0.37 same=0.15 score_us-bot=39.2-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=070 step=080 beta=0.333 caught=080 first_cap=1 bc=0.8762 cluster=0.02 idle=0.00 flip=0.33 same=0.54 score_us-bot=49.0-56.0 result=LOSS\n",
      "[DAgger-PvP] ep=071 step=079 beta=0.330 caught=080 first_cap=1 bc=0.9799 cluster=0.00 idle=0.00 flip=0.31 same=0.15 score_us-bot=49.0-48.5 result=WIN\n",
      "[DAgger-PvP] ep=072 step=075 beta=0.327 caught=100 first_cap=1 bc=0.9775 cluster=0.00 idle=0.01 flip=0.42 same=0.01 score_us-bot=61.2-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=073 step=080 beta=0.324 caught=100 first_cap=1 bc=1.0432 cluster=0.00 idle=0.01 flip=0.35 same=0.16 score_us-bot=60.0-62.2 result=LOSS\n",
      "[DAgger-PvP] ep=074 step=076 beta=0.322 caught=100 first_cap=1 bc=0.9835 cluster=0.00 idle=0.00 flip=0.32 same=0.12 score_us-bot=66.0-57.0 result=WIN\n",
      "[DAgger-PvP] ep=075 step=079 beta=0.319 caught=100 first_cap=1 bc=1.0453 cluster=0.03 idle=0.00 flip=0.33 same=0.16 score_us-bot=62.0-69.0 result=LOSS\n",
      "[DAgger-PvP] ep=076 step=072 beta=0.316 caught=100 first_cap=1 bc=1.0812 cluster=0.08 idle=0.00 flip=0.21 same=0.33 score_us-bot=51.5-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=077 step=076 beta=0.314 caught=100 first_cap=0 bc=1.0620 cluster=0.03 idle=0.00 flip=0.27 same=0.13 score_us-bot=59.5-66.2 result=LOSS\n",
      "[DAgger-PvP] ep=078 step=073 beta=0.311 caught=100 first_cap=1 bc=1.0080 cluster=0.00 idle=0.00 flip=0.27 same=0.16 score_us-bot=57.0-49.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_079.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep079.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_079.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep079.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=079 step=054 beta=0.309 caught=100 first_cap=0 bc=0.9822 cluster=0.00 idle=0.00 flip=0.22 same=0.20 score_us-bot=50.8-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=080 step=066 beta=0.306 caught=100 first_cap=0 bc=1.0078 cluster=0.00 idle=0.00 flip=0.36 same=0.06 score_us-bot=52.0-70.0 result=LOSS\n",
      "[DAgger-PvP] ep=081 step=065 beta=0.304 caught=100 first_cap=0 bc=0.9428 cluster=0.02 idle=0.00 flip=0.47 same=0.08 score_us-bot=68.0-57.5 result=WIN\n",
      "[DAgger-PvP] ep=082 step=056 beta=0.301 caught=100 first_cap=0 bc=0.9641 cluster=0.05 idle=0.00 flip=0.35 same=0.04 score_us-bot=51.0-63.8 result=LOSS\n",
      "[DAgger-PvP] ep=083 step=087 beta=0.299 caught=100 first_cap=2 bc=0.9940 cluster=0.00 idle=0.00 flip=0.41 same=0.14 score_us-bot=48.8-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=084 step=074 beta=0.296 caught=100 first_cap=0 bc=1.0433 cluster=0.05 idle=0.01 flip=0.32 same=0.28 score_us-bot=48.0-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=085 step=068 beta=0.294 caught=100 first_cap=0 bc=1.1136 cluster=0.01 idle=0.00 flip=0.29 same=0.16 score_us-bot=48.8-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=086 step=068 beta=0.291 caught=100 first_cap=2 bc=1.0845 cluster=0.03 idle=0.01 flip=0.25 same=0.09 score_us-bot=51.0-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=087 step=065 beta=0.289 caught=100 first_cap=1 bc=0.9903 cluster=0.00 idle=0.02 flip=0.26 same=0.09 score_us-bot=63.0-61.0 result=WIN\n",
      "[DAgger-PvP] ep=088 step=076 beta=0.286 caught=100 first_cap=0 bc=0.9868 cluster=0.01 idle=0.01 flip=0.42 same=0.14 score_us-bot=60.0-67.5 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_089.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep089.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_089.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep089.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=089 step=075 beta=0.284 caught=100 first_cap=0 bc=1.0179 cluster=0.04 idle=0.00 flip=0.29 same=0.05 score_us-bot=55.5-70.2 result=LOSS\n",
      "[DAgger-PvP] ep=090 step=072 beta=0.282 caught=100 first_cap=0 bc=1.0131 cluster=0.01 idle=0.00 flip=0.40 same=0.07 score_us-bot=56.5-54.5 result=WIN\n",
      "[DAgger-PvP] ep=091 step=056 beta=0.279 caught=100 first_cap=0 bc=0.8771 cluster=0.00 idle=0.00 flip=0.47 same=0.00 score_us-bot=51.2-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=092 step=061 beta=0.277 caught=100 first_cap=2 bc=0.9791 cluster=0.05 idle=0.02 flip=0.45 same=0.19 score_us-bot=52.8-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=093 step=074 beta=0.275 caught=100 first_cap=1 bc=0.9858 cluster=0.04 idle=0.00 flip=0.37 same=0.16 score_us-bot=56.2-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=094 step=087 beta=0.272 caught=100 first_cap=1 bc=0.9759 cluster=0.00 idle=0.00 flip=0.38 same=0.31 score_us-bot=62.8-54.0 result=WIN\n",
      "[DAgger-PvP] ep=095 step=127 beta=0.270 caught=100 first_cap=0 bc=0.9174 cluster=0.04 idle=0.00 flip=0.30 same=0.29 score_us-bot=57.0-68.5 result=LOSS\n",
      "[DAgger-PvP] ep=096 step=088 beta=0.268 caught=080 first_cap=3 bc=1.1065 cluster=0.01 idle=0.00 flip=0.24 same=0.15 score_us-bot=39.2-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=097 step=067 beta=0.266 caught=080 first_cap=0 bc=0.9580 cluster=0.00 idle=0.01 flip=0.29 same=0.00 score_us-bot=38.5-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=098 step=091 beta=0.264 caught=080 first_cap=1 bc=0.9989 cluster=0.00 idle=0.00 flip=0.35 same=0.30 score_us-bot=49.0-34.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_099.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep099.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_099.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep099.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=099 step=059 beta=0.261 caught=080 first_cap=1 bc=1.0175 cluster=0.00 idle=0.02 flip=0.32 same=0.10 score_us-bot=50.0-45.0 result=WIN\n",
      "[DAgger-PvP] ep=100 step=072 beta=0.259 caught=080 first_cap=2 bc=0.9474 cluster=0.00 idle=0.00 flip=0.32 same=0.03 score_us-bot=50.0-53.8 result=LOSS\n",
      "[DAgger-PvP] ep=101 step=093 beta=0.257 caught=080 first_cap=0 bc=1.0188 cluster=0.00 idle=0.00 flip=0.35 same=0.11 score_us-bot=52.2-56.0 result=LOSS\n",
      "[DAgger-PvP] ep=102 step=093 beta=0.255 caught=080 first_cap=2 bc=1.0580 cluster=0.02 idle=0.00 flip=0.45 same=0.06 score_us-bot=42.0-41.0 result=WIN\n",
      "[DAgger-PvP] ep=103 step=090 beta=0.253 caught=080 first_cap=1 bc=0.9731 cluster=0.00 idle=0.00 flip=0.37 same=0.00 score_us-bot=62.0-52.8 result=WIN\n",
      "[DAgger-PvP] ep=104 step=082 beta=0.251 caught=080 first_cap=2 bc=0.9304 cluster=0.01 idle=0.00 flip=0.30 same=0.17 score_us-bot=51.0-50.0 result=WIN\n",
      "[DAgger-PvP] ep=105 step=088 beta=0.249 caught=080 first_cap=4 bc=0.8718 cluster=0.00 idle=0.00 flip=0.42 same=0.06 score_us-bot=43.8-43.5 result=WIN\n",
      "[DAgger-PvP] ep=106 step=104 beta=0.247 caught=080 first_cap=3 bc=0.9951 cluster=0.00 idle=0.01 flip=0.33 same=0.10 score_us-bot=50.0-39.5 result=WIN\n",
      "[DAgger-PvP] ep=107 step=102 beta=0.245 caught=080 first_cap=1 bc=0.9088 cluster=0.02 idle=0.01 flip=0.25 same=0.39 score_us-bot=46.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=108 step=082 beta=0.243 caught=080 first_cap=1 bc=1.0322 cluster=0.01 idle=0.01 flip=0.24 same=0.30 score_us-bot=32.0-54.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_109.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep109.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_109.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep109.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=109 step=064 beta=0.241 caught=080 first_cap=0 bc=1.0869 cluster=0.00 idle=0.00 flip=0.23 same=0.05 score_us-bot=28.0-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=110 step=086 beta=0.239 caught=080 first_cap=1 bc=1.0276 cluster=0.00 idle=0.01 flip=0.45 same=0.09 score_us-bot=41.0-58.0 result=LOSS\n",
      "[DAgger-PvP] ep=111 step=079 beta=0.237 caught=080 first_cap=1 bc=0.9502 cluster=0.01 idle=0.01 flip=0.26 same=0.10 score_us-bot=51.0-45.5 result=WIN\n",
      "[DAgger-PvP] ep=112 step=070 beta=0.235 caught=080 first_cap=0 bc=1.0259 cluster=0.00 idle=0.00 flip=0.35 same=0.00 score_us-bot=46.5-53.0 result=LOSS\n",
      "[DAgger-PvP] ep=113 step=098 beta=0.233 caught=080 first_cap=0 bc=1.0976 cluster=0.00 idle=0.00 flip=0.37 same=0.19 score_us-bot=51.2-53.2 result=LOSS\n",
      "[DAgger-PvP] ep=114 step=100 beta=0.231 caught=080 first_cap=0 bc=1.0298 cluster=0.03 idle=0.00 flip=0.25 same=0.22 score_us-bot=57.2-48.8 result=WIN\n",
      "[DAgger-PvP] ep=115 step=089 beta=0.229 caught=080 first_cap=4 bc=0.9218 cluster=0.01 idle=0.00 flip=0.31 same=0.16 score_us-bot=49.0-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=116 step=087 beta=0.227 caught=080 first_cap=3 bc=0.9410 cluster=0.01 idle=0.00 flip=0.38 same=0.07 score_us-bot=54.0-32.0 result=WIN\n",
      "[DAgger-PvP] ep=117 step=094 beta=0.225 caught=080 first_cap=0 bc=0.9552 cluster=0.00 idle=0.00 flip=0.39 same=0.12 score_us-bot=47.0-50.8 result=LOSS\n",
      "[DAgger-PvP] ep=118 step=092 beta=0.223 caught=080 first_cap=3 bc=0.9381 cluster=0.02 idle=0.00 flip=0.29 same=0.35 score_us-bot=43.0-43.8 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_119.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep119.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_119.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep119.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=119 step=103 beta=0.221 caught=080 first_cap=3 bc=0.9412 cluster=0.00 idle=0.00 flip=0.38 same=0.02 score_us-bot=41.2-52.8 result=LOSS\n",
      "[DAgger-PvP] ep=120 step=105 beta=0.219 caught=100 first_cap=0 bc=0.9560 cluster=0.00 idle=0.00 flip=0.26 same=0.11 score_us-bot=56.0-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=121 step=086 beta=0.218 caught=100 first_cap=1 bc=0.9485 cluster=0.00 idle=0.00 flip=0.25 same=0.08 score_us-bot=46.2-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=122 step=079 beta=0.216 caught=100 first_cap=0 bc=1.0728 cluster=0.00 idle=0.00 flip=0.38 same=0.10 score_us-bot=46.8-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=123 step=085 beta=0.214 caught=100 first_cap=0 bc=1.0084 cluster=0.00 idle=0.01 flip=0.34 same=0.07 score_us-bot=57.2-75.0 result=LOSS\n",
      "[DAgger-PvP] ep=124 step=079 beta=0.212 caught=100 first_cap=1 bc=1.0340 cluster=0.04 idle=0.00 flip=0.21 same=0.35 score_us-bot=59.8-57.0 result=WIN\n",
      "[DAgger-PvP] ep=125 step=115 beta=0.211 caught=100 first_cap=0 bc=1.1044 cluster=0.02 idle=0.01 flip=0.33 same=0.10 score_us-bot=71.5-68.0 result=WIN\n",
      "[DAgger-PvP] ep=126 step=073 beta=0.209 caught=100 first_cap=3 bc=0.9551 cluster=0.01 idle=0.00 flip=0.22 same=0.01 score_us-bot=57.0-72.2 result=LOSS\n",
      "[DAgger-PvP] ep=127 step=096 beta=0.207 caught=100 first_cap=0 bc=0.9043 cluster=0.00 idle=0.00 flip=0.33 same=0.08 score_us-bot=58.0-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=128 step=100 beta=0.205 caught=100 first_cap=1 bc=1.0287 cluster=0.00 idle=0.00 flip=0.37 same=0.04 score_us-bot=46.5-58.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_129.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep129.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_129.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep129.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=129 step=082 beta=0.204 caught=100 first_cap=0 bc=0.9085 cluster=0.00 idle=0.00 flip=0.36 same=0.17 score_us-bot=48.2-56.0 result=LOSS\n",
      "[DAgger-PvP] ep=130 step=125 beta=0.202 caught=100 first_cap=0 bc=0.9517 cluster=0.00 idle=0.01 flip=0.40 same=0.03 score_us-bot=57.0-49.2 result=WIN\n",
      "[DAgger-PvP] ep=131 step=130 beta=0.200 caught=100 first_cap=2 bc=0.8942 cluster=0.01 idle=0.00 flip=0.28 same=0.25 score_us-bot=59.0-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=132 step=079 beta=0.199 caught=100 first_cap=0 bc=0.9915 cluster=0.03 idle=0.01 flip=0.23 same=0.12 score_us-bot=50.2-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=133 step=088 beta=0.197 caught=100 first_cap=1 bc=0.9901 cluster=0.02 idle=0.01 flip=0.29 same=0.07 score_us-bot=56.2-58.5 result=LOSS\n",
      "[DAgger-PvP] ep=134 step=093 beta=0.195 caught=100 first_cap=1 bc=1.0134 cluster=0.00 idle=0.00 flip=0.32 same=0.05 score_us-bot=52.0-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=135 step=070 beta=0.194 caught=100 first_cap=0 bc=1.0980 cluster=0.00 idle=0.01 flip=0.28 same=0.18 score_us-bot=49.8-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=136 step=095 beta=0.192 caught=100 first_cap=2 bc=1.0033 cluster=0.00 idle=0.01 flip=0.31 same=0.12 score_us-bot=64.0-61.2 result=WIN\n",
      "[DAgger-PvP] ep=137 step=100 beta=0.191 caught=100 first_cap=0 bc=1.0228 cluster=0.00 idle=0.00 flip=0.32 same=0.02 score_us-bot=48.5-75.0 result=LOSS\n",
      "[DAgger-PvP] ep=138 step=081 beta=0.189 caught=100 first_cap=0 bc=0.9420 cluster=0.01 idle=0.00 flip=0.28 same=0.21 score_us-bot=57.5-55.8 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_139.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep139.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_139.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep139.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=139 step=100 beta=0.187 caught=100 first_cap=2 bc=0.9458 cluster=0.00 idle=0.00 flip=0.41 same=0.04 score_us-bot=67.0-63.0 result=WIN\n",
      "[DAgger-PvP] ep=140 step=088 beta=0.186 caught=100 first_cap=1 bc=0.9127 cluster=0.02 idle=0.00 flip=0.33 same=0.15 score_us-bot=54.0-54.0 result=DRAW\n",
      "[DAgger-PvP] ep=141 step=122 beta=0.184 caught=100 first_cap=1 bc=0.9304 cluster=0.00 idle=0.00 flip=0.31 same=0.09 score_us-bot=64.8-65.8 result=LOSS\n",
      "[DAgger-PvP] ep=142 step=092 beta=0.183 caught=100 first_cap=2 bc=0.9068 cluster=0.02 idle=0.00 flip=0.43 same=0.26 score_us-bot=60.0-53.0 result=WIN\n",
      "[DAgger-PvP] ep=143 step=145 beta=0.181 caught=100 first_cap=0 bc=0.9075 cluster=0.01 idle=0.00 flip=0.35 same=0.14 score_us-bot=65.5-58.2 result=WIN\n",
      "[DAgger-PvP] ep=144 step=078 beta=0.180 caught=080 first_cap=1 bc=0.9664 cluster=0.03 idle=0.01 flip=0.30 same=0.11 score_us-bot=34.0-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=145 step=073 beta=0.178 caught=080 first_cap=0 bc=1.0409 cluster=0.00 idle=0.01 flip=0.27 same=0.04 score_us-bot=32.0-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=146 step=070 beta=0.177 caught=080 first_cap=4 bc=1.0263 cluster=0.00 idle=0.00 flip=0.30 same=0.13 score_us-bot=31.0-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=147 step=088 beta=0.175 caught=080 first_cap=2 bc=1.0322 cluster=0.00 idle=0.00 flip=0.34 same=0.11 score_us-bot=34.0-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=148 step=071 beta=0.174 caught=080 first_cap=3 bc=1.0604 cluster=0.00 idle=0.00 flip=0.25 same=0.06 score_us-bot=47.2-59.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_149.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep149.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_149.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep149.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=149 step=095 beta=0.172 caught=080 first_cap=0 bc=1.0697 cluster=0.00 idle=0.00 flip=0.41 same=0.07 score_us-bot=41.8-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=150 step=086 beta=0.171 caught=080 first_cap=0 bc=0.9995 cluster=0.00 idle=0.00 flip=0.32 same=0.17 score_us-bot=44.2-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=151 step=076 beta=0.170 caught=080 first_cap=0 bc=0.9382 cluster=0.00 idle=0.00 flip=0.32 same=0.31 score_us-bot=47.8-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=152 step=093 beta=0.168 caught=080 first_cap=3 bc=0.9664 cluster=0.00 idle=0.01 flip=0.31 same=0.09 score_us-bot=35.8-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=153 step=084 beta=0.167 caught=080 first_cap=0 bc=0.8921 cluster=0.00 idle=0.01 flip=0.47 same=0.12 score_us-bot=32.0-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=154 step=100 beta=0.165 caught=080 first_cap=1 bc=0.9565 cluster=0.00 idle=0.00 flip=0.40 same=0.04 score_us-bot=44.0-44.0 result=DRAW\n",
      "[DAgger-PvP] ep=155 step=115 beta=0.164 caught=080 first_cap=0 bc=0.9952 cluster=0.03 idle=0.01 flip=0.49 same=0.35 score_us-bot=34.5-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=156 step=082 beta=0.163 caught=080 first_cap=1 bc=1.0445 cluster=0.00 idle=0.01 flip=0.33 same=0.00 score_us-bot=37.0-49.0 result=LOSS\n",
      "[DAgger-PvP] ep=157 step=083 beta=0.161 caught=080 first_cap=1 bc=0.9846 cluster=0.00 idle=0.01 flip=0.24 same=0.10 score_us-bot=36.0-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=158 step=082 beta=0.160 caught=080 first_cap=4 bc=0.9643 cluster=0.00 idle=0.01 flip=0.34 same=0.14 score_us-bot=46.5-43.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_159.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep159.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_159.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep159.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=159 step=089 beta=0.159 caught=080 first_cap=0 bc=1.0326 cluster=0.00 idle=0.01 flip=0.28 same=0.02 score_us-bot=50.0-33.0 result=WIN\n",
      "[DAgger-PvP] ep=160 step=100 beta=0.157 caught=080 first_cap=4 bc=0.9776 cluster=0.00 idle=0.01 flip=0.35 same=0.04 score_us-bot=37.2-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=161 step=112 beta=0.156 caught=080 first_cap=0 bc=1.0424 cluster=0.00 idle=0.00 flip=0.30 same=0.28 score_us-bot=46.0-42.5 result=WIN\n",
      "[DAgger-PvP] ep=162 step=112 beta=0.155 caught=080 first_cap=0 bc=0.9964 cluster=0.01 idle=0.00 flip=0.27 same=0.23 score_us-bot=54.8-57.2 result=LOSS\n",
      "[DAgger-PvP] ep=163 step=096 beta=0.154 caught=080 first_cap=3 bc=0.9586 cluster=0.01 idle=0.00 flip=0.38 same=0.13 score_us-bot=35.0-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=164 step=104 beta=0.152 caught=080 first_cap=1 bc=0.8936 cluster=0.02 idle=0.00 flip=0.31 same=0.37 score_us-bot=40.0-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=165 step=100 beta=0.151 caught=080 first_cap=2 bc=0.9899 cluster=0.00 idle=0.00 flip=0.37 same=0.02 score_us-bot=37.0-52.8 result=LOSS\n",
      "[DAgger-PvP] ep=166 step=130 beta=0.150 caught=080 first_cap=0 bc=0.9597 cluster=0.02 idle=0.00 flip=0.36 same=0.10 score_us-bot=41.8-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=167 step=098 beta=0.149 caught=080 first_cap=0 bc=0.8644 cluster=0.00 idle=0.01 flip=0.36 same=0.11 score_us-bot=45.8-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=168 step=081 beta=0.147 caught=100 first_cap=0 bc=0.9771 cluster=0.02 idle=0.01 flip=0.20 same=0.05 score_us-bot=43.0-60.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_169.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep169.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_169.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep169.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=169 step=072 beta=0.146 caught=100 first_cap=1 bc=0.9844 cluster=0.00 idle=0.00 flip=0.21 same=0.03 score_us-bot=46.0-60.5 result=LOSS\n",
      "[DAgger-PvP] ep=170 step=097 beta=0.145 caught=100 first_cap=2 bc=1.0018 cluster=0.00 idle=0.01 flip=0.30 same=0.18 score_us-bot=61.5-57.0 result=WIN\n",
      "[DAgger-PvP] ep=171 step=085 beta=0.144 caught=100 first_cap=1 bc=1.0004 cluster=0.00 idle=0.01 flip=0.36 same=0.00 score_us-bot=46.0-60.5 result=LOSS\n",
      "[DAgger-PvP] ep=172 step=106 beta=0.142 caught=100 first_cap=1 bc=1.0637 cluster=0.00 idle=0.01 flip=0.36 same=0.09 score_us-bot=50.5-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=173 step=071 beta=0.141 caught=100 first_cap=1 bc=1.0497 cluster=0.00 idle=0.01 flip=0.28 same=0.06 score_us-bot=47.0-53.0 result=LOSS\n",
      "[DAgger-PvP] ep=174 step=104 beta=0.140 caught=100 first_cap=0 bc=0.9921 cluster=0.00 idle=0.01 flip=0.45 same=0.02 score_us-bot=45.8-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=175 step=103 beta=0.139 caught=100 first_cap=0 bc=0.9963 cluster=0.01 idle=0.00 flip=0.34 same=0.01 score_us-bot=56.0-50.5 result=WIN\n",
      "[DAgger-PvP] ep=176 step=091 beta=0.138 caught=100 first_cap=0 bc=0.8923 cluster=0.00 idle=0.00 flip=0.34 same=0.25 score_us-bot=46.0-77.0 result=LOSS\n",
      "[DAgger-PvP] ep=177 step=122 beta=0.137 caught=100 first_cap=1 bc=0.9800 cluster=0.00 idle=0.01 flip=0.35 same=0.19 score_us-bot=57.2-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=178 step=096 beta=0.136 caught=100 first_cap=1 bc=0.8830 cluster=0.00 idle=0.01 flip=0.20 same=0.16 score_us-bot=55.5-55.8 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_179.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep179.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_179.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep179.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=179 step=093 beta=0.134 caught=100 first_cap=1 bc=0.8641 cluster=0.00 idle=0.00 flip=0.45 same=0.10 score_us-bot=41.0-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=180 step=091 beta=0.133 caught=100 first_cap=1 bc=0.9622 cluster=0.00 idle=0.00 flip=0.23 same=0.16 score_us-bot=51.2-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=181 step=098 beta=0.132 caught=100 first_cap=0 bc=1.0237 cluster=0.01 idle=0.01 flip=0.26 same=0.02 score_us-bot=61.0-75.0 result=LOSS\n",
      "[DAgger-PvP] ep=182 step=079 beta=0.131 caught=100 first_cap=0 bc=1.0952 cluster=0.00 idle=0.00 flip=0.20 same=0.05 score_us-bot=40.0-69.0 result=LOSS\n",
      "[DAgger-PvP] ep=183 step=097 beta=0.130 caught=100 first_cap=1 bc=1.0664 cluster=0.02 idle=0.01 flip=0.32 same=0.11 score_us-bot=61.8-61.2 result=WIN\n",
      "[DAgger-PvP] ep=184 step=080 beta=0.129 caught=100 first_cap=0 bc=0.9683 cluster=0.00 idle=0.00 flip=0.41 same=0.09 score_us-bot=60.8-55.0 result=WIN\n",
      "[DAgger-PvP] ep=185 step=100 beta=0.128 caught=100 first_cap=1 bc=1.0061 cluster=0.00 idle=0.00 flip=0.50 same=0.01 score_us-bot=52.8-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=186 step=124 beta=0.127 caught=100 first_cap=0 bc=0.9921 cluster=0.00 idle=0.00 flip=0.31 same=0.16 score_us-bot=60.0-64.0 result=LOSS\n",
      "[DAgger-PvP] ep=187 step=115 beta=0.126 caught=100 first_cap=0 bc=0.9394 cluster=0.00 idle=0.00 flip=0.43 same=0.09 score_us-bot=49.8-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=188 step=111 beta=0.125 caught=100 first_cap=1 bc=0.9870 cluster=0.00 idle=0.01 flip=0.41 same=0.02 score_us-bot=42.0-64.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_189.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep189.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_189.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep189.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=189 step=105 beta=0.124 caught=100 first_cap=0 bc=0.9176 cluster=0.00 idle=0.00 flip=0.46 same=0.04 score_us-bot=56.5-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=190 step=108 beta=0.123 caught=100 first_cap=0 bc=1.0328 cluster=0.06 idle=0.01 flip=0.36 same=0.34 score_us-bot=48.8-76.0 result=LOSS\n",
      "[DAgger-PvP] ep=191 step=107 beta=0.122 caught=100 first_cap=0 bc=1.0483 cluster=0.00 idle=0.00 flip=0.42 same=0.00 score_us-bot=39.0-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=192 step=112 beta=0.121 caught=080 first_cap=2 bc=0.9699 cluster=0.00 idle=0.01 flip=0.27 same=0.08 score_us-bot=39.0-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=193 step=099 beta=0.120 caught=080 first_cap=4 bc=1.0350 cluster=0.00 idle=0.01 flip=0.29 same=0.06 score_us-bot=43.8-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=194 step=111 beta=0.119 caught=080 first_cap=0 bc=1.0214 cluster=0.01 idle=0.01 flip=0.27 same=0.03 score_us-bot=48.8-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=195 step=101 beta=0.118 caught=080 first_cap=2 bc=1.0243 cluster=0.00 idle=0.00 flip=0.36 same=0.02 score_us-bot=40.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=196 step=110 beta=0.117 caught=080 first_cap=2 bc=1.1121 cluster=0.00 idle=0.01 flip=0.27 same=0.12 score_us-bot=37.0-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=197 step=157 beta=0.116 caught=080 first_cap=3 bc=1.0314 cluster=0.00 idle=0.00 flip=0.39 same=0.04 score_us-bot=63.0-50.8 result=WIN\n",
      "[DAgger-PvP] ep=198 step=102 beta=0.115 caught=080 first_cap=4 bc=1.0044 cluster=0.00 idle=0.01 flip=0.40 same=0.06 score_us-bot=37.0-43.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_199.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep199.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_199.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep199.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=199 step=125 beta=0.114 caught=080 first_cap=2 bc=0.9834 cluster=0.00 idle=0.00 flip=0.33 same=0.24 score_us-bot=50.2-49.5 result=WIN\n",
      "[DAgger-PvP] ep=200 step=133 beta=0.113 caught=080 first_cap=0 bc=0.9782 cluster=0.01 idle=0.00 flip=0.24 same=0.10 score_us-bot=43.2-44.0 result=LOSS\n",
      "[DAgger-PvP] ep=201 step=150 beta=0.112 caught=080 first_cap=0 bc=0.8867 cluster=0.01 idle=0.01 flip=0.43 same=0.17 score_us-bot=50.0-45.2 result=WIN\n",
      "[DAgger-PvP] ep=202 step=154 beta=0.111 caught=080 first_cap=2 bc=0.9809 cluster=0.00 idle=0.01 flip=0.45 same=0.07 score_us-bot=43.0-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=203 step=110 beta=0.110 caught=080 first_cap=0 bc=0.9636 cluster=0.00 idle=0.00 flip=0.36 same=0.26 score_us-bot=36.8-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=204 step=086 beta=0.109 caught=080 first_cap=0 bc=0.9846 cluster=0.00 idle=0.01 flip=0.29 same=0.07 score_us-bot=42.8-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=205 step=105 beta=0.108 caught=080 first_cap=3 bc=0.9872 cluster=0.00 idle=0.01 flip=0.27 same=0.06 score_us-bot=45.0-41.2 result=WIN\n",
      "[DAgger-PvP] ep=206 step=089 beta=0.107 caught=080 first_cap=2 bc=1.0128 cluster=0.00 idle=0.01 flip=0.20 same=0.01 score_us-bot=41.5-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=207 step=093 beta=0.107 caught=080 first_cap=2 bc=1.0234 cluster=0.00 idle=0.01 flip=0.27 same=0.12 score_us-bot=50.0-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=208 step=108 beta=0.106 caught=080 first_cap=3 bc=1.0126 cluster=0.00 idle=0.01 flip=0.39 same=0.09 score_us-bot=33.0-56.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_209.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep209.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_209.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep209.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=209 step=116 beta=0.105 caught=080 first_cap=0 bc=1.0518 cluster=0.00 idle=0.00 flip=0.28 same=0.01 score_us-bot=45.8-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=210 step=120 beta=0.104 caught=080 first_cap=2 bc=1.0557 cluster=0.00 idle=0.00 flip=0.50 same=0.04 score_us-bot=51.2-52.8 result=LOSS\n",
      "[DAgger-PvP] ep=211 step=114 beta=0.103 caught=080 first_cap=1 bc=0.9598 cluster=0.00 idle=0.00 flip=0.37 same=0.02 score_us-bot=24.0-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=212 step=107 beta=0.102 caught=080 first_cap=0 bc=0.9260 cluster=0.04 idle=0.00 flip=0.30 same=0.21 score_us-bot=33.0-53.0 result=LOSS\n",
      "[DAgger-PvP] ep=213 step=146 beta=0.101 caught=080 first_cap=0 bc=0.8947 cluster=0.10 idle=0.00 flip=0.36 same=0.10 score_us-bot=39.2-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=214 step=117 beta=0.100 caught=080 first_cap=0 bc=0.9295 cluster=0.01 idle=0.00 flip=0.36 same=0.09 score_us-bot=36.0-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=215 step=177 beta=0.100 caught=080 first_cap=1 bc=0.9584 cluster=0.06 idle=0.00 flip=0.29 same=0.32 score_us-bot=49.0-74.0 result=LOSS\n",
      "[DAgger-PvP] ep=216 step=116 beta=0.099 caught=100 first_cap=0 bc=1.0088 cluster=0.00 idle=0.01 flip=0.29 same=0.20 score_us-bot=67.5-61.0 result=WIN\n",
      "[DAgger-PvP] ep=217 step=123 beta=0.098 caught=100 first_cap=2 bc=0.9923 cluster=0.00 idle=0.00 flip=0.32 same=0.04 score_us-bot=59.8-55.0 result=WIN\n",
      "[DAgger-PvP] ep=218 step=115 beta=0.097 caught=100 first_cap=1 bc=0.9685 cluster=0.00 idle=0.01 flip=0.34 same=0.00 score_us-bot=61.0-63.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_219.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep219.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_219.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep219.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=219 step=112 beta=0.096 caught=100 first_cap=1 bc=1.0087 cluster=0.00 idle=0.01 flip=0.31 same=0.00 score_us-bot=59.2-58.0 result=WIN\n",
      "[DAgger-PvP] ep=220 step=126 beta=0.096 caught=100 first_cap=2 bc=1.0222 cluster=0.01 idle=0.01 flip=0.46 same=0.09 score_us-bot=74.0-48.0 result=WIN\n",
      "[DAgger-PvP] ep=221 step=154 beta=0.095 caught=100 first_cap=0 bc=1.0273 cluster=0.00 idle=0.01 flip=0.48 same=0.03 score_us-bot=62.0-52.0 result=WIN\n",
      "[DAgger-PvP] ep=222 step=131 beta=0.094 caught=100 first_cap=3 bc=0.9886 cluster=0.00 idle=0.00 flip=0.33 same=0.04 score_us-bot=59.2-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=223 step=108 beta=0.093 caught=100 first_cap=2 bc=1.0079 cluster=0.00 idle=0.00 flip=0.38 same=0.12 score_us-bot=39.0-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=224 step=160 beta=0.092 caught=100 first_cap=1 bc=0.9477 cluster=0.01 idle=0.00 flip=0.37 same=0.37 score_us-bot=54.0-54.5 result=LOSS\n",
      "[DAgger-PvP] ep=225 step=107 beta=0.092 caught=100 first_cap=2 bc=0.9553 cluster=0.00 idle=0.00 flip=0.39 same=0.11 score_us-bot=42.0-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=226 step=207 beta=0.091 caught=100 first_cap=1 bc=0.9469 cluster=0.00 idle=0.00 flip=0.43 same=0.05 score_us-bot=63.2-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=227 step=140 beta=0.090 caught=100 first_cap=0 bc=1.0058 cluster=0.05 idle=0.01 flip=0.30 same=0.35 score_us-bot=49.5-71.0 result=LOSS\n",
      "[DAgger-PvP] ep=228 step=098 beta=0.089 caught=100 first_cap=3 bc=1.0113 cluster=0.00 idle=0.01 flip=0.21 same=0.02 score_us-bot=58.0-71.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_229.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep229.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_229.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep229.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=229 step=108 beta=0.089 caught=100 first_cap=3 bc=0.9720 cluster=0.00 idle=0.01 flip=0.35 same=0.13 score_us-bot=50.5-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=230 step=109 beta=0.088 caught=100 first_cap=2 bc=1.0678 cluster=0.00 idle=0.01 flip=0.28 same=0.00 score_us-bot=41.0-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=231 step=089 beta=0.087 caught=100 first_cap=0 bc=0.9369 cluster=0.00 idle=0.01 flip=0.26 same=0.06 score_us-bot=41.0-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=232 step=127 beta=0.087 caught=100 first_cap=1 bc=1.0562 cluster=0.00 idle=0.01 flip=0.30 same=0.18 score_us-bot=68.0-69.0 result=LOSS\n",
      "[DAgger-PvP] ep=233 step=148 beta=0.086 caught=100 first_cap=1 bc=1.0725 cluster=0.03 idle=0.00 flip=0.36 same=0.17 score_us-bot=66.2-63.0 result=WIN\n",
      "[DAgger-PvP] ep=234 step=157 beta=0.085 caught=100 first_cap=1 bc=1.0371 cluster=0.00 idle=0.01 flip=0.53 same=0.00 score_us-bot=37.0-69.0 result=LOSS\n",
      "[DAgger-PvP] ep=235 step=145 beta=0.084 caught=100 first_cap=0 bc=0.9916 cluster=0.06 idle=0.01 flip=0.43 same=0.21 score_us-bot=51.0-59.2 result=LOSS\n",
      "[DAgger-PvP] ep=236 step=098 beta=0.084 caught=100 first_cap=2 bc=0.8892 cluster=0.02 idle=0.00 flip=0.43 same=0.11 score_us-bot=41.5-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=237 step=189 beta=0.083 caught=100 first_cap=6 bc=0.9909 cluster=0.02 idle=0.01 flip=0.33 same=0.17 score_us-bot=53.0-62.8 result=LOSS\n",
      "[DAgger-PvP] ep=238 step=133 beta=0.082 caught=100 first_cap=1 bc=0.8907 cluster=0.06 idle=0.02 flip=0.37 same=0.28 score_us-bot=54.0-64.2 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_239.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep239.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_239.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep239.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=239 step=156 beta=0.082 caught=100 first_cap=0 bc=0.9315 cluster=0.01 idle=0.00 flip=0.52 same=0.15 score_us-bot=43.5-75.5 result=LOSS\n",
      "[DAgger-PvP] ep=240 step=085 beta=0.081 caught=080 first_cap=0 bc=0.9888 cluster=0.03 idle=0.00 flip=0.28 same=0.26 score_us-bot=50.5-48.2 result=WIN\n",
      "[DAgger-PvP] ep=241 step=071 beta=0.080 caught=080 first_cap=0 bc=1.0983 cluster=0.03 idle=0.01 flip=0.29 same=0.33 score_us-bot=43.0-46.5 result=LOSS\n",
      "[DAgger-PvP] ep=242 step=056 beta=0.080 caught=080 first_cap=1 bc=1.0294 cluster=0.00 idle=0.00 flip=0.21 same=0.00 score_us-bot=52.2-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=243 step=068 beta=0.079 caught=080 first_cap=1 bc=1.1097 cluster=0.04 idle=0.01 flip=0.38 same=0.28 score_us-bot=30.0-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=244 step=074 beta=0.078 caught=080 first_cap=0 bc=1.0361 cluster=0.04 idle=0.01 flip=0.36 same=0.04 score_us-bot=37.5-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=245 step=073 beta=0.078 caught=080 first_cap=3 bc=1.0616 cluster=0.03 idle=0.01 flip=0.43 same=0.15 score_us-bot=50.2-50.8 result=LOSS\n",
      "[DAgger-PvP] ep=246 step=060 beta=0.077 caught=080 first_cap=1 bc=1.0753 cluster=0.03 idle=0.00 flip=0.39 same=0.05 score_us-bot=37.0-43.0 result=LOSS\n",
      "[DAgger-PvP] ep=247 step=064 beta=0.076 caught=080 first_cap=0 bc=0.9777 cluster=0.03 idle=0.02 flip=0.25 same=0.31 score_us-bot=46.2-56.8 result=LOSS\n",
      "[DAgger-PvP] ep=248 step=075 beta=0.076 caught=080 first_cap=0 bc=0.9685 cluster=0.00 idle=0.01 flip=0.39 same=0.14 score_us-bot=34.0-49.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_249.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep249.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_249.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep249.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=249 step=075 beta=0.075 caught=080 first_cap=2 bc=0.9709 cluster=0.03 idle=0.01 flip=0.34 same=0.03 score_us-bot=47.2-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=250 step=071 beta=0.075 caught=080 first_cap=0 bc=0.9729 cluster=0.04 idle=0.01 flip=0.43 same=0.24 score_us-bot=36.8-56.0 result=LOSS\n",
      "[DAgger-PvP] ep=251 step=074 beta=0.074 caught=080 first_cap=1 bc=0.9699 cluster=0.03 idle=0.01 flip=0.28 same=0.44 score_us-bot=40.0-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=252 step=072 beta=0.073 caught=080 first_cap=0 bc=1.0092 cluster=0.00 idle=0.01 flip=0.34 same=0.27 score_us-bot=46.5-44.0 result=WIN\n",
      "[DAgger-PvP] ep=253 step=071 beta=0.073 caught=080 first_cap=0 bc=1.0639 cluster=0.00 idle=0.01 flip=0.43 same=0.07 score_us-bot=45.8-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=254 step=066 beta=0.072 caught=080 first_cap=0 bc=1.0096 cluster=0.00 idle=0.00 flip=0.22 same=0.00 score_us-bot=33.0-53.0 result=LOSS\n",
      "[DAgger-PvP] ep=255 step=071 beta=0.071 caught=080 first_cap=0 bc=1.0301 cluster=0.00 idle=0.01 flip=0.42 same=0.08 score_us-bot=37.5-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=256 step=070 beta=0.071 caught=080 first_cap=0 bc=1.0547 cluster=0.00 idle=0.01 flip=0.30 same=0.21 score_us-bot=48.5-44.5 result=WIN\n",
      "[DAgger-PvP] ep=257 step=095 beta=0.070 caught=080 first_cap=3 bc=1.0785 cluster=0.02 idle=0.01 flip=0.42 same=0.20 score_us-bot=43.0-45.5 result=LOSS\n",
      "[DAgger-PvP] ep=258 step=066 beta=0.070 caught=080 first_cap=0 bc=1.0628 cluster=0.03 idle=0.01 flip=0.36 same=0.13 score_us-bot=46.2-46.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_259.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep259.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_259.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep259.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=259 step=068 beta=0.069 caught=080 first_cap=0 bc=0.9810 cluster=0.00 idle=0.00 flip=0.38 same=0.09 score_us-bot=42.5-49.0 result=LOSS\n",
      "[DAgger-PvP] ep=260 step=062 beta=0.069 caught=080 first_cap=1 bc=0.9710 cluster=0.10 idle=0.02 flip=0.27 same=0.25 score_us-bot=49.8-43.0 result=WIN\n",
      "[DAgger-PvP] ep=261 step=066 beta=0.068 caught=080 first_cap=1 bc=1.0130 cluster=0.00 idle=0.01 flip=0.49 same=0.04 score_us-bot=28.0-58.0 result=LOSS\n",
      "[DAgger-PvP] ep=262 step=074 beta=0.067 caught=080 first_cap=0 bc=0.8929 cluster=0.03 idle=0.01 flip=0.40 same=0.05 score_us-bot=35.0-61.2 result=LOSS\n",
      "[DAgger-PvP] ep=263 step=090 beta=0.067 caught=080 first_cap=1 bc=1.0353 cluster=0.03 idle=0.01 flip=0.49 same=0.13 score_us-bot=38.0-42.0 result=LOSS\n",
      "[DAgger-PvP] ep=264 step=081 beta=0.066 caught=100 first_cap=1 bc=1.1196 cluster=0.00 idle=0.01 flip=0.18 same=0.01 score_us-bot=56.5-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=265 step=068 beta=0.066 caught=100 first_cap=1 bc=0.9589 cluster=0.00 idle=0.01 flip=0.42 same=0.06 score_us-bot=57.5-55.0 result=WIN\n",
      "[DAgger-PvP] ep=266 step=074 beta=0.065 caught=100 first_cap=0 bc=1.0232 cluster=0.04 idle=0.00 flip=0.25 same=0.11 score_us-bot=47.8-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=267 step=069 beta=0.065 caught=100 first_cap=0 bc=1.0343 cluster=0.03 idle=0.01 flip=0.29 same=0.01 score_us-bot=57.8-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=268 step=096 beta=0.064 caught=100 first_cap=1 bc=1.1137 cluster=0.00 idle=0.00 flip=0.38 same=0.02 score_us-bot=60.0-72.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_269.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep269.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_269.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep269.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=269 step=079 beta=0.064 caught=100 first_cap=2 bc=0.9887 cluster=0.03 idle=0.01 flip=0.34 same=0.14 score_us-bot=51.5-59.8 result=LOSS\n",
      "[DAgger-PvP] ep=270 step=062 beta=0.063 caught=100 first_cap=1 bc=1.0608 cluster=0.00 idle=0.02 flip=0.38 same=0.03 score_us-bot=55.8-54.0 result=WIN\n",
      "[DAgger-PvP] ep=271 step=074 beta=0.063 caught=100 first_cap=0 bc=0.9830 cluster=0.03 idle=0.01 flip=0.19 same=0.05 score_us-bot=56.8-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=272 step=091 beta=0.062 caught=100 first_cap=0 bc=1.0031 cluster=0.08 idle=0.00 flip=0.39 same=0.25 score_us-bot=61.8-59.0 result=WIN\n",
      "[DAgger-PvP] ep=273 step=083 beta=0.062 caught=100 first_cap=0 bc=0.9643 cluster=0.01 idle=0.01 flip=0.42 same=0.23 score_us-bot=63.5-71.0 result=LOSS\n",
      "[DAgger-PvP] ep=274 step=095 beta=0.061 caught=100 first_cap=1 bc=0.9114 cluster=0.03 idle=0.01 flip=0.41 same=0.26 score_us-bot=60.2-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=275 step=093 beta=0.061 caught=100 first_cap=1 bc=0.8278 cluster=0.10 idle=0.00 flip=0.28 same=0.29 score_us-bot=81.0-67.5 result=WIN\n",
      "[DAgger-PvP] ep=276 step=062 beta=0.060 caught=100 first_cap=0 bc=1.0532 cluster=0.02 idle=0.00 flip=0.30 same=0.30 score_us-bot=55.0-69.2 result=LOSS\n",
      "[DAgger-PvP] ep=277 step=064 beta=0.060 caught=100 first_cap=1 bc=1.0456 cluster=0.06 idle=0.02 flip=0.20 same=0.17 score_us-bot=53.2-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=278 step=075 beta=0.059 caught=100 first_cap=0 bc=1.0203 cluster=0.04 idle=0.01 flip=0.32 same=0.34 score_us-bot=69.0-52.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_279.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep279.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_279.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep279.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=279 step=079 beta=0.059 caught=100 first_cap=1 bc=1.0904 cluster=0.00 idle=0.01 flip=0.36 same=0.04 score_us-bot=58.0-60.2 result=LOSS\n",
      "[DAgger-PvP] ep=280 step=074 beta=0.058 caught=100 first_cap=0 bc=1.0294 cluster=0.00 idle=0.00 flip=0.31 same=0.16 score_us-bot=56.5-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=281 step=063 beta=0.058 caught=100 first_cap=0 bc=0.9214 cluster=0.05 idle=0.00 flip=0.36 same=0.12 score_us-bot=60.0-59.5 result=WIN\n",
      "[DAgger-PvP] ep=282 step=090 beta=0.057 caught=100 first_cap=0 bc=1.0509 cluster=0.01 idle=0.01 flip=0.48 same=0.12 score_us-bot=69.0-59.0 result=WIN\n",
      "[DAgger-PvP] ep=283 step=072 beta=0.057 caught=100 first_cap=1 bc=0.9911 cluster=0.00 idle=0.00 flip=0.48 same=0.10 score_us-bot=52.0-64.2 result=LOSS\n",
      "[DAgger-PvP] ep=284 step=080 beta=0.056 caught=100 first_cap=0 bc=0.9108 cluster=0.00 idle=0.01 flip=0.37 same=0.09 score_us-bot=52.5-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=285 step=067 beta=0.056 caught=100 first_cap=0 bc=1.0185 cluster=0.04 idle=0.01 flip=0.32 same=0.19 score_us-bot=41.0-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=286 step=074 beta=0.055 caught=100 first_cap=2 bc=1.0009 cluster=0.00 idle=0.00 flip=0.27 same=0.35 score_us-bot=56.0-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=287 step=063 beta=0.055 caught=100 first_cap=0 bc=0.9176 cluster=0.00 idle=0.00 flip=0.42 same=0.12 score_us-bot=55.0-60.5 result=LOSS\n",
      "[DAgger-PvP] ep=288 step=070 beta=0.054 caught=080 first_cap=0 bc=1.0082 cluster=0.01 idle=0.01 flip=0.21 same=0.08 score_us-bot=34.8-55.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_289.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep289.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_289.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep289.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=289 step=070 beta=0.054 caught=080 first_cap=0 bc=1.0816 cluster=0.00 idle=0.01 flip=0.34 same=0.03 score_us-bot=50.5-50.0 result=WIN\n",
      "[DAgger-PvP] ep=290 step=078 beta=0.053 caught=080 first_cap=0 bc=1.0558 cluster=0.01 idle=0.01 flip=0.24 same=0.15 score_us-bot=50.2-49.0 result=WIN\n",
      "[DAgger-PvP] ep=291 step=073 beta=0.053 caught=080 first_cap=1 bc=1.0094 cluster=0.03 idle=0.01 flip=0.28 same=0.26 score_us-bot=52.5-48.0 result=WIN\n",
      "[DAgger-PvP] ep=292 step=078 beta=0.053 caught=080 first_cap=0 bc=1.0507 cluster=0.01 idle=0.01 flip=0.27 same=0.29 score_us-bot=49.8-45.5 result=WIN\n",
      "[DAgger-PvP] ep=293 step=055 beta=0.052 caught=080 first_cap=1 bc=1.0660 cluster=0.02 idle=0.02 flip=0.41 same=0.14 score_us-bot=41.5-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=294 step=075 beta=0.052 caught=080 first_cap=0 bc=1.0331 cluster=0.11 idle=0.03 flip=0.33 same=0.29 score_us-bot=42.0-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=295 step=079 beta=0.051 caught=080 first_cap=0 bc=0.9260 cluster=0.15 idle=0.00 flip=0.41 same=0.24 score_us-bot=32.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=296 step=084 beta=0.051 caught=080 first_cap=0 bc=0.9281 cluster=0.00 idle=0.01 flip=0.49 same=0.08 score_us-bot=47.2-45.0 result=WIN\n",
      "[DAgger-PvP] ep=297 step=080 beta=0.050 caught=080 first_cap=0 bc=1.0478 cluster=0.00 idle=0.00 flip=0.33 same=0.19 score_us-bot=45.0-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=298 step=080 beta=0.050 caught=080 first_cap=3 bc=0.9085 cluster=0.00 idle=0.00 flip=0.48 same=0.14 score_us-bot=41.5-48.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_299.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep299.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_299.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep299.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=299 step=073 beta=0.050 caught=080 first_cap=0 bc=0.8425 cluster=0.00 idle=0.01 flip=0.41 same=0.07 score_us-bot=40.0-48.0 result=LOSS\n",
      "Final PvP checkpoint: C:\\Users\\aveexela\\Desktop\\rl_project\\logs\\checkpoints\\agent_pvp.pkl\n",
      "Сохранено также в ./agent.pkl\n",
      "[STAGE 3] Distill → Lite model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5a932b8703468cb369ef09b635d6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DAgger-PvP:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_000.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep000.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_000.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep000.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=000 step=064 beta=0.494 caught=080 first_cap=0 bc=1.3111 cluster=0.00 idle=0.00 flip=0.46 same=0.09 score_us-bot=44.0-47.8 result=LOSS\n",
      "[DAgger-PvP] ep=001 step=054 beta=0.489 caught=080 first_cap=0 bc=1.2954 cluster=0.00 idle=0.00 flip=0.35 same=0.04 score_us-bot=44.0-59.2 result=LOSS\n",
      "[DAgger-PvP] ep=002 step=064 beta=0.483 caught=080 first_cap=1 bc=1.2339 cluster=0.00 idle=0.00 flip=0.34 same=0.08 score_us-bot=47.8-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=003 step=056 beta=0.477 caught=080 first_cap=0 bc=1.2089 cluster=0.02 idle=0.00 flip=0.40 same=0.26 score_us-bot=45.0-42.0 result=WIN\n",
      "[DAgger-PvP] ep=004 step=074 beta=0.472 caught=080 first_cap=0 bc=1.2650 cluster=0.07 idle=0.01 flip=0.28 same=0.23 score_us-bot=42.0-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=005 step=075 beta=0.466 caught=080 first_cap=0 bc=1.2228 cluster=0.00 idle=0.01 flip=0.34 same=0.09 score_us-bot=39.0-50.2 result=LOSS\n",
      "[DAgger-PvP] ep=006 step=057 beta=0.461 caught=080 first_cap=0 bc=1.2274 cluster=0.02 idle=0.02 flip=0.40 same=0.34 score_us-bot=37.0-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=007 step=055 beta=0.456 caught=080 first_cap=1 bc=1.2241 cluster=0.00 idle=0.00 flip=0.43 same=0.34 score_us-bot=39.0-44.5 result=LOSS\n",
      "[DAgger-PvP] ep=008 step=065 beta=0.451 caught=080 first_cap=0 bc=1.1486 cluster=0.00 idle=0.00 flip=0.42 same=0.03 score_us-bot=47.0-44.2 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_009.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep009.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_009.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep009.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=009 step=056 beta=0.445 caught=080 first_cap=2 bc=1.0427 cluster=0.00 idle=0.00 flip=0.35 same=0.40 score_us-bot=59.8-45.8 result=WIN\n",
      "[DAgger-PvP] ep=010 step=065 beta=0.440 caught=080 first_cap=1 bc=1.1314 cluster=0.17 idle=0.02 flip=0.33 same=0.14 score_us-bot=47.8-45.0 result=WIN\n",
      "[DAgger-PvP] ep=011 step=062 beta=0.435 caught=080 first_cap=0 bc=1.2123 cluster=0.00 idle=0.00 flip=0.40 same=0.13 score_us-bot=43.8-44.0 result=LOSS\n",
      "[DAgger-PvP] ep=012 step=074 beta=0.430 caught=100 first_cap=1 bc=1.2935 cluster=0.00 idle=0.00 flip=0.33 same=0.04 score_us-bot=65.5-57.0 result=WIN\n",
      "[DAgger-PvP] ep=013 step=076 beta=0.425 caught=100 first_cap=0 bc=1.3073 cluster=0.01 idle=0.00 flip=0.27 same=0.13 score_us-bot=54.2-63.2 result=LOSS\n",
      "[DAgger-PvP] ep=014 step=059 beta=0.420 caught=100 first_cap=0 bc=1.2951 cluster=0.00 idle=0.00 flip=0.42 same=0.00 score_us-bot=58.0-54.8 result=WIN\n",
      "[DAgger-PvP] ep=015 step=065 beta=0.415 caught=100 first_cap=0 bc=1.1963 cluster=0.02 idle=0.00 flip=0.38 same=0.03 score_us-bot=55.0-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=016 step=080 beta=0.411 caught=100 first_cap=0 bc=1.2221 cluster=0.00 idle=0.01 flip=0.33 same=0.10 score_us-bot=44.5-66.5 result=LOSS\n",
      "[DAgger-PvP] ep=017 step=068 beta=0.406 caught=100 first_cap=0 bc=1.1919 cluster=0.00 idle=0.00 flip=0.33 same=0.01 score_us-bot=39.0-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=018 step=068 beta=0.401 caught=100 first_cap=0 bc=1.1639 cluster=0.09 idle=0.00 flip=0.32 same=0.28 score_us-bot=48.8-64.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_019.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep019.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_019.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep019.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=019 step=081 beta=0.397 caught=100 first_cap=1 bc=1.1047 cluster=0.02 idle=0.01 flip=0.38 same=0.28 score_us-bot=57.8-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=020 step=082 beta=0.392 caught=100 first_cap=0 bc=1.1423 cluster=0.02 idle=0.01 flip=0.27 same=0.27 score_us-bot=43.0-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=021 step=067 beta=0.388 caught=100 first_cap=0 bc=1.1165 cluster=0.00 idle=0.00 flip=0.28 same=0.12 score_us-bot=57.8-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=022 step=065 beta=0.383 caught=100 first_cap=0 bc=1.1476 cluster=0.03 idle=0.02 flip=0.27 same=0.24 score_us-bot=52.0-56.0 result=LOSS\n",
      "[DAgger-PvP] ep=023 step=095 beta=0.379 caught=100 first_cap=2 bc=1.0852 cluster=0.02 idle=0.00 flip=0.32 same=0.08 score_us-bot=63.0-63.5 result=LOSS\n",
      "[DAgger-PvP] ep=024 step=077 beta=0.374 caught=100 first_cap=0 bc=1.2170 cluster=0.00 idle=0.01 flip=0.29 same=0.05 score_us-bot=61.0-62.8 result=LOSS\n",
      "[DAgger-PvP] ep=025 step=071 beta=0.370 caught=100 first_cap=1 bc=1.2561 cluster=0.00 idle=0.00 flip=0.26 same=0.10 score_us-bot=59.5-60.2 result=LOSS\n",
      "[DAgger-PvP] ep=026 step=057 beta=0.366 caught=100 first_cap=0 bc=1.2172 cluster=0.02 idle=0.00 flip=0.31 same=0.09 score_us-bot=61.2-54.0 result=WIN\n",
      "[DAgger-PvP] ep=027 step=062 beta=0.362 caught=100 first_cap=1 bc=1.2016 cluster=0.05 idle=0.00 flip=0.30 same=0.11 score_us-bot=51.2-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=028 step=069 beta=0.357 caught=100 first_cap=0 bc=1.2150 cluster=0.01 idle=0.01 flip=0.26 same=0.13 score_us-bot=59.2-68.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_029.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep029.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_029.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep029.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=029 step=074 beta=0.353 caught=100 first_cap=0 bc=1.2100 cluster=0.00 idle=0.01 flip=0.17 same=0.29 score_us-bot=55.5-71.0 result=LOSS\n",
      "[DAgger-PvP] ep=030 step=077 beta=0.349 caught=100 first_cap=0 bc=1.1720 cluster=0.00 idle=0.00 flip=0.50 same=0.09 score_us-bot=63.8-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=031 step=074 beta=0.345 caught=100 first_cap=0 bc=1.1359 cluster=0.04 idle=0.01 flip=0.39 same=0.15 score_us-bot=56.0-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=032 step=079 beta=0.341 caught=100 first_cap=0 bc=1.1228 cluster=0.05 idle=0.01 flip=0.26 same=0.26 score_us-bot=49.0-63.8 result=LOSS\n",
      "[DAgger-PvP] ep=033 step=076 beta=0.337 caught=100 first_cap=0 bc=1.1435 cluster=0.00 idle=0.00 flip=0.42 same=0.22 score_us-bot=63.0-67.8 result=LOSS\n",
      "[DAgger-PvP] ep=034 step=092 beta=0.333 caught=100 first_cap=0 bc=1.1342 cluster=0.01 idle=0.00 flip=0.27 same=0.30 score_us-bot=75.5-59.2 result=WIN\n",
      "[DAgger-PvP] ep=035 step=086 beta=0.330 caught=100 first_cap=0 bc=1.1505 cluster=0.07 idle=0.01 flip=0.30 same=0.20 score_us-bot=54.8-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=036 step=085 beta=0.326 caught=080 first_cap=0 bc=1.2201 cluster=0.00 idle=0.00 flip=0.26 same=0.05 score_us-bot=35.0-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=037 step=085 beta=0.322 caught=080 first_cap=0 bc=1.2225 cluster=0.00 idle=0.00 flip=0.30 same=0.00 score_us-bot=50.0-39.5 result=WIN\n",
      "[DAgger-PvP] ep=038 step=072 beta=0.318 caught=080 first_cap=0 bc=1.2494 cluster=0.00 idle=0.01 flip=0.38 same=0.05 score_us-bot=48.0-38.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_039.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep039.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_039.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep039.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=039 step=089 beta=0.315 caught=080 first_cap=1 bc=1.1688 cluster=0.00 idle=0.00 flip=0.33 same=0.09 score_us-bot=45.8-47.2 result=LOSS\n",
      "[DAgger-PvP] ep=040 step=080 beta=0.311 caught=080 first_cap=0 bc=1.1748 cluster=0.00 idle=0.00 flip=0.28 same=0.05 score_us-bot=40.0-45.0 result=LOSS\n",
      "[DAgger-PvP] ep=041 step=092 beta=0.308 caught=080 first_cap=0 bc=1.1901 cluster=0.00 idle=0.00 flip=0.37 same=0.06 score_us-bot=47.0-44.5 result=WIN\n",
      "[DAgger-PvP] ep=042 step=101 beta=0.304 caught=080 first_cap=5 bc=1.1958 cluster=0.00 idle=0.00 flip=0.39 same=0.00 score_us-bot=51.0-46.2 result=WIN\n",
      "[DAgger-PvP] ep=043 step=080 beta=0.301 caught=080 first_cap=2 bc=1.1348 cluster=0.00 idle=0.00 flip=0.30 same=0.10 score_us-bot=34.0-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=044 step=075 beta=0.297 caught=080 first_cap=2 bc=1.1250 cluster=0.00 idle=0.01 flip=0.30 same=0.01 score_us-bot=39.2-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=045 step=086 beta=0.294 caught=080 first_cap=1 bc=1.1003 cluster=0.02 idle=0.00 flip=0.17 same=0.21 score_us-bot=45.0-38.0 result=WIN\n",
      "[DAgger-PvP] ep=046 step=108 beta=0.290 caught=080 first_cap=0 bc=1.0776 cluster=0.01 idle=0.00 flip=0.33 same=0.12 score_us-bot=49.8-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=047 step=086 beta=0.287 caught=080 first_cap=0 bc=1.0615 cluster=0.03 idle=0.00 flip=0.23 same=0.40 score_us-bot=52.0-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=048 step=091 beta=0.284 caught=080 first_cap=2 bc=1.2282 cluster=0.00 idle=0.01 flip=0.42 same=0.01 score_us-bot=55.5-54.5 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_049.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep049.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_049.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep049.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=049 step=068 beta=0.280 caught=080 first_cap=1 bc=1.1863 cluster=0.00 idle=0.00 flip=0.33 same=0.00 score_us-bot=43.5-40.0 result=WIN\n",
      "[DAgger-PvP] ep=050 step=074 beta=0.277 caught=080 first_cap=0 bc=1.1621 cluster=0.00 idle=0.01 flip=0.24 same=0.19 score_us-bot=54.0-50.0 result=WIN\n",
      "[DAgger-PvP] ep=051 step=076 beta=0.274 caught=080 first_cap=2 bc=1.1505 cluster=0.00 idle=0.01 flip=0.21 same=0.09 score_us-bot=44.8-45.0 result=LOSS\n",
      "[DAgger-PvP] ep=052 step=081 beta=0.271 caught=080 first_cap=0 bc=1.2001 cluster=0.00 idle=0.00 flip=0.44 same=0.01 score_us-bot=47.0-47.2 result=LOSS\n",
      "[DAgger-PvP] ep=053 step=094 beta=0.268 caught=080 first_cap=1 bc=1.1311 cluster=0.02 idle=0.00 flip=0.51 same=0.18 score_us-bot=44.0-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=054 step=078 beta=0.265 caught=080 first_cap=0 bc=1.2374 cluster=0.00 idle=0.01 flip=0.41 same=0.13 score_us-bot=56.2-39.0 result=WIN\n",
      "[DAgger-PvP] ep=055 step=091 beta=0.262 caught=080 first_cap=0 bc=1.1468 cluster=0.00 idle=0.01 flip=0.33 same=0.04 score_us-bot=48.2-43.2 result=WIN\n",
      "[DAgger-PvP] ep=056 step=079 beta=0.259 caught=080 first_cap=2 bc=1.0459 cluster=0.04 idle=0.01 flip=0.25 same=0.20 score_us-bot=49.2-42.8 result=WIN\n",
      "[DAgger-PvP] ep=057 step=078 beta=0.256 caught=080 first_cap=2 bc=1.0409 cluster=0.00 idle=0.01 flip=0.20 same=0.33 score_us-bot=47.8-46.8 result=WIN\n",
      "[DAgger-PvP] ep=058 step=130 beta=0.253 caught=080 first_cap=1 bc=1.0509 cluster=0.00 idle=0.00 flip=0.24 same=0.09 score_us-bot=61.0-55.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_059.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep059.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_059.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep059.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=059 step=112 beta=0.250 caught=080 first_cap=3 bc=1.0253 cluster=0.00 idle=0.00 flip=0.22 same=0.19 score_us-bot=21.2-69.0 result=LOSS\n",
      "[DAgger-PvP] ep=060 step=103 beta=0.247 caught=100 first_cap=1 bc=1.2183 cluster=0.02 idle=0.01 flip=0.30 same=0.09 score_us-bot=56.8-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=061 step=091 beta=0.244 caught=100 first_cap=0 bc=1.1629 cluster=0.00 idle=0.01 flip=0.32 same=0.27 score_us-bot=60.5-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=062 step=082 beta=0.241 caught=100 first_cap=2 bc=1.2395 cluster=0.00 idle=0.01 flip=0.34 same=0.04 score_us-bot=58.8-58.0 result=WIN\n",
      "[DAgger-PvP] ep=063 step=094 beta=0.238 caught=100 first_cap=1 bc=1.2270 cluster=0.00 idle=0.00 flip=0.28 same=0.19 score_us-bot=58.2-57.0 result=WIN\n",
      "[DAgger-PvP] ep=064 step=090 beta=0.236 caught=100 first_cap=0 bc=1.1620 cluster=0.00 idle=0.01 flip=0.29 same=0.08 score_us-bot=42.0-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=065 step=100 beta=0.233 caught=100 first_cap=0 bc=1.2029 cluster=0.01 idle=0.00 flip=0.24 same=0.11 score_us-bot=63.0-57.5 result=WIN\n",
      "[DAgger-PvP] ep=066 step=101 beta=0.230 caught=100 first_cap=1 bc=1.1599 cluster=0.00 idle=0.00 flip=0.51 same=0.03 score_us-bot=58.2-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=067 step=078 beta=0.228 caught=100 first_cap=2 bc=1.1321 cluster=0.00 idle=0.00 flip=0.33 same=0.03 score_us-bot=59.8-52.0 result=WIN\n",
      "[DAgger-PvP] ep=068 step=124 beta=0.225 caught=100 first_cap=4 bc=1.0218 cluster=0.02 idle=0.00 flip=0.22 same=0.19 score_us-bot=55.8-59.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_069.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep069.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_069.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep069.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=069 step=118 beta=0.222 caught=100 first_cap=1 bc=1.0770 cluster=0.02 idle=0.01 flip=0.33 same=0.18 score_us-bot=60.0-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=070 step=089 beta=0.220 caught=100 first_cap=1 bc=1.1044 cluster=0.00 idle=0.00 flip=0.46 same=0.04 score_us-bot=56.5-59.2 result=LOSS\n",
      "[DAgger-PvP] ep=071 step=113 beta=0.217 caught=100 first_cap=1 bc=1.0418 cluster=0.00 idle=0.00 flip=0.30 same=0.15 score_us-bot=36.5-71.0 result=LOSS\n",
      "[DAgger-PvP] ep=072 step=079 beta=0.215 caught=100 first_cap=2 bc=1.2018 cluster=0.00 idle=0.01 flip=0.34 same=0.10 score_us-bot=47.0-76.0 result=LOSS\n",
      "[DAgger-PvP] ep=073 step=085 beta=0.212 caught=100 first_cap=1 bc=1.2273 cluster=0.00 idle=0.01 flip=0.38 same=0.06 score_us-bot=41.0-70.0 result=LOSS\n",
      "[DAgger-PvP] ep=074 step=075 beta=0.210 caught=100 first_cap=0 bc=1.2260 cluster=0.00 idle=0.01 flip=0.38 same=0.01 score_us-bot=59.2-47.0 result=WIN\n",
      "[DAgger-PvP] ep=075 step=092 beta=0.208 caught=100 first_cap=2 bc=1.2058 cluster=0.00 idle=0.01 flip=0.43 same=0.06 score_us-bot=56.2-61.5 result=LOSS\n",
      "[DAgger-PvP] ep=076 step=081 beta=0.205 caught=100 first_cap=1 bc=1.1086 cluster=0.02 idle=0.01 flip=0.21 same=0.04 score_us-bot=51.8-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=077 step=098 beta=0.203 caught=100 first_cap=1 bc=1.1499 cluster=0.00 idle=0.00 flip=0.35 same=0.06 score_us-bot=56.0-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=078 step=078 beta=0.200 caught=100 first_cap=0 bc=1.1406 cluster=0.00 idle=0.00 flip=0.37 same=0.00 score_us-bot=60.0-46.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_079.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep079.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_079.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep079.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=079 step=096 beta=0.198 caught=100 first_cap=5 bc=1.0806 cluster=0.02 idle=0.01 flip=0.41 same=0.19 score_us-bot=53.2-71.0 result=LOSS\n",
      "[DAgger-PvP] ep=080 step=094 beta=0.196 caught=100 first_cap=2 bc=1.0806 cluster=0.00 idle=0.01 flip=0.40 same=0.00 score_us-bot=62.8-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=081 step=096 beta=0.194 caught=100 first_cap=0 bc=1.1056 cluster=0.00 idle=0.00 flip=0.28 same=0.14 score_us-bot=54.0-52.0 result=WIN\n",
      "[DAgger-PvP] ep=082 step=119 beta=0.191 caught=100 first_cap=0 bc=1.0709 cluster=0.04 idle=0.01 flip=0.27 same=0.29 score_us-bot=55.5-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=083 step=107 beta=0.189 caught=100 first_cap=3 bc=1.0596 cluster=0.00 idle=0.00 flip=0.31 same=0.16 score_us-bot=60.8-58.0 result=WIN\n",
      "[DAgger-PvP] ep=084 step=074 beta=0.187 caught=080 first_cap=2 bc=1.2507 cluster=0.00 idle=0.01 flip=0.29 same=0.03 score_us-bot=48.0-46.0 result=WIN\n",
      "[DAgger-PvP] ep=085 step=085 beta=0.185 caught=080 first_cap=1 bc=1.1932 cluster=0.00 idle=0.01 flip=0.31 same=0.13 score_us-bot=40.8-45.0 result=LOSS\n",
      "[DAgger-PvP] ep=086 step=075 beta=0.183 caught=080 first_cap=1 bc=1.1963 cluster=0.00 idle=0.00 flip=0.43 same=0.00 score_us-bot=48.8-48.2 result=WIN\n",
      "[DAgger-PvP] ep=087 step=079 beta=0.181 caught=080 first_cap=2 bc=1.2094 cluster=0.03 idle=0.01 flip=0.39 same=0.10 score_us-bot=35.0-45.0 result=LOSS\n",
      "[DAgger-PvP] ep=088 step=079 beta=0.179 caught=080 first_cap=1 bc=1.1875 cluster=0.00 idle=0.01 flip=0.46 same=0.07 score_us-bot=43.5-48.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_089.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep089.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_089.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep089.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=089 step=084 beta=0.176 caught=080 first_cap=4 bc=1.1862 cluster=0.00 idle=0.01 flip=0.31 same=0.01 score_us-bot=38.0-42.0 result=LOSS\n",
      "[DAgger-PvP] ep=090 step=090 beta=0.174 caught=080 first_cap=0 bc=1.1326 cluster=0.00 idle=0.00 flip=0.38 same=0.07 score_us-bot=41.0-49.0 result=LOSS\n",
      "[DAgger-PvP] ep=091 step=092 beta=0.172 caught=080 first_cap=2 bc=1.0859 cluster=0.01 idle=0.01 flip=0.31 same=0.33 score_us-bot=47.0-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=092 step=065 beta=0.170 caught=080 first_cap=1 bc=1.1389 cluster=0.00 idle=0.02 flip=0.50 same=0.09 score_us-bot=41.2-45.0 result=LOSS\n",
      "[DAgger-PvP] ep=093 step=087 beta=0.169 caught=080 first_cap=2 bc=1.1020 cluster=0.00 idle=0.01 flip=0.36 same=0.19 score_us-bot=52.0-52.0 result=DRAW\n",
      "[DAgger-PvP] ep=094 step=103 beta=0.167 caught=080 first_cap=3 bc=1.0677 cluster=0.06 idle=0.01 flip=0.25 same=0.49 score_us-bot=52.0-41.2 result=WIN\n",
      "[DAgger-PvP] ep=095 step=111 beta=0.165 caught=080 first_cap=0 bc=1.1153 cluster=0.00 idle=0.00 flip=0.36 same=0.06 score_us-bot=34.0-49.5 result=LOSS\n",
      "[DAgger-PvP] ep=096 step=077 beta=0.163 caught=080 first_cap=2 bc=1.2034 cluster=0.00 idle=0.01 flip=0.26 same=0.00 score_us-bot=46.5-37.0 result=WIN\n",
      "[DAgger-PvP] ep=097 step=072 beta=0.161 caught=080 first_cap=1 bc=1.1666 cluster=0.01 idle=0.01 flip=0.27 same=0.23 score_us-bot=37.8-58.0 result=LOSS\n",
      "[DAgger-PvP] ep=098 step=077 beta=0.159 caught=080 first_cap=1 bc=1.1918 cluster=0.00 idle=0.01 flip=0.31 same=0.09 score_us-bot=50.5-52.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_099.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep099.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_099.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep099.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=099 step=089 beta=0.157 caught=080 first_cap=1 bc=1.2363 cluster=0.01 idle=0.00 flip=0.44 same=0.07 score_us-bot=42.0-50.0 result=LOSS\n",
      "[DAgger-PvP] ep=100 step=085 beta=0.155 caught=080 first_cap=0 bc=1.1635 cluster=0.00 idle=0.00 flip=0.24 same=0.14 score_us-bot=55.2-48.0 result=WIN\n",
      "[DAgger-PvP] ep=101 step=073 beta=0.154 caught=080 first_cap=0 bc=1.1553 cluster=0.00 idle=0.01 flip=0.31 same=0.01 score_us-bot=36.0-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=102 step=095 beta=0.152 caught=080 first_cap=1 bc=1.2023 cluster=0.01 idle=0.00 flip=0.38 same=0.15 score_us-bot=36.2-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=103 step=079 beta=0.150 caught=080 first_cap=1 bc=1.1077 cluster=0.03 idle=0.01 flip=0.26 same=0.16 score_us-bot=45.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=104 step=108 beta=0.148 caught=080 first_cap=1 bc=1.0410 cluster=0.00 idle=0.01 flip=0.28 same=0.00 score_us-bot=65.0-60.8 result=WIN\n",
      "[DAgger-PvP] ep=105 step=103 beta=0.147 caught=080 first_cap=2 bc=1.0812 cluster=0.01 idle=0.00 flip=0.36 same=0.08 score_us-bot=47.0-42.5 result=WIN\n",
      "[DAgger-PvP] ep=106 step=100 beta=0.145 caught=080 first_cap=1 bc=1.0966 cluster=0.00 idle=0.00 flip=0.42 same=0.05 score_us-bot=51.8-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=107 step=125 beta=0.143 caught=080 first_cap=0 bc=1.1292 cluster=0.01 idle=0.00 flip=0.35 same=0.05 score_us-bot=64.0-31.0 result=WIN\n",
      "[DAgger-PvP] ep=108 step=097 beta=0.142 caught=100 first_cap=0 bc=1.2717 cluster=0.01 idle=0.01 flip=0.39 same=0.02 score_us-bot=70.2-63.8 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_109.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep109.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_109.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep109.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=109 step=085 beta=0.140 caught=100 first_cap=1 bc=1.2595 cluster=0.00 idle=0.01 flip=0.33 same=0.09 score_us-bot=71.0-58.0 result=WIN\n",
      "[DAgger-PvP] ep=110 step=096 beta=0.138 caught=100 first_cap=2 bc=1.2091 cluster=0.00 idle=0.01 flip=0.34 same=0.01 score_us-bot=52.2-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=111 step=081 beta=0.137 caught=100 first_cap=2 bc=1.2228 cluster=0.00 idle=0.01 flip=0.35 same=0.07 score_us-bot=47.5-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=112 step=094 beta=0.135 caught=100 first_cap=0 bc=1.2280 cluster=0.04 idle=0.00 flip=0.39 same=0.33 score_us-bot=53.2-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=113 step=087 beta=0.134 caught=100 first_cap=0 bc=1.1550 cluster=0.00 idle=0.01 flip=0.57 same=0.09 score_us-bot=51.5-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=114 step=110 beta=0.132 caught=100 first_cap=0 bc=1.1336 cluster=0.03 idle=0.00 flip=0.37 same=0.05 score_us-bot=47.5-71.0 result=LOSS\n",
      "[DAgger-PvP] ep=115 step=072 beta=0.131 caught=100 first_cap=0 bc=1.1261 cluster=0.00 idle=0.01 flip=0.30 same=0.05 score_us-bot=51.0-61.8 result=LOSS\n",
      "[DAgger-PvP] ep=116 step=108 beta=0.129 caught=100 first_cap=1 bc=1.0591 cluster=0.00 idle=0.00 flip=0.28 same=0.17 score_us-bot=43.0-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=117 step=111 beta=0.128 caught=100 first_cap=5 bc=0.9983 cluster=0.00 idle=0.01 flip=0.33 same=0.32 score_us-bot=52.8-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=118 step=119 beta=0.126 caught=100 first_cap=2 bc=1.0731 cluster=0.00 idle=0.00 flip=0.39 same=0.07 score_us-bot=63.0-54.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_119.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep119.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_119.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep119.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=119 step=126 beta=0.125 caught=100 first_cap=1 bc=1.0058 cluster=0.04 idle=0.02 flip=0.25 same=0.16 score_us-bot=59.2-70.5 result=LOSS\n",
      "[DAgger-PvP] ep=120 step=093 beta=0.123 caught=100 first_cap=2 bc=1.2689 cluster=0.00 idle=0.01 flip=0.26 same=0.04 score_us-bot=65.5-56.0 result=WIN\n",
      "[DAgger-PvP] ep=121 step=100 beta=0.122 caught=100 first_cap=0 bc=1.1723 cluster=0.02 idle=0.00 flip=0.27 same=0.08 score_us-bot=54.2-73.0 result=LOSS\n",
      "[DAgger-PvP] ep=122 step=092 beta=0.120 caught=100 first_cap=0 bc=1.1622 cluster=0.03 idle=0.01 flip=0.34 same=0.32 score_us-bot=38.0-62.0 result=LOSS\n",
      "[DAgger-PvP] ep=123 step=075 beta=0.119 caught=100 first_cap=0 bc=1.1948 cluster=0.05 idle=0.00 flip=0.36 same=0.24 score_us-bot=49.2-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=124 step=085 beta=0.118 caught=100 first_cap=1 bc=1.1927 cluster=0.00 idle=0.01 flip=0.34 same=0.02 score_us-bot=57.5-60.2 result=LOSS\n",
      "[DAgger-PvP] ep=125 step=094 beta=0.116 caught=100 first_cap=0 bc=1.1539 cluster=0.00 idle=0.01 flip=0.38 same=0.13 score_us-bot=56.0-70.0 result=LOSS\n",
      "[DAgger-PvP] ep=126 step=098 beta=0.115 caught=100 first_cap=1 bc=1.1216 cluster=0.02 idle=0.00 flip=0.32 same=0.06 score_us-bot=51.0-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=127 step=086 beta=0.114 caught=100 first_cap=1 bc=1.0515 cluster=0.00 idle=0.00 flip=0.16 same=0.30 score_us-bot=50.2-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=128 step=086 beta=0.112 caught=100 first_cap=2 bc=1.0866 cluster=0.00 idle=0.01 flip=0.34 same=0.06 score_us-bot=59.0-61.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_129.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep129.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_129.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep129.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=129 step=110 beta=0.111 caught=100 first_cap=2 bc=1.1095 cluster=0.00 idle=0.00 flip=0.28 same=0.06 score_us-bot=54.2-57.8 result=LOSS\n",
      "[DAgger-PvP] ep=130 step=101 beta=0.110 caught=100 first_cap=1 bc=1.0908 cluster=0.01 idle=0.01 flip=0.35 same=0.13 score_us-bot=52.2-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=131 step=130 beta=0.109 caught=100 first_cap=2 bc=1.0471 cluster=0.03 idle=0.02 flip=0.27 same=0.24 score_us-bot=52.0-58.5 result=LOSS\n",
      "[DAgger-PvP] ep=132 step=080 beta=0.107 caught=080 first_cap=1 bc=1.1710 cluster=0.00 idle=0.01 flip=0.36 same=0.06 score_us-bot=36.5-56.0 result=LOSS\n",
      "[DAgger-PvP] ep=133 step=087 beta=0.106 caught=080 first_cap=2 bc=1.2102 cluster=0.00 idle=0.01 flip=0.34 same=0.14 score_us-bot=44.2-58.0 result=LOSS\n",
      "[DAgger-PvP] ep=134 step=097 beta=0.105 caught=080 first_cap=0 bc=1.2528 cluster=0.00 idle=0.01 flip=0.27 same=0.09 score_us-bot=38.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=135 step=087 beta=0.104 caught=080 first_cap=0 bc=1.1631 cluster=0.00 idle=0.01 flip=0.28 same=0.16 score_us-bot=41.0-46.2 result=LOSS\n",
      "[DAgger-PvP] ep=136 step=091 beta=0.102 caught=080 first_cap=1 bc=1.0950 cluster=0.00 idle=0.00 flip=0.39 same=0.00 score_us-bot=37.0-56.2 result=LOSS\n",
      "[DAgger-PvP] ep=137 step=126 beta=0.101 caught=080 first_cap=2 bc=1.0770 cluster=0.02 idle=0.01 flip=0.47 same=0.09 score_us-bot=31.8-79.0 result=LOSS\n",
      "[DAgger-PvP] ep=138 step=140 beta=0.100 caught=080 first_cap=4 bc=1.1154 cluster=0.00 idle=0.01 flip=0.41 same=0.05 score_us-bot=56.8-64.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_139.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep139.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_139.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep139.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=139 step=099 beta=0.099 caught=080 first_cap=0 bc=1.0826 cluster=0.00 idle=0.00 flip=0.29 same=0.13 score_us-bot=48.0-35.0 result=WIN\n",
      "[DAgger-PvP] ep=140 step=137 beta=0.098 caught=080 first_cap=0 bc=1.0786 cluster=0.03 idle=0.01 flip=0.42 same=0.12 score_us-bot=41.5-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=141 step=111 beta=0.097 caught=080 first_cap=1 bc=1.0635 cluster=0.01 idle=0.01 flip=0.29 same=0.12 score_us-bot=40.8-53.0 result=LOSS\n",
      "[DAgger-PvP] ep=142 step=103 beta=0.096 caught=080 first_cap=1 bc=1.0704 cluster=0.00 idle=0.01 flip=0.26 same=0.07 score_us-bot=29.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=143 step=180 beta=0.094 caught=080 first_cap=0 bc=1.0373 cluster=0.03 idle=0.01 flip=0.24 same=0.19 score_us-bot=28.0-61.0 result=LOSS\n",
      "[DAgger-PvP] ep=144 step=096 beta=0.093 caught=080 first_cap=1 bc=1.1936 cluster=0.00 idle=0.01 flip=0.37 same=0.04 score_us-bot=28.0-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=145 step=090 beta=0.092 caught=080 first_cap=3 bc=1.2492 cluster=0.00 idle=0.01 flip=0.36 same=0.02 score_us-bot=41.2-42.0 result=LOSS\n",
      "[DAgger-PvP] ep=146 step=090 beta=0.091 caught=080 first_cap=1 bc=1.1755 cluster=0.02 idle=0.01 flip=0.36 same=0.05 score_us-bot=39.0-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=147 step=114 beta=0.090 caught=080 first_cap=0 bc=1.2053 cluster=0.00 idle=0.00 flip=0.36 same=0.07 score_us-bot=38.8-48.0 result=LOSS\n",
      "[DAgger-PvP] ep=148 step=095 beta=0.089 caught=080 first_cap=1 bc=1.1898 cluster=0.00 idle=0.01 flip=0.29 same=0.10 score_us-bot=37.0-61.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_149.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep149.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_149.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep149.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=149 step=100 beta=0.088 caught=080 first_cap=1 bc=1.1697 cluster=0.00 idle=0.00 flip=0.30 same=0.01 score_us-bot=39.5-67.0 result=LOSS\n",
      "[DAgger-PvP] ep=150 step=106 beta=0.087 caught=080 first_cap=3 bc=1.1762 cluster=0.00 idle=0.01 flip=0.50 same=0.05 score_us-bot=42.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=151 step=149 beta=0.086 caught=080 first_cap=2 bc=1.0423 cluster=0.00 idle=0.01 flip=0.33 same=0.02 score_us-bot=52.0-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=152 step=114 beta=0.085 caught=080 first_cap=0 bc=1.0236 cluster=0.00 idle=0.01 flip=0.35 same=0.00 score_us-bot=36.0-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=153 step=113 beta=0.084 caught=080 first_cap=2 bc=1.0956 cluster=0.02 idle=0.01 flip=0.32 same=0.33 score_us-bot=42.2-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=154 step=161 beta=0.083 caught=080 first_cap=2 bc=1.0433 cluster=0.09 idle=0.07 flip=0.33 same=0.28 score_us-bot=42.2-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=155 step=132 beta=0.082 caught=080 first_cap=1 bc=1.1161 cluster=0.00 idle=0.00 flip=0.47 same=0.08 score_us-bot=37.5-49.0 result=LOSS\n",
      "[DAgger-PvP] ep=156 step=107 beta=0.081 caught=100 first_cap=1 bc=1.1278 cluster=0.00 idle=0.01 flip=0.20 same=0.22 score_us-bot=47.2-59.0 result=LOSS\n",
      "[DAgger-PvP] ep=157 step=111 beta=0.080 caught=100 first_cap=2 bc=1.1754 cluster=0.00 idle=0.00 flip=0.31 same=0.08 score_us-bot=57.2-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=158 step=114 beta=0.079 caught=100 first_cap=1 bc=1.1203 cluster=0.00 idle=0.01 flip=0.30 same=0.04 score_us-bot=64.2-61.0 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_159.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep159.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_159.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep159.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=159 step=144 beta=0.079 caught=100 first_cap=1 bc=1.2010 cluster=0.00 idle=0.00 flip=0.39 same=0.05 score_us-bot=57.5-59.2 result=LOSS\n",
      "[DAgger-PvP] ep=160 step=115 beta=0.078 caught=100 first_cap=2 bc=1.1395 cluster=0.01 idle=0.01 flip=0.32 same=0.08 score_us-bot=73.2-74.5 result=LOSS\n",
      "[DAgger-PvP] ep=161 step=105 beta=0.077 caught=100 first_cap=1 bc=1.1588 cluster=0.00 idle=0.01 flip=0.39 same=0.08 score_us-bot=61.0-60.0 result=WIN\n",
      "[DAgger-PvP] ep=162 step=143 beta=0.076 caught=100 first_cap=1 bc=1.1606 cluster=0.00 idle=0.01 flip=0.38 same=0.06 score_us-bot=43.0-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=163 step=142 beta=0.075 caught=100 first_cap=2 bc=1.0713 cluster=0.01 idle=0.01 flip=0.38 same=0.13 score_us-bot=49.8-73.8 result=LOSS\n",
      "[DAgger-PvP] ep=164 step=143 beta=0.074 caught=100 first_cap=0 bc=1.0728 cluster=0.00 idle=0.00 flip=0.40 same=0.15 score_us-bot=62.2-61.0 result=WIN\n",
      "[DAgger-PvP] ep=165 step=123 beta=0.073 caught=100 first_cap=0 bc=1.0911 cluster=0.00 idle=0.01 flip=0.36 same=0.15 score_us-bot=66.8-69.0 result=LOSS\n",
      "[DAgger-PvP] ep=166 step=114 beta=0.072 caught=100 first_cap=0 bc=1.0478 cluster=0.00 idle=0.00 flip=0.29 same=0.10 score_us-bot=54.5-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=167 step=177 beta=0.072 caught=100 first_cap=1 bc=1.0615 cluster=0.00 idle=0.00 flip=0.40 same=0.12 score_us-bot=56.0-47.0 result=WIN\n",
      "[DAgger-PvP] ep=168 step=104 beta=0.071 caught=100 first_cap=2 bc=1.2188 cluster=0.01 idle=0.01 flip=0.30 same=0.10 score_us-bot=49.0-60.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_169.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep169.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_169.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep169.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=169 step=101 beta=0.070 caught=100 first_cap=0 bc=1.2456 cluster=0.02 idle=0.01 flip=0.30 same=0.12 score_us-bot=37.0-72.0 result=LOSS\n",
      "[DAgger-PvP] ep=170 step=087 beta=0.069 caught=100 first_cap=2 bc=1.1694 cluster=0.00 idle=0.00 flip=0.30 same=0.01 score_us-bot=46.0-57.0 result=LOSS\n",
      "[DAgger-PvP] ep=171 step=100 beta=0.068 caught=100 first_cap=0 bc=1.1660 cluster=0.00 idle=0.00 flip=0.43 same=0.05 score_us-bot=53.0-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=172 step=116 beta=0.068 caught=100 first_cap=2 bc=1.1935 cluster=0.00 idle=0.01 flip=0.43 same=0.12 score_us-bot=63.5-65.0 result=LOSS\n",
      "[DAgger-PvP] ep=173 step=120 beta=0.067 caught=100 first_cap=0 bc=1.1210 cluster=0.00 idle=0.01 flip=0.35 same=0.10 score_us-bot=41.0-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=174 step=167 beta=0.066 caught=100 first_cap=0 bc=1.0883 cluster=0.02 idle=0.00 flip=0.46 same=0.16 score_us-bot=42.8-73.0 result=LOSS\n",
      "[DAgger-PvP] ep=175 step=132 beta=0.065 caught=100 first_cap=0 bc=1.0587 cluster=0.00 idle=0.01 flip=0.33 same=0.10 score_us-bot=62.0-66.5 result=LOSS\n",
      "[DAgger-PvP] ep=176 step=144 beta=0.064 caught=100 first_cap=3 bc=1.0453 cluster=0.00 idle=0.01 flip=0.39 same=0.00 score_us-bot=39.0-76.0 result=LOSS\n",
      "[DAgger-PvP] ep=177 step=097 beta=0.064 caught=100 first_cap=0 bc=1.0956 cluster=0.00 idle=0.00 flip=0.32 same=0.09 score_us-bot=45.0-68.0 result=LOSS\n",
      "[DAgger-PvP] ep=178 step=170 beta=0.063 caught=100 first_cap=3 bc=1.0089 cluster=0.03 idle=0.02 flip=0.39 same=0.28 score_us-bot=71.2-81.0 result=LOSS\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_179.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep179.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_179.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep179.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=179 step=175 beta=0.062 caught=100 first_cap=1 bc=1.0878 cluster=0.01 idle=0.01 flip=0.40 same=0.07 score_us-bot=62.0-61.5 result=WIN\n",
      "[DAgger-PvP] ep=180 step=071 beta=0.062 caught=080 first_cap=1 bc=1.1408 cluster=0.00 idle=0.01 flip=0.31 same=0.08 score_us-bot=31.0-64.0 result=LOSS\n",
      "[DAgger-PvP] ep=181 step=058 beta=0.061 caught=080 first_cap=2 bc=1.2435 cluster=0.00 idle=0.02 flip=0.34 same=0.02 score_us-bot=37.8-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=182 step=060 beta=0.060 caught=080 first_cap=0 bc=1.2151 cluster=0.03 idle=0.02 flip=0.28 same=0.05 score_us-bot=49.2-51.0 result=LOSS\n",
      "[DAgger-PvP] ep=183 step=057 beta=0.059 caught=080 first_cap=0 bc=1.2025 cluster=0.02 idle=0.02 flip=0.31 same=0.10 score_us-bot=29.0-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=184 step=069 beta=0.059 caught=080 first_cap=0 bc=1.1480 cluster=0.03 idle=0.01 flip=0.43 same=0.16 score_us-bot=34.0-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=185 step=080 beta=0.058 caught=080 first_cap=2 bc=1.1432 cluster=0.00 idle=0.01 flip=0.37 same=0.06 score_us-bot=51.5-63.0 result=LOSS\n",
      "[DAgger-PvP] ep=186 step=080 beta=0.057 caught=080 first_cap=0 bc=1.0909 cluster=0.02 idle=0.01 flip=0.48 same=0.15 score_us-bot=42.5-54.0 result=LOSS\n",
      "[DAgger-PvP] ep=187 step=075 beta=0.057 caught=080 first_cap=1 bc=1.1359 cluster=0.01 idle=0.01 flip=0.38 same=0.05 score_us-bot=44.0-47.0 result=LOSS\n",
      "[DAgger-PvP] ep=188 step=069 beta=0.056 caught=080 first_cap=0 bc=1.1453 cluster=0.04 idle=0.01 flip=0.33 same=0.14 score_us-bot=53.5-53.5 result=DRAW\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_189.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep189.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_189.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep189.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=189 step=084 beta=0.055 caught=080 first_cap=0 bc=1.0661 cluster=0.06 idle=0.01 flip=0.38 same=0.08 score_us-bot=45.2-66.0 result=LOSS\n",
      "[DAgger-PvP] ep=190 step=063 beta=0.055 caught=080 first_cap=0 bc=1.0845 cluster=0.00 idle=0.02 flip=0.42 same=0.12 score_us-bot=50.2-52.0 result=LOSS\n",
      "[DAgger-PvP] ep=191 step=072 beta=0.054 caught=080 first_cap=1 bc=1.0383 cluster=0.05 idle=0.01 flip=0.41 same=0.33 score_us-bot=37.0-46.0 result=LOSS\n",
      "[DAgger-PvP] ep=192 step=075 beta=0.054 caught=080 first_cap=0 bc=1.1243 cluster=0.00 idle=0.01 flip=0.28 same=0.16 score_us-bot=53.2-50.2 result=WIN\n",
      "[DAgger-PvP] ep=193 step=074 beta=0.053 caught=080 first_cap=0 bc=1.1605 cluster=0.01 idle=0.00 flip=0.31 same=0.08 score_us-bot=53.0-48.0 result=WIN\n",
      "[DAgger-PvP] ep=194 step=080 beta=0.052 caught=080 first_cap=1 bc=1.1889 cluster=0.01 idle=0.01 flip=0.47 same=0.33 score_us-bot=38.2-55.0 result=LOSS\n",
      "[DAgger-PvP] ep=195 step=064 beta=0.052 caught=080 first_cap=3 bc=1.1664 cluster=0.00 idle=0.00 flip=0.38 same=0.06 score_us-bot=37.5-60.0 result=LOSS\n",
      "[DAgger-PvP] ep=196 step=055 beta=0.051 caught=080 first_cap=1 bc=1.1044 cluster=0.00 idle=0.02 flip=0.39 same=0.04 score_us-bot=38.0-51.5 result=LOSS\n",
      "[DAgger-PvP] ep=197 step=069 beta=0.051 caught=080 first_cap=2 bc=1.1415 cluster=0.03 idle=0.01 flip=0.41 same=0.09 score_us-bot=43.8-49.0 result=LOSS\n",
      "[DAgger-PvP] ep=198 step=080 beta=0.050 caught=080 first_cap=0 bc=1.1192 cluster=0.00 idle=0.01 flip=0.36 same=0.11 score_us-bot=57.2-55.5 result=WIN\n",
      "GIF сохранён: logs\\frames\\pvp_dagger_ep_199.gif\n",
      "Командная карта посещений сохранена: logs\\maps\\pvp_team_ep199.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display:flex; align-items:flex-start; gap:8px; justify-content:center;\">\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\frames\\pvp_dagger_ep_199.gif\" width=\"300\" style=\"border:1px solid #444; image-rendering:pixelated; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "        <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "            <img src=\"logs\\maps\\pvp_team_ep199.png\" width=\"300\" style=\"border:1px solid #444; aspect-ratio:1/1;\">\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAgger-PvP] ep=199 step=056 beta=0.050 caught=080 first_cap=1 bc=1.1817 cluster=0.04 idle=0.02 flip=0.44 same=0.00 score_us-bot=44.0-51.0 result=LOSS\n",
      "Lite checkpoint: C:\\Users\\aveexela\\Desktop\\rl_project\\logs\\checkpoints\\agent_lite.pkl\n",
      "Сохранено также в ./agent_lite.pkl\n"
     ]
    }
   ],
   "source": [
    "# === ЕДИНАЯ ЯЧЕЙКА: пресет под ~3 часа CPU (без рендера) ===",
    "\n",
    "# --- тумблеры скорости ---",
    "RENDER_EVERY_BIG = 10 # фактически отключить визуализацию",
    "\n",
    "# немного ограничим «тяжёлые» maps: чаще 32/40, реже 48/56",
    "SIZES_MIX  = (32, 32, 40, 40, 48)      # 56 убираем (when желании вернёшь)",
    "PREYS_GRID = (80, 100)                 # 120 реже/дороже — убрал",
    "\n",
    "TEAM_SIZE  = 5\n",
    "PATCH_SIZE = PATCH_SIZE\n",
    "K_NEAREST  = K_NEAREST\n",
    "\n",
    "# steps на episode: Stage1 чуть больше, Stage2 чуть меньше (PvP дороже)",
    "STEPS_STAGE1 = 300\n",
    "STEPS_STAGE2 = 240\n",
    "\n",
    "# --- ЭТАП 1: SOLO DAGGER ---",
    "mixed_loader = build_singleteam_mixed_loader_B(\n",
    "    sizes=SIZES_MIX,\n",
    "    preys_num_grid=PREYS_GRID,\n",
    "    spawn_points_grid=(TEAM_SIZE,),\n",
    "    pregenerated_dir=None\n",
    ")\n",
    "\n",
    "realm = Realm(\n",
    "    map_loader=mixed_loader,\n",
    "    playable_teams_num=1,\n",
    "    playable_team_size=TEAM_SIZE,\n",
    "    step_limit=STEPS_STAGE1\n",
    ")\n",
    "\n",
    "env = OnePlayerEnv(realm)\n",
    "env_wrapper = RenderedEnvWrapper(env)\n",
    "\n",
    "# агент",
    "fbuild  = FeatureBuilder(patch_size=PATCH_SIZE, k_nearest=K_NEAREST)\n",
    "agent   = NetAgentShared(fbuild, team_size=TEAM_SIZE)\n",
    "\n",
    "print(\"[STAGE 1] SOLO DAgger...\")\n",
    "train_dagger_multi(\n",
    "    agent, env_wrapper,\n",
    "    episodes=400,        # <— основная «тушка» времени",
    "    beta_start=0.95,\n",
    "    beta_end=0.05,\n",
    "    render_every=(0),\n",
    "    seed_base=12345\n",
    ")\n",
    "\n",
    "# чекпойнт",
    "os.makedirs(os.path.join(LOG_DIR, \"checkpoints\"), exist_ok=True)\n",
    "ckpt_solo = _export_agent_pkl(agent, path=os.path.join(LOG_DIR, \"checkpoints\", \"agent_solo.pkl\"))\n",
    "shutil.copyfile(ckpt_solo, \"agent_solo.pkl\")\n",
    "print(\"Solo checkpoint:\", ckpt_solo)\n",
    "\n",
    "# --- ЭТАП 2: PvP DAgger против ClosestTarget ---",
    "print(\"[STAGE 2] PvP DAgger vs ClosestTarget...\")\n",
    "\n",
    "mixed_loader_pvp = build_twoteam_mixed_loader_B(\n",
    "    sizes=SIZES_MIX,\n",
    "    preys_num_grid=PREYS_GRID,\n",
    "    team_size=TEAM_SIZE,\n",
    "    spawn_points_grid=(TEAM_SIZE, TEAM_SIZE+1)  # иногда +1 — ок",
    ")\n",
    "\n",
    "bot_enemy = ClosestTargetAgent(num_predators=TEAM_SIZE)\n",
    "\n",
    "realm_pvp = Realm(\n",
    "    map_loader=mixed_loader_pvp,\n",
    "    playable_teams_num=2,\n",
    "    bots={1: bot_enemy},\n",
    "    playable_team_size=TEAM_SIZE,\n",
    "    step_limit=STEPS_STAGE2\n",
    ")\n",
    "\n",
    "env_pvp  = VersusBotEnv(realm_pvp)\n",
    "envw_pvp = RenderedEnvWrapper(env_pvp)\n",
    "\n",
    "train_dagger_pvp(\n",
    "    agent, envw_pvp,\n",
    "    episodes=300,         # <— баланс со Stage1",
    "    beta_start=0.60,\n",
    "    beta_end=0.05,\n",
    "    render_every=(10),\n",
    "    seed_base=777000\n",
    ")\n",
    "\n",
    "final_ckpt = _export_agent_pkl(agent, path=os.path.join(LOG_DIR, \"checkpoints\", \"agent_pvp.pkl\"))\n",
    "shutil.copyfile(final_ckpt, \"agent.pkl\")\n",
    "print(\"Final PvP checkpoint:\", final_ckpt)\n",
    "print(\"Сохранено также в ./agent.pkl\")\n",
    "\n",
    "# --- ЭТАП 3: Distill → Lite + короткий PvP DAgger (ужатый) ---",
    "print(\"[STAGE 3] Distill → Lite model...\")\n",
    "\n",
    "def _ensure_teacher_replay(agent_teacher, env_for_rollout, min_items=800, seed_base=999000):\n",
    "    if len(agent_teacher.rb_feats) >= min_items:\n",
    "        return\n",
    "    print(f\"[warmup] teacher replay {len(agent_teacher.rb_feats)} < {min_items} — добиваем коротким роллаутом...\")\n",
    "    import inspect\n",
    "    teacher_bot = AssignedClosestTargetAgent(num_predators=agent_teacher.team_size)\n",
    "    episodes_warm, steps_cap = 4, 120\n",
    "    for ep in range(episodes_warm):\n",
    "        base_env = getattr(env_for_rollout, \"base_env\", None)\n",
    "        seed_ep = seed_base + ep\n",
    "        did = False\n",
    "        if base_env is not None and hasattr(base_env, \"reset\"):\n",
    "            try:\n",
    "                if \"seed\" in inspect.signature(base_env.reset).parameters:\n",
    "                    state, info = base_env.reset(seed=seed_ep); did = True\n",
    "            except Exception:\n",
    "                did = False\n",
    "        if not did:\n",
    "            state, info = env_for_rollout.reset()\n",
    "        teacher_bot.reset(state, team=0)\n",
    "        for step in range(min(getattr(env_for_rollout.base_env.realm, \"step_limit\", 240), steps_cap)):\n",
    "            t_actions = teacher_bot.get_actions(state, team=0)\n",
    "            _, feats_batch, y_batch = agent_teacher.train_step_bc_multi(info, env_for_rollout.base_env.realm.world.map, t_actions)\n",
    "            agent_teacher.add_replay(feats_batch, y_batch)\n",
    "            state, done, info = env_for_rollout.step(t_actions)\n",
    "            if done: break\n",
    "        if len(agent_teacher.rb_feats) >= min_items:\n",
    "            break\n",
    "    print(f\"[warmup] teacher replay filled: {len(agent_teacher.rb_feats)}\")\n",
    "\n",
    "agent_lite = NetAgentSharedLite(fbuild, team_size=TEAM_SIZE)\n",
    "\n",
    "try:\n",
    "    _env_for_warmup = envw_pvp\n",
    "except NameError:\n",
    "    _env_for_warmup = envw_pvp  # уже есть",
    "\n",
    "_ensure_teacher_replay(agent, _env_for_warmup, min_items=800)\n",
    "\n",
    "if agent_lite.model is None:\n",
    "    assert len(agent.rb_feats) > 0, \"Пустой реплей учителя — нечем инициализировать lite\"\n",
    "    agent_lite._ensure_model(agent.rb_feats[0][None, :])\n",
    "\n",
    "# синхронизация девайсов and аккуратный LR",
    "device_t = next(agent.model.parameters()).device\n",
    "agent_lite.model.to(device_t)\n",
    "if getattr(agent_lite, \"optimizer\", None) is None:\n",
    "    agent_lite.optimizer = torch.optim.Adam(agent_lite.model.parameters(), lr=LR)\n",
    "for pg in agent_lite.optimizer.param_groups:\n",
    "    pg[\"lr\"] = max(pg[\"lr\"] * 0.5, 1e-4)\n",
    "\n",
    "# дистилляция ужата (сократил steps/batch)",
    "distill_to_light(  \n",
    "    alpha_kd=0.85\n",
    ")\n",
    "\n",
    "# короткий PvP DAgger уже лёгким",
    "train_dagger_pvp(\n",
    "    agent_lite, _env_for_warmup,\n",
    "    episodes=200,\n",
    "    beta_start=0.50,\n",
    "    beta_end=0.05,\n",
    "    render_every=(10),\n",
    "    seed_base=888000\n",
    ")\n",
    "\n",
    "# сохранение lite",
    "lite_ckpt = _export_agent_pkl(agent_lite, path=os.path.join(LOG_DIR, \"checkpoints\", \"agent_lite.pkl\"))\n",
    "shutil.copyfile(lite_ckpt, \"agent_lite.pkl\")\n",
    "print(\"Lite checkpoint:\", lite_ckpt)\n",
    "print(\"Сохранено также в ./agent_lite.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8330834,
     "sourceId": 13160516,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "translated_to_english": true,
  "note": "Only Markdown, comments, and docstrings were translated. Code logic and tokens remain unchanged.",
  "cells_changed": 30
 },
 "nbformat": 4,
 "nbformat_minor": 4
}